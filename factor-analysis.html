
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>19. Factor Analysis (Dimensionality Reduction) &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'factor-analysis';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/factor-analysis.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="20. The Ethics of Generative Models in Sociotechnical Systems" href="ethics-of-generative-models.html" />
    <link rel="prev" title="18. Gaussian Mixture Models (Clustering)" href="gmms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/factor-analysis.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Factor Analysis (Dimensionality Reduction)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-analysis-model">19.1. Factor Analysis Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-factor-analysis-models-to-data">19.2. Fitting Factor Analysis Models to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-challenges-with-marginalization">19.3. Computational Challenges with Marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-analysis-in-numpyro">19.4. Factor Analysis in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="factor-analysis-dimensionality-reduction">
<h1><span class="section-number">19. </span>Factor Analysis (Dimensionality Reduction)<a class="headerlink" href="#factor-analysis-dimensionality-reduction" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">probabilistic_foundations_of_ml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pfml</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> As we found in the chapter on Gaussian Mixture Models (GMMs), our data might contain latent structure. In the IHH context, for example, each patient can be described by some underlying condition that explains their data, where there’s a fixed number of possible conditions. However, what happens if the hidden structure is not discrete, but is continuous. By this, we mean that, instead of supposing there exists a discrete number of latent underlying patient conditions, there may be a <em>spectrum</em> of underlying conditions. Every patient lies somewhere on this spectrum, and our goal uncover it.</p>
<p><strong>Challenge:</strong> In order to place patients on this “spectrum,” we’ll have to use a continuous latent variable. We will introduce a new model, called Factor Analysis, which does exactly this. However, when using a continuous latent variable, we’ll run into some challenges. First, we’ll notice that we need a continuous version of the law of total probability. Second, we’ll need some efficient computational scheme to apply the law of total probability in practice.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce the Factor Analysis model</p></li>
<li><p>Introduce the continuous version of the law of total probability</p></li>
<li><p>Implement Factor Analysis in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></p></li>
</ul>
<p><strong>Data:</strong> In the past week, the IHH has been hit hard by some unknown disease, spreading through the in-patient population. The IHH’s Center for Epidemiology has recently been tasked with understanding the disease and stopping its spread. You’re already attempted to group patients using their symptoms. In their newest effort, they’ve been taking patient saliva samples and screening them under a microscope to understand what’s going on. They sent you a data set consisting of microscope images—each <span class="math notranslate nohighlight">\(24 \times 24\)</span> in size (576 pixels in total)—in which each pixel takes on values from <span class="math notranslate nohighlight">\(0.0\)</span> to <span class="math notranslate nohighlight">\(1.0\)</span> (black to white). To summarize, each observation, <span class="math notranslate nohighlight">\(x_n\)</span> is a 576-long array of values between <span class="math notranslate nohighlight">\([0, 1]\)</span>: <span class="math notranslate nohighlight">\(x_n \in [0, 1]^{567}\)</span>.</p>
<p>You opened one of the microscope images and here’s what you found:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="c1"># Load the data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;data/microscope.npy&#39;</span><span class="p">)</span>

<span class="c1"># Visualize one of the microscope images</span>
<span class="n">visualize_microscope_samples</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3b6bffff7b53d944be281585648866ef17b8ebd93c17e49b84c80f1eef907c6a.png" src="_images/3b6bffff7b53d944be281585648866ef17b8ebd93c17e49b84c80f1eef907c6a.png" />
</div>
</div>
<p>The microscope image actually shows an intergalactic virus! And this specific intergalactic virus seems to be wearing a hat! If it weren’t for the epidemic at the IHH, this would have been super cute. Let’s have a look at a bunch more of these.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_microscope_samples</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5dc58d037242905cda7ec14fe41aff2c8ab8ab9fc8bf00888f8fcb2cb39bcfac.png" src="_images/5dc58d037242905cda7ec14fe41aff2c8ab8ab9fc8bf00888f8fcb2cb39bcfac.png" />
</div>
</div>
<p>It seems like we have a whole collection of viruses here, each different, and wreaking havoc in a different way. Your colleagues at the Center for Epidemiology have limited bandwidth. They can’t possibly look at every virus sample collected from every patient to understand the various mechanisms that cause the disease. As such, they’ve enlisted you to help them out. They hypothesize that although these viruses look different, most aren’t important for the presentation of the disease. As such, they would like you to learn a low-dimensional representation so that they can better explore these viruses.</p>
<p><strong>Acknowledgements.</strong> Data adapted from <a class="reference external" href="https://huggingface.co/datasets/calcuis/pixel-character" rel="noreferrer" target="_blank">this repository</a>.</p>
<section id="factor-analysis-model">
<h2><span class="section-number">19.1. </span>Factor Analysis Model<a class="headerlink" href="#factor-analysis-model" title="Link to this heading">#</a></h2>
<p><strong>Data Generating Process.</strong> So how can we help our IHH colleagues? Let’s think about their hypothesis. They said they believe there the data can actually be represented by a small number of dimensions. Using this idea, let’s assume our data <em>was actually</em> generated by a distribution on a low dimensional space. For simplicity, let’s go with a 2-dimensional Gaussian with unit variance:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f967f480-5323-4b7a-a208-4c98112ac384">
<span class="eqno">(19.1)<a class="headerlink" href="#equation-f967f480-5323-4b7a-a208-4c98112ac384" title="Permalink to this equation">#</a></span>\[\begin{align}
z_n &amp;\sim p_Z(\cdot) = \mathcal{N}\left( 
\begin{bmatrix} 0.0 \\ 0.0 \end{bmatrix},
\begin{bmatrix} 1.0 &amp; 0.0 \\ 0.0 &amp; 1.0 \end{bmatrix}
\right).
\end{align}\]</div>
<p><span class="math notranslate nohighlight">\(Z\)</span> represents our <em>latent</em> variable, since we never actually observed it—we only observed the high-dimensional microscope images. To make notation easier, we will denote square matrices with 1s along the diagonal and 0s everywhere else with <span class="math notranslate nohighlight">\(I_d\)</span> (this is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Identity_matrix" rel="noreferrer" target="_blank">identity matrix</a>). We will use the subscript-<span class="math notranslate nohighlight">\(d\)</span> to denote the size of the matrix. Using this notation, we have:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1575407c-4385-48d5-939e-46eb0657b096">
<span class="eqno">(19.2)<a class="headerlink" href="#equation-1575407c-4385-48d5-939e-46eb0657b096" title="Permalink to this equation">#</a></span>\[\begin{align}
z_n &amp;\sim p_Z(\cdot) = \mathcal{N}(0, I_2),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(0\)</span> is now an array of 0s that matches the size of <span class="math notranslate nohighlight">\(I_2\)</span> (i.e. it’s a 2-dimensional array).</p>
<p>Next, we need some way of translating the latent variable into our observed variable. That is, we need to specify the distribution of the observed data given the latent variable:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b972966b-36d8-4625-9c70-8a03bd1c167d">
<span class="eqno">(19.3)<a class="headerlink" href="#equation-b972966b-36d8-4625-9c70-8a03bd1c167d" title="Permalink to this equation">#</a></span>\[\begin{align}
x_n | z_n &amp;\sim p_{X | Z}(\cdot | z_n; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> are the parameters of the distribution. We can do this by assuming there exists some function <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span>, with parameters <span class="math notranslate nohighlight">\(W\)</span>, that maps the low-dimensional latent space to the high-dimensional observation space. In our case, <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span> will map a 2-dimensional space to a 576-dimensional space: <span class="math notranslate nohighlight">\(f: \mathbb{R}^2 \rightarrow [0, 1]^{576}\)</span>. This function can be anything we’d like it to be. In many applications of factor analysis (e.g. in psychology), it’s common to use a linear function; however, for image data, like in our case, it’s better to use a non-linear function. We will use a neural network.</p>
<p>We can then assume that there’s some “observation error” (e.g. microscope inaccuracies) that corrupt the translation from the low-dimensional space to the observed space. As is common in factor analysis, we’ll go with a multivariate Gaussian for now:</p>
<div class="amsmath math notranslate nohighlight" id="equation-99f64926-c4c6-488c-a7c5-9fbea0980992">
<span class="eqno">(19.4)<a class="headerlink" href="#equation-99f64926-c4c6-488c-a7c5-9fbea0980992" title="Permalink to this equation">#</a></span>\[\begin{align}
x_n | z_n &amp;\sim p_{X | Z}(\cdot | z_n; \theta) = \mathcal{N}(\underbrace{f(z_n;  W)}_{\text{&quot;decoder&quot;}}, \sigma^2 \cdot I_{576}),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = \{W, \sigma \}\)</span> are our model parameters, and <span class="math notranslate nohighlight">\(\sigma^2 \cdot I_{576}\)</span> is a matrix of <span class="math notranslate nohighlight">\(\sigma^2\)</span> along the diagonal and 0s everywhere else. This covariance matrix indicates that the observation errors for each pixel are uncorrelated. Lastly, we often call <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span> the “decoder,” since it decodes the compressed, low-dimensional representation of the data into the original, high-dimensional representation.</p>
<p>Putting this all together, we have the following generative process:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dbb4b595-4c94-4d00-bd08-2b246fccadac">
<span class="eqno">(19.5)<a class="headerlink" href="#equation-dbb4b595-4c94-4d00-bd08-2b246fccadac" title="Permalink to this equation">#</a></span>\[\begin{align}
z_n &amp;\sim p_Z(\cdot) = \mathcal{N}(0, I_2) \\
x_n | z_n &amp;\sim p_{X | Z}(\cdot | z_n; \theta) = \mathcal{N}(f(z_n;  W), \sigma^2 \cdot I_{576})
\end{align}\]</div>
<p><strong>Directed Graphical Model.</strong> Depicting this generative process as a DGM, we have:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMi4QzMVk&#x2F;H47_jnFt704nduRNv9JhRw&#x2F;view?embed">
    </iframe>
  </div>
</div>
<p>Apart from the parameters, you can see that this model is identical to that of a GMM. The only differences are that:</p>
<ol class="arabic simple">
<li><p>The latent variable is continuous in the factor analysis model, while its discrete in a GMM.</p></li>
<li><p>A factor analysis model uses a fancy function <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span> to map samples from the latent space to the observed space. In contrast, in a GMM, samples from the latent space are discrete—they are used to choose which “cluster” to use to sample the observed data.</p></li>
</ol>
<p><strong>Intuition.</strong> Let’s instantiate the above model for a simple, low-dimensional example. We’ll then <em>generate</em> data from this model to gain some intuition. We’ll start by assuming a distribution over a 1-dimensional latent space:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1a733ef6-b887-4ba2-a718-1ebf2fd2b68d">
<span class="eqno">(19.6)<a class="headerlink" href="#equation-1a733ef6-b887-4ba2-a718-1ebf2fd2b68d" title="Permalink to this equation">#</a></span>\[\begin{align}
z_n &amp;\sim p_Z(\cdot) = \mathcal{N}(0, 1)
\end{align}\]</div>
<p>Let’s define and visualize this distribution in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the distribution over Z</span>
<span class="n">p_z</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>


<span class="c1"># Plot its PDF</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># The support from a Gaussian is the entire real line. </span>
<span class="c1"># Here we&#39;ll plot it only where it&#39;s interesting</span>
<span class="n">z_support</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_support</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_z</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z_support</span><span class="p">)))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$z$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p_Z(z)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of Latent Variable&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cd0ffd3b7e15879980c0a5d695fbf1b6e4d3db647a67590a18cc4c79338cb410.png" src="_images/cd0ffd3b7e15879980c0a5d695fbf1b6e4d3db647a67590a18cc4c79338cb410.png" />
</div>
</div>
<p>Next, we define our <em>decoder</em>. Since a decoder typically maps a low dimensional latent space to the higher-dimensional observation space, we’ll generate observations in 2-dimensions. Specifically, we’ll go with a decoder that maps the 1-dimensional space to a 2-dimensional circle.</p>
<div class="amsmath math notranslate nohighlight" id="equation-a6041ac8-b91f-4061-ae61-3a72cc72d46a">
<span class="eqno">(19.7)<a class="headerlink" href="#equation-a6041ac8-b91f-4061-ae61-3a72cc72d46a" title="Permalink to this equation">#</a></span>\[\begin{align}
f(z; W) &amp;= \begin{bmatrix}
\cos(0.5 \cdot \pi \cdot z) \\
\sin(0.5 \cdot \pi \cdot z) \\
\end{bmatrix}.
\end{align}\]</div>
<p>Let’s implement this decoder and visualize what it does to the latent samples:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="c1"># Expects z to be of shape (N,)</span>
    <span class="c1"># Reshapes z to be of shape (N, 1) so we can concatenate it along the last axis</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

    <span class="c1"># Compute each dimension of the observed data, x</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>

    <span class="c1"># Concatenate the dimensions of x into a single vector</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Decode each point in the latent space to the observation space</span>
<span class="n">mu_support</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z_support</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu_support</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mu_support</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decoder&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe4e0102190813c2bbdac198c27f318eafb247be04b138d7c97f07c946c68432.png" src="_images/fe4e0102190813c2bbdac198c27f318eafb247be04b138d7c97f07c946c68432.png" />
</div>
</div>
<p>Finally, let’s define the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-423f704d-10cf-4f3a-9d8c-e5ac9e70082a">
<span class="eqno">(19.8)<a class="headerlink" href="#equation-423f704d-10cf-4f3a-9d8c-e5ac9e70082a" title="Permalink to this equation">#</a></span>\[\begin{align}
x_n | z_n &amp;\sim p_{X | Z}(\cdot | z_n; \theta) = \mathcal{N}(f(z_n; W), \sigma^2 \cdot I_2),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma = 0.1\)</span>. This distribution will add Gaussian noise around the circle. Let’s sample from the generative process to see what this looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define random generator keys for sampling</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">key_z</span><span class="p">,</span> <span class="n">key_x</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Choose number of samples to draw</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Draw samples of the latent variable</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">p_z</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">key_z</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,))</span>

<span class="c1"># Decode them</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># Define and sample from the conditional distribution of X given Z</span>
<span class="c1"># Notice that we didn&#39;t use D.MultivariateNormal -- we&#39;ll explain why below</span>
<span class="n">p_x_given_z</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">p_x_given_z</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">key_x</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(\cdot; W)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x_n$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from Factor Analysis Model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cd235e4d4ea2bbf7e79dd2c46b7479e2ced6969ee700009e5cc5074bc9a6e4e7.png" src="_images/cd235e4d4ea2bbf7e79dd2c46b7479e2ced6969ee700009e5cc5074bc9a6e4e7.png" />
</div>
</div>
<p>As you can see, the Factor Analysis model is quite powerful. It allows us to specify all sorts of interesting distributions (like a distribution on a circle).</p>
<p><strong>Multivariate Normals in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</strong> You may have noticed above that, even though <span class="math notranslate nohighlight">\(p_{X | Z}\)</span> is a multivariate normal, we didn’t use <code class="docutils literal notranslate"><span class="pre">D.MultivariateNormal</span></code> when defining it in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>. In many cases, we just need a multivariate normal with a <em>diagonal covariance</em> (i.e. <span class="math notranslate nohighlight">\(I\)</span>), as we did for both <span class="math notranslate nohighlight">\(p_Z\)</span> and <span class="math notranslate nohighlight">\(p_{X | Z}\)</span>. In these cases, there is a more stable and efficient way to construct multivariate normals in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> using <code class="docutils literal notranslate"><span class="pre">D.Normal</span></code>. Please use the method we introduce here.</p>
<p>Suppose we wanted to implement the following multivariate normal distribution in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dfed812f-d8ee-493b-9f1a-e523bc2a299b">
<span class="eqno">(19.9)<a class="headerlink" href="#equation-dfed812f-d8ee-493b-9f1a-e523bc2a299b" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathcal{N}\left( 
\begin{bmatrix} -2.0 \\ 1.0 \end{bmatrix},
\begin{bmatrix} 0.5^2 &amp; 0.0 \\ 0.0 &amp; 3.0^2 \end{bmatrix}
\right).
\end{align}\]</div>
<p>This is the corresponding <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mvn</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this implementation, we pass in the mean of the Gaussian, as well as the (square-root of the) diagonal elements of the covariance into <code class="docutils literal notranslate"><span class="pre">D.Normal</span></code>. In doing so, <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> will create two independent Gaussians: <span class="math notranslate nohighlight">\(\mathcal{N}(-2.0, 0.5^2)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{N}(1.0, 3.0^2)\)</span>, which is exactly what we want (since a diagonal covariance implies the dimensions of the Gaussian are uncorrelated). Lastly, we call <code class="docutils literal notranslate"><span class="pre">.to_event(1)</span></code>, which tells <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> that even though the two dimensions of the Gaussian are independent, they still make up one variable, <span class="math notranslate nohighlight">\(X\)</span>. To see this, you can print the distribution’s <code class="docutils literal notranslate"><span class="pre">event_shape</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mvn</span><span class="o">.</span><span class="n">event_shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2,)
</pre></div>
</div>
</div>
</div>
<p>The distribution’s <code class="docutils literal notranslate"><span class="pre">event_shape</span></code> should match the dimensionality of the variable. Since we wanted to sample from a 2-dimensional Gaussian, we want the distributions <code class="docutils literal notranslate"><span class="pre">event_shape</span></code> to equal <code class="docutils literal notranslate"><span class="pre">(2,)</span></code>. Whenever defining multivariate distributions in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, it’s good practice to print out the shapes of the distributions.</p>
</section>
<section id="fitting-factor-analysis-models-to-data">
<h2><span class="section-number">19.2. </span>Fitting Factor Analysis Models to Data<a class="headerlink" href="#fitting-factor-analysis-models-to-data" title="Link to this heading">#</a></h2>
<p><strong>Challenges Deriving the MLE.</strong> Now that we’ve specified our model, we’re ready to derive its MLE objective. As we’ve seen with the GMM, we’ll run into challenges; our generative process specifies a <em>joint</em> distribution over random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span>—but we just want to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize what we observed, <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-54b950c4-dc63-4f1d-8340-cc5e80d7c0fd">
<span class="eqno">(19.10)<a class="headerlink" href="#equation-54b950c4-dc63-4f1d-8340-cc5e80d7c0fd" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}; \theta) &amp;= \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \\
&amp;= \prod\limits_{n=1}^N p_X(x_n; \theta)
\end{align}\]</div>
<p>Looking at the above, what is <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span>? Our data-generating process gives us the following joint distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-47c94173-f0c1-4f71-aa77-b074e47a199f">
<span class="eqno">(19.11)<a class="headerlink" href="#equation-47c94173-f0c1-4f71-aa77-b074e47a199f" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{X | Z}(x_n, z_n; \theta) &amp;= p_{X | Z}(x_n | z_n; \theta) \cdot p_Z(z_n; \theta)
\end{align}\]</div>
<p>Somehow, we need to compute <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span> from <span class="math notranslate nohighlight">\(p_{X | Z}(x_n, z_n; \theta)\)</span>.</p>
<p>As before, we’ll use the law of total probability to marginalize out <span class="math notranslate nohighlight">\(Z\)</span> and compute <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span>. Since <span class="math notranslate nohighlight">\(Z\)</span> is a continuous random variable, we need a continuous version of the law of total probability.</p>
<p><strong>Law of Total Probability (Continuous).</strong> Recall that the law of total probability tells us how to compute a marginal distribution from a joint distribution. Suppose you have two random variables, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, and suppose that <span class="math notranslate nohighlight">\(A\)</span> is continuous with support <span class="math notranslate nohighlight">\(S\)</span>. Then the law of total probability says we can compute the marginal <span class="math notranslate nohighlight">\(p_B(b)\)</span> from the joint <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a91bda46-2914-4860-923e-0f71a8dc3266">
<span class="eqno">(19.12)<a class="headerlink" href="#equation-a91bda46-2914-4860-923e-0f71a8dc3266" title="Permalink to this equation">#</a></span>\[\begin{align}
p_B(b) &amp;= \int\limits_{a \in S} p_{A, B}(a, b) \cdot da
\end{align}\]</div>
<p>The only difference between the discrete and continuous versions of this law is that in the continuous case we have an <em>integral</em> instead of a <em>sum</em>.</p>
<p>As in the discrete case, we can also write it using expectations by factorizing the joint distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ca57d7e5-7512-4898-9ddd-bb9c50f56cf2">
<span class="eqno">(19.13)<a class="headerlink" href="#equation-ca57d7e5-7512-4898-9ddd-bb9c50f56cf2" title="Permalink to this equation">#</a></span>\[\begin{align}
p_B(b) &amp;= \int\limits_{a \in S} p_{A, B}(a, b) \cdot da \\
&amp;= \int\limits_{a \in S} p_{B | A}(b | a) \cdot p_A(a) \cdot da \\
&amp;= \mathbb{E}_{a \sim p_A(\cdot)} \left[ p_{B | A}(b | a) \right] \\
\end{align}\]</div>
<p>The law of total probability therefore says that the probability of <span class="math notranslate nohighlight">\(B\)</span> is that of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A = a\)</span>, averaged over all values of <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p><strong>The MLE Objective.</strong> Using the law of total probaility, we can finish our derivation of the MLE objective:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d0be3354-0c76-4eb8-ba3c-c03e9012d819">
<span class="eqno">(19.14)<a class="headerlink" href="#equation-d0be3354-0c76-4eb8-ba3c-c03e9012d819" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_\theta \log p(\mathcal{D}; \theta) \\
&amp;= \mathrm{argmax}_\theta \log \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log p(\mathcal{D}_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log p_X(x_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log \int\limits_{-\infty}^\infty p_{X, Z}(x_n, z_n; \theta) \cdot dz_n \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log \int\limits_{-\infty}^\infty p_{X | Z}(x_n | z_n; \theta) \cdot p_Z(z_n) \cdot dz_n \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log \mathbb{E}_{z_n \sim p_Z(\cdot)} \left[ p_{X | Z}(x_n | z_n; \theta) \right] \\
\end{align}\]</div>
<p>Unfortunately, as we will show next, for arbitrary choices of <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span>, the expectation/integral above is intractable. By this, we mean there’s no formula for it (it cannot be computed by hand, using techniques from calculus), and approximating it naively may take a computer more than our combined life-times to finish. Next, we will explain why.</p>
</section>
<section id="computational-challenges-with-marginalization">
<h2><span class="section-number">19.3. </span>Computational Challenges with Marginalization<a class="headerlink" href="#computational-challenges-with-marginalization" title="Link to this heading">#</a></h2>
<p><strong>Intractability of Computing Integrals in High Dimensions.</strong> Let’s focus on the integral we need to evaluate:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ec90b593-610b-4f78-9613-93b8d5884b97">
<span class="eqno">(19.15)<a class="headerlink" href="#equation-ec90b593-610b-4f78-9613-93b8d5884b97" title="Permalink to this equation">#</a></span>\[\begin{align}
p_X(x_n) &amp;= \int\limits_{-\infty}^\infty p_{X | Z}(x_n | z_n; \theta) \cdot p_Z(z_n) \cdot dz_n
\end{align}\]</div>
<p>For arbitrary choices of <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span>, there’s no analytic solution to this integral. We therefore have to resort to approximation methods. Recall that integrals are the <em>area under the curve:</em></p>
<figure class="align-center" id="reimann-sum-1d">
<a class="reference internal image-reference" href="_images/reimann-sum-1d.png"><img alt="_images/reimann-sum-1d.png" src="_images/reimann-sum-1d.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19.1 </span><span class="caption-text">Depiction of integral <span class="math notranslate nohighlight">\(\int_a^b f(x) dx\)</span> as area under the curve (left), and Reimann sum approximation of integral (right). Figure adapted from <a class="reference external" href="https://calcworkshop.com/integrals/riemann-sum/" rel="noreferrer" target="_blank">this website</a>.</span><a class="headerlink" href="#reimann-sum-1d" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>By dividing up the area under the curve into a bunch of rectangles, we can approximate the integral. This approximation is called a <em>Reimann sum</em>, and as the number of rectangles grows, the approximation should converge to the true integral. In high dimensions, however, how many squares would we need? Here are some reimann sums for a 2-dimensional function:</p>
<figure class="align-center" id="reimann-sum-2d">
<a class="reference internal image-reference" href="_images/reimann-sum-2d.jpg"><img alt="_images/reimann-sum-2d.jpg" src="_images/reimann-sum-2d.jpg" style="width: 300px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19.2 </span><span class="caption-text">Reimnann sums for a 2-dimensional function from <a class="reference external" href="https://www.pinterest.com/pin/threedimensional-riemann-sums--767652699013903203/" rel="noreferrer" target="_blank">this website</a>.</span><a class="headerlink" href="#reimann-sum-2d" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>If we divided up the input space into <span class="math notranslate nohighlight">\(S\)</span> rectangles in the 1-dimensional case, we’d have to divide it into <span class="math notranslate nohighlight">\(S^2\)</span> rectangles for the 2-dimensional function. And unfortunately, the pattern continues: for a <span class="math notranslate nohighlight">\(D\)</span> dimensional space, we’d have to divide it into <span class="math notranslate nohighlight">\(S^D\)</span>. As a result, the cost of this approximation method is <em>exponential</em> in the dimensions of the space. Can we do any better?</p>
<p><strong>Monte-Carlo (MC) Estimate.</strong> By re-writing the above integral as an expectation,</p>
<div class="amsmath math notranslate nohighlight" id="equation-83bc7c1c-162c-4071-948f-57ba3b0b4edd">
<span class="eqno">(19.16)<a class="headerlink" href="#equation-83bc7c1c-162c-4071-948f-57ba3b0b4edd" title="Permalink to this equation">#</a></span>\[\begin{align}
p_X(x_n) &amp;= \mathbb{E}_{z_n \sim p_Z(\cdot)} \left[ p_{X | Z}(x_n | z_n; \theta) \right] 
\end{align}\]</div>
<p>we can approximate it by drawing samples from <span class="math notranslate nohighlight">\(p_Z(\cdot)\)</span> and averaging the expression inside the square brackets:</p>
<div class="amsmath math notranslate nohighlight" id="equation-eb8558b1-24d6-4ee3-825f-b6ea768fccaa">
<span class="eqno">(19.17)<a class="headerlink" href="#equation-eb8558b1-24d6-4ee3-825f-b6ea768fccaa" title="Permalink to this equation">#</a></span>\[\begin{align}
p_X(x_n) &amp;\approx \frac{1}{S} \sum\limits_{s=1}^S p_{X | Z}(x_n | z_s; \theta), \quad z_s \sim p_Z(\cdot) 
\end{align}\]</div>
<p>This is known as an <em>MC-estimate</em>. What’s cool about this estimate is that its accuracy is asymptotically unaffected by the dimentionality. Specifically, the approximation improves at a rate of <span class="math notranslate nohighlight">\(1 / \sqrt{S}\)</span> (which is not a function of the dimensionality, <span class="math notranslate nohighlight">\(D\)</span>). As a result, MC-estimates are ubiquitous in probabilistic ML. Unfortunately, for the expectation we have here, even an MC-estimate doesn’t suffice. Let’s see why.</p>
<p>We represent the MC estimate formula visually, using the figure below. This formula tells us we can approximate <span class="math notranslate nohighlight">\(p_X(x_n)\)</span> by drawing a bunch of <span class="math notranslate nohighlight">\(z_s\)</span> (depicted in blue dots), checking the probability of <span class="math notranslate nohighlight">\(x_n\)</span> (red dot) under a Gaussian centered at <span class="math notranslate nohighlight">\(f(z_s; W)\)</span> (blue circles). As you can see, it may take a large number of samples to draw a <span class="math notranslate nohighlight">\(z_s\)</span> whose blue circle hits the red dot. By this we mean, that it may take a large number of samples to draw a <span class="math notranslate nohighlight">\(z_n\)</span> for which <span class="math notranslate nohighlight">\(p_{X | Z}(x_n | z_s; \theta)\)</span> is high.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu_support</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mu_support</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(\cdot; W)$&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(z_s; W), z_s \sim p_Z(\cdot)$&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x_n$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Computational Challenges with Marginalization&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1a884fb0d9845af19f4b7b87bc982814c410f80527f2e10f57b015bfc0208ad9.png" src="_images/1a884fb0d9845af19f4b7b87bc982814c410f80527f2e10f57b015bfc0208ad9.png" />
</div>
</div>
<p><strong>Variational Inference (VI).</strong> In practice, we approximate the above expectation using a technique called VI. We will not get into it here, but we will ask <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> to use it under the hood. When using VI to fit a Factor Analysis model, we call the overall method a <a class="reference external" href="https://arxiv.org/pdf/1606.05908" rel="noreferrer" target="_blank"><em>Variational Autoencoder (VAE)</em></a>, which is a very popular generative model. We will use this approach help our IHH collaborators better understand their epidemic.</p>
</section>
<section id="factor-analysis-in-numpyro">
<h2><span class="section-number">19.4. </span>Factor Analysis in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code><a class="headerlink" href="#factor-analysis-in-numpyro" title="Link to this heading">#</a></h2>
<p>Whenever implementing complicated ML models, it’s good to follow these principles:</p>
<ol class="arabic simple">
<li><p>Implement the simplest version of the model first. This will help you test your conceptual understanding of the model before moving on to a more complicated problem.</p></li>
<li><p>Test the simple version of the model on a data set you <em>know</em> it should be able to fit. You can do this by generating data from the model and then seeing if you can fit the model to this data. If it can’t, something went wrong!</p></li>
<li><p>Once everything works for the simple version of the model on a simple data, then you can start complicating it.</p></li>
<li><p>Finally, when things don’t work, think of <em>all</em> places where things could have gone wrong: bug, modeling assumption, optimizer stuck in local optima, etc.</p></li>
</ol>
<p>Complete the exercises below, which follow this principles.</p>
<div class="admonition-exercise-prototyping-the-factor-analysis-model admonition">
<p class="admonition-title">Exercise: Prototyping the Factor Analysis Model</p>
<p>In this exercise, you will fit a simple factor analysis model to the circle data. Before starting to write code, please use the above code to generate a data set of 2-dimensional observations as above.</p>
<p><strong>Part 1:</strong> Implement a factor analysis model with a 1-dimensional latent space. Your factor analysis model should use the following decoder function:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(z; W) &amp;= \begin{bmatrix}
W_1 \cdot \cos(0.5 \cdot \pi \cdot z) + b_1 \\
W_2 \cdot \sin(0.5 \cdot \pi \cdot z) + b_2 \\
\end{bmatrix},
\end{align*}\]</div>
<p>where the model’s parameters are <span class="math notranslate nohighlight">\(\theta = \{ W_1, W_2, b_1, b_2, \sigma \}\)</span>.</p>
<p>Please use the following function signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">factor_analysis_circle</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p><strong>Part 2:</strong> Fit your model to the data you generated. Use the function <code class="docutils literal notranslate"><span class="pre">pfml.mle_continuous_lvm</span></code> to perform the MLE. This function will be able to efficiently approximate the intractable integrals. Usage for this function is the same as for <code class="docutils literal notranslate"><span class="pre">pfml.mle</span></code>.</p>
<p><strong>Part 3:</strong> Visualize samples from your model against the observed data. Do they look the same? If not, go back and figure out what went wrong. As you’ve done for previous models, use <code class="docutils literal notranslate"><span class="pre">pfml.sample_generative_process</span></code> to draw samples from your model.</p>
<p><strong>Part 4:</strong> After having gotten the model to work, we can now replace the above <span class="math notranslate nohighlight">\(f(\cdot; W)\)</span> with a neural network. We’ve created a function to create neural networks for you, <code class="docutils literal notranslate"><span class="pre">neural_network_fn</span></code>. This function will create a neural network for you and register all of its parameters with <code class="docutils literal notranslate"><span class="pre">numpyro.param</span></code> to simplify your code. You can learn about how to use it by reading its comments in <code class="docutils literal notranslate"><span class="pre">utils.py</span></code>. Please use the following function signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">factor_analysis_nn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p><strong>Part 5:</strong> Fit the model to the data and visualize samples from your model against the observed data. Do they look the same? If not, go back and figure out what went wrong.</p>
</div>
<div class="admonition-exercise-implementing-a-factor-analysis-model admonition">
<p class="admonition-title">Exercise: Implementing a Factor Analysis Model</p>
<p>Now that you have a working implementation of a factor analysis model working on relatively simple data, let’s adapt it to the IHH data.</p>
<p><strong>Part 1:</strong> Implement a Factor Analysis model in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, following the specification below.</p>
<ul>
<li><p>Use the following function signature for your model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">microscope_generative_model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">576</span><span class="p">]):</span>
    <span class="k">pass</span>
</pre></div>
</div>
</li>
<li><p>Your latent space should be 2-dimensional.</p></li>
<li><p>Name your latent variable <code class="docutils literal notranslate"><span class="pre">'z'</span></code> and your observed variable <code class="docutils literal notranslate"><span class="pre">'x'</span></code>. Use <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code> to name the output of <span class="math notranslate nohighlight">\(f(z_n; W)\)</span> <code class="docutils literal notranslate"><span class="pre">'mu'</span></code>. This is crucial for the methods that will help you visualize the model. What does <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code> do? Notice how <code class="docutils literal notranslate"><span class="pre">pfml.sample_generative_process</span></code> returns variables created with <code class="docutils literal notranslate"><span class="pre">numpyro.param</span></code> and <code class="docutils literal notranslate"><span class="pre">numpyro.sample</span></code>? This new primitive, <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code>, allows you to save all other variables. In this case, since we’re interested in visualizing <em>epistemic uncertainty</em>, we want to visualize <span class="math notranslate nohighlight">\(\mu(\cdot; \theta)\)</span>. This new primitive allows us to save it. When calling <code class="docutils literal notranslate"><span class="pre">pfml.sample_generative_process</span></code>, we’ll now be able to see a new variable called <code class="docutils literal notranslate"><span class="pre">mu</span></code>.</p></li>
<li><p>Your decoder, <span class="math notranslate nohighlight">\(f(\cdot, W)\)</span> should be a neural network.</p></li>
<li><p>Remember that each pixel lies on the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>. Because of this, you cannot use a Gaussian distribution for <span class="math notranslate nohighlight">\(p_{X | Z}\)</span>, since a Gaussian distribution’s support is over the entire real line. You’ll have to use a different distribution. You can find a list of distributions implemented in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> <a class="reference external" href="https://num.pyro.ai/en/stable/distributions.html#continuous-distributions" rel="noreferrer" target="_blank">here</a>.</p></li>
</ul>
<p><strong>Part 2:</strong> Fit your model to the IHH data. Use the function <code class="docutils literal notranslate"><span class="pre">pfml.mle_continuous_lvm</span></code> to perform the MLE. This function will be able to efficiently approximate the intractable integrals. Usage for this function is the same as for <code class="docutils literal notranslate"><span class="pre">pfml.mle</span></code>.</p>
<p><strong>Part 3:</strong> Draw 100 samples from your model. Visualize them using <code class="docutils literal notranslate"><span class="pre">visualize_microscope_samples</span></code> as we do above. Do your samples look like the data? If not, go back and figure out what went wrong!</p>
<p><strong>Part 4:</strong>  Now that we have learned a model that represents our data as a “spectrum” in a low dimensional space, let’s visualize what happens as we move along the spectrum. We can do this by choosing two points in the latent space, <span class="math notranslate nohighlight">\(z_1\)</span> and <span class="math notranslate nohighlight">\(z_2\)</span>. We will connect them using a line and then decode every <span class="math notranslate nohighlight">\(z\)</span> on that line. You can do this by running this code in your notebook:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">z1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>
<span class="n">animate_latent_space_path</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">result</span></code> is the output of <code class="docutils literal notranslate"><span class="pre">pfml.mle_continuous_lvm</span></code>. Make sure the call to <code class="docutils literal notranslate"><span class="pre">animate_latent_space_path</span></code> is the last line in its cell.</p>
<p>Play around with different choices of <span class="math notranslate nohighlight">\(z_1\)</span> and <span class="math notranslate nohighlight">\(z_2\)</span>. What do you notice about the images generated along the path in the latent space? Do the “axes” of the latent space have any meaning?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="gmms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">18. </span>Gaussian Mixture Models (Clustering)</p>
      </div>
    </a>
    <a class="right-next"
       href="ethics-of-generative-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">20. </span>The Ethics of Generative Models in Sociotechnical Systems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-analysis-model">19.1. Factor Analysis Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-factor-analysis-models-to-data">19.2. Fitting Factor Analysis Models to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-challenges-with-marginalization">19.3. Computational Challenges with Marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-analysis-in-numpyro">19.4. Factor Analysis in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>