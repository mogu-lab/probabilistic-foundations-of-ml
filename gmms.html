

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>18. Gaussian Mixture Models (Clustering) &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gmms';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/gmms.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19. Factor Analysis (Dimensionality Reduction)" href="factor-analysis.html" />
    <link rel="prev" title="17. The Ethics of Predictive Models in Sociotechnical Systems" href="ethics-of-predictive-models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Syllabus</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="goals-and-expectations.html">Goals and Expectations</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="policies.html">Course Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="skills-check.html">Skills Check</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule &amp; Office Hours</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prior-and-posterior.html">21. Bayesian Inference: Prior and Posterior</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictive.html">22. Bayesian Inference: Posterior Predictive</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human/AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Special Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/gmms.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussian Mixture Models (Clustering)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-variable-models-lvms">18.1. Latent Variable Models (LVMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-law-of-total-probability-discrete">18.2. The Law of Total Probability (Discrete)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-gmms">18.3. MLE for GMMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gmms">18.4. Multivariate GMMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-gmms-to-data">18.5. Fitting GMMs to Data</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-mixture-models-clustering">
<h1><span class="section-number">18. </span>Gaussian Mixture Models (Clustering)<a class="headerlink" href="#gaussian-mixture-models-clustering" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> Sometimes our data contains hidden structure—structure that we’d like to uncover in order to answer some scientific question. For example, recall the data set we analyzed in the unit on continuous probability from IHH’s Center for Telekinesis Research (CTR). The researchers at the IHH’s CTR study the propensity of intergalactic beings for telekinesis—the ability. They were interested in understanding how different physiological conditions affect a being’s telekinetic abilities. That is, they observed each patient’s telekinetic-ability and wanted to understand how it related to some underlying condition (allergic reaction, intoxication, and entangled antennas). In their specific case, their data did contain the underlying condition. However, often times, our data doesn’t contain this information. In such cases, our goal is to <em>uncover</em> the underlying types of patients. Doing so may help identify patients that benefit from different treatments. For example, in addition to each patient’s underlying physiological condition, their telekinetic ability could have been impacted by environmental factors growing up, their genetics, etc. It’s hard to know a priori which of these factors are truly important, so its not worth investing in collecting all this data (which is expensive).</p>
<p><strong>Challenge:</strong> But how can we possibly uncover a variable that’s not in the data? By making assumptions about the distribution of this variable, as well as how it relates to the other variables in the data, we can! In statistical lingo, such <em>unobserved</em> variables are called <em>latent</em> variables. As we will show here, there’s only one rule of probability we need to learn in order to use our existing toolkit to model latent variables.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce latent variable models, as well as our first latent variable model (LVM)—the Gaussian Mixture Model (GMM)</p></li>
<li><p>Introduce the law of total probability (in the descrete case), which will allow us to compute the MLE for LVMs</p></li>
<li><p>Compute the MLE for the GMM</p></li>
<li><p>Implement a GMM in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></p></li>
</ul>
<p><strong>Data:</strong> We will start by modeling the data introduced in the chapter on continuous probability. The data includes two variables—the patient’s telekinetic ability, and their underlying condition. We will <em>pretend</em> that we did not observe their underlying condition. Our goal will then be to <em>infer</em> it given their telekinetic ability. Let’s remind ourselves what the data looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a bunch of libraries we&#39;ll be using below</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the data into a pandas dataframe</span>
<span class="n">csv_fname</span> <span class="o">=</span> <span class="s1">&#39;data/IHH-CTR.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_fname</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Patient ID&#39;</span><span class="p">)</span>

<span class="c1"># Print a random sample of patients, just to see what&#39;s in the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Condition</th>
      <th>Telekinetic-Ability</th>
    </tr>
    <tr>
      <th>Patient ID</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>398</th>
      <td>Allergic Reaction</td>
      <td>0.510423</td>
    </tr>
    <tr>
      <th>3833</th>
      <td>Allergic Reaction</td>
      <td>0.479960</td>
    </tr>
    <tr>
      <th>4836</th>
      <td>Intoxication</td>
      <td>2.043218</td>
    </tr>
    <tr>
      <th>4572</th>
      <td>Allergic Reaction</td>
      <td>-0.443333</td>
    </tr>
    <tr>
      <th>636</th>
      <td>Intoxication</td>
      <td>1.423190</td>
    </tr>
    <tr>
      <th>2545</th>
      <td>Intoxication</td>
      <td>1.392568</td>
    </tr>
    <tr>
      <th>1161</th>
      <td>Intoxication</td>
      <td>2.110151</td>
    </tr>
    <tr>
      <th>2230</th>
      <td>Intoxication</td>
      <td>2.102866</td>
    </tr>
    <tr>
      <th>148</th>
      <td>Intoxication</td>
      <td>1.865081</td>
    </tr>
    <tr>
      <th>2530</th>
      <td>Allergic Reaction</td>
      <td>0.401414</td>
    </tr>
    <tr>
      <th>4070</th>
      <td>Intoxication</td>
      <td>2.271342</td>
    </tr>
    <tr>
      <th>1261</th>
      <td>Allergic Reaction</td>
      <td>-0.455159</td>
    </tr>
    <tr>
      <th>4682</th>
      <td>Entangled Antennas</td>
      <td>-1.713834</td>
    </tr>
    <tr>
      <th>333</th>
      <td>Intoxication</td>
      <td>2.000120</td>
    </tr>
    <tr>
      <th>906</th>
      <td>Intoxication</td>
      <td>1.693633</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="latent-variable-models-lvms">
<h2><span class="section-number">18.1. </span>Latent Variable Models (LVMs)<a class="headerlink" href="#latent-variable-models-lvms" title="Permalink to this heading">#</a></h2>
<p><strong>Overview.</strong> Latent variable models allow us to model variables we actually did not observe. We will do this using the very same toolkit we’ve used so far; we’ll write down a joint distribution for all variables—observed and latent—as well as a directed graphical model. We will then perform MLE on the resultant model. We will instantiate everything with a specific model—Gaussian Mixture Model (GMM)—which will help us find patient types in the above IHH data.</p>
<p><strong>Gaussian Mixture Models (GMMs).</strong> In our IHH example, we assume the patients’ underlying type in some way “explains” their observed data. We can encode this into a model by saying that each patient’s data is generated by:</p>
<ol class="arabic">
<li><p>Sampling their <em>patient type</em> from some distribution. We’ll call the latent type <span class="math notranslate nohighlight">\(z\)</span>, and assume it’s drawn from a Categorical distribution with parameter <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f29d3a58-9b6d-4e29-89ae-35660dcf5482">
<span class="eqno">(18.1)<a class="headerlink" href="#equation-f29d3a58-9b6d-4e29-89ae-35660dcf5482" title="Permalink to this equation">#</a></span>\[\begin{align}
    z_n &amp;\sim \mathrm{Cat}(\pi)
    \end{align}\]</div>
</li>
<li><p>Given the type, we can now sample the <em>observed data</em>, <span class="math notranslate nohighlight">\(x\)</span>. As the name suggests, we’ll set this distribution to be a Gaussian. The Gaussian’s mean and variance will be selected by <span class="math notranslate nohighlight">\(z_n\)</span>. By this we mean that if the patient has underlying type 1 (i.e. <span class="math notranslate nohighlight">\(z_n = 1\)</span>), then their observed data is sampled from <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_1, \sigma^2_1)\)</span>. Similarly, if they have underlying type 2, their observed data is sampled from <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_2, \sigma^2_2)\)</span>. Putting this together, we have:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7c6abdac-3322-43c0-8035-a8dbf39668f7">
<span class="eqno">(18.2)<a class="headerlink" href="#equation-7c6abdac-3322-43c0-8035-a8dbf39668f7" title="Permalink to this equation">#</a></span>\[\begin{align}
    x_n | z_n &amp;\sim \mathcal{N}(\mu_{z_n}, \sigma^2_{z_n})
    \end{align}\]</div>
</li>
</ol>
<p>For 1-dimensional <span class="math notranslate nohighlight">\(x\)</span>, the final data-generating process is then:</p>
<div class="amsmath math notranslate nohighlight" id="equation-00b4a571-f279-41d8-8849-ff685eebb61c">
<span class="eqno">(18.3)<a class="headerlink" href="#equation-00b4a571-f279-41d8-8849-ff685eebb61c" title="Permalink to this equation">#</a></span>\[\begin{align}
z_n &amp;\sim p_Z(\cdot; \pi) = \mathrm{Cat}(\pi) \quad (\text{mixture})\\
x_n | z_n &amp;\sim p_{X | Z}(\cdot | x_n; \mu_0, \dots, \mu_{K-1}, \sigma_0, \dots, \sigma_{K-1}) = \mathcal{N}(\mu_{z_n}, \sigma^2_{z_n}) \quad (\text{components})
\end{align}\]</div>
<p>The distribution over the latent variable is often called the “mixture,” and each Gaussian is called a “component” in the mixture. From here on, we’ll use <span class="math notranslate nohighlight">\(\theta = \{ \pi, \mu_0, \dots, \mu_{K-1}, \sigma_0, \dots, \sigma_{K-1}\}\)</span> to refer to the model’s parameters.</p>
<p><strong>Directed Graphical Model.</strong> Graphically, we can depict a GMM as follows:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIs1vP9i0&#x2F;-IWykjjF-dWy5DOBnqfudA&#x2F;view?embed">
    </iframe>
  </div>
</div>
<p>As you can see, our observed data, <span class="math notranslate nohighlight">\(x_n\)</span>, depends on the latent patient type, <span class="math notranslate nohighlight">\(z_n\)</span>. Since <span class="math notranslate nohighlight">\(z_n\)</span> is not observed, <em>its circle is left white</em> (i.e. not shaded in).</p>
<p><strong>What are GMMs useful for?</strong> To better understand what GMMs are useful for, let’s visualize them. Here’s an example GMM:</p>
<figure class="align-center" id="fig-gmm-1d">
<img alt="_images/example_1d_gmm.png" src="_images/example_1d_gmm.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.1 </span><span class="caption-text">The PDF of a GMM’s mixture components (left) and data marginal (right).</span><a class="headerlink" href="#fig-gmm-1d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>On the left, you can see the GMM’s mixture components (i.e. each Gaussian), and on the right, you can see the probability of the <em>observed</em> data, <span class="math notranslate nohighlight">\(p_X(\cdot; \theta)\)</span>. Looking at the above figure, you can see two things:</p>
<ol class="arabic simple">
<li><p><em>Clustering.</em> Looking at the left plot, you can see that GMMs can “cluster” the observed data; every observation likely belongs to one of three Gaussians—we just need to figure out which observation belongs to which cluster.</p></li>
<li><p><em>Complicated Distributions.</em> Looking at the right plot, you can see that GMMs can describe more complicated distributions. Unlikely the continuous distributions we’ve used so far, which all have one mode (or one “bump”), using a GMM we can easily describe a distribution with multiple bumps.</p></li>
</ol>
<p><strong>Challenges Deriving the MLE for GMMs.</strong> Now that we have our directed graphical model and our data generating process, we can try to derive the MLE for GMMs. Unfortunately, as you will see, we’ll run into some issues. Then our joint data likelihood (which we’d like to maximize) is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2d81dd09-b4cb-4d4e-9078-8cd86b9f8337">
<span class="eqno">(18.4)<a class="headerlink" href="#equation-2d81dd09-b4cb-4d4e-9078-8cd86b9f8337" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}; \theta) &amp;= \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \\
&amp;= \prod\limits_{n=1}^N p_X(x_n; \theta)
\end{align}\]</div>
<p>Looking at the above, what is <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span>? Our data-generating process gives us the following joint distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3118783a-47c2-48f2-bd23-e191866bd826">
<span class="eqno">(18.5)<a class="headerlink" href="#equation-3118783a-47c2-48f2-bd23-e191866bd826" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{X | Z}(x_n, z_n; \theta) &amp;= p_{X | Z}(x_n | z_n; \theta) \cdot p_Z(z_n; \theta)
\end{align}\]</div>
<p>Somehow, we need to compute <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span> from <span class="math notranslate nohighlight">\(p_{X | Z}(x_n, z_n; \theta)\)</span>.</p>
<p>As we will show next, we can compute <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9c20f84b-3f7b-4539-b8d8-1c3af6d261c6">
<span class="eqno">(18.6)<a class="headerlink" href="#equation-9c20f84b-3f7b-4539-b8d8-1c3af6d261c6" title="Permalink to this equation">#</a></span>\[\begin{align}
p_X(x_n; \theta) &amp;= \sum\limits_{z_n \in S} p_{X, Z}(x_n, z_n; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(S = \{0, \dots, K - 1 \}\)</span> is the support of <span class="math notranslate nohighlight">\(Z\)</span>, and <span class="math notranslate nohighlight">\(K\)</span> is the number of clusters. This formula shows that we can compute <span class="math notranslate nohighlight">\(p_X(x_n; \theta)\)</span> by summing the joint over every value of <span class="math notranslate nohighlight">\(z_n\)</span>.</p>
</section>
<section id="the-law-of-total-probability-discrete">
<h2><span class="section-number">18.2. </span>The Law of Total Probability (Discrete)<a class="headerlink" href="#the-law-of-total-probability-discrete" title="Permalink to this heading">#</a></h2>
<p><strong>Definition.</strong> The law of total probability tells us how to compute a marginal distribution from a joint distribution. Suppose you have two random variables, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, and suppose that <span class="math notranslate nohighlight">\(A\)</span> is discrete with support <span class="math notranslate nohighlight">\(S\)</span>. Then the law of total probability says we can compute the marginal <span class="math notranslate nohighlight">\(p_B(b)\)</span> from the joint <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ff2aba52-6c47-4d47-8a83-5a2a9cf53e60">
<span class="eqno">(18.7)<a class="headerlink" href="#equation-ff2aba52-6c47-4d47-8a83-5a2a9cf53e60" title="Permalink to this equation">#</a></span>\[\begin{align}
p_B(b) &amp;= \sum\limits_{a \in S} p_{A, B}(a, b)
\end{align}\]</div>
<p>We can also write it using expectations by factorizing the joint distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e39a78a2-2c71-4ebf-9f87-8fbdc87b8b20">
<span class="eqno">(18.8)<a class="headerlink" href="#equation-e39a78a2-2c71-4ebf-9f87-8fbdc87b8b20" title="Permalink to this equation">#</a></span>\[\begin{align}
p_B(b) &amp;= \sum\limits_{a \in S} p_{A, B}(a, b) \\
&amp;= \sum\limits_{a \in S} p_{B | A}(b | a) \cdot p_A(a) \\
&amp;= \mathbb{E}_{a \sim p_A(\cdot)} \left[ p_{B | A}(b | a) \right] \\
\end{align}\]</div>
<p>The law of total probability says that the probability of <span class="math notranslate nohighlight">\(B\)</span> is that of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A = a\)</span>, averaged over all values of <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p><strong>Lingo.</strong> Since saying “law of total probability” is a mouthful, we typically say “marginalize out.” So in the above example, we <em>marginalized out</em> <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>Intuition.</strong> To get intuition, let’s depict <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as follows, each with support <span class="math notranslate nohighlight">\(S = \{0, 1\}\)</span>:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdzJ3f3k&#x2F;PhWElQiQSXIIPklC6ED39w&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>In this diagram, each shaded area represents the probability of an event—i.e. area is proportional to probability. The marginal probability of <span class="math notranslate nohighlight">\(B = 1\)</span> is therefore the ratio of the blue square relative to the whole space (the gray square):</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdw0ybko&#x2F;3Aap1TJZMrHHD8kPFhfEzQ&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>Using the law of total probability, we can equivalently compute the marginal probability <span class="math notranslate nohighlight">\(p_B(1)\)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b89d43d8-c314-4172-9690-fd03b5e56cac">
<span class="eqno">(18.9)<a class="headerlink" href="#equation-b89d43d8-c314-4172-9690-fd03b5e56cac" title="Permalink to this equation">#</a></span>\[\begin{align}
p_B(1) &amp;= \sum\limits_{a \in S} p_{A, B}(a, 1) \\
&amp;= p_{A, B}(0, 1) + p_{A, B}(1, 1) \\
\end{align}\]</div>
<p>Re-writing this equation visually, we get:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdzAtxTY&#x2F;406MzXJ_IH4tJuHLwoYFEg&#x2F;view?embed">
  </iframe>
</div>
</div><p>As you can see from the diagram, this formula holds. Now let’s add to our pictoral intuition by assigning meaning to <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Suppose <span class="math notranslate nohighlight">\(B = 1\)</span> is the event in which a patient has pneumonia (<span class="math notranslate nohighlight">\(B = 0\)</span> implies they don’t have pneumonia), and suppose that <span class="math notranslate nohighlight">\(A = 1\)</span> is the event of rain. We can attribute meaning to the law of total probability as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8252fd91-781f-448b-b057-1ddf96de903c">
<span class="eqno">(18.10)<a class="headerlink" href="#equation-8252fd91-781f-448b-b057-1ddf96de903c" title="Permalink to this equation">#</a></span>\[\begin{align}
\underbrace{p_B(1)}_{\text{has pneumonia}} &amp;= \underbrace{p_{A, B}(0, 1)}_{\text{has pneumonia and no rain}} + \underbrace{p_{A, B}(1, 1)}_{\text{has pneumonia and rain}}
\end{align}\]</div>
<p>As you can see, the formula aggregates the probability of a patient having pneumonia across all possible scenarios—rain or no rain.</p>
</section>
<section id="mle-for-gmms">
<h2><span class="section-number">18.3. </span>MLE for GMMs<a class="headerlink" href="#mle-for-gmms" title="Permalink to this heading">#</a></h2>
<p><strong>MLE Objective.</strong> Using the law of total probability, we can now write our MLE objective:</p>
<div class="amsmath math notranslate nohighlight" id="equation-24ab7856-00b7-494c-a252-d8e41d8ade3a">
<span class="eqno">(18.11)<a class="headerlink" href="#equation-24ab7856-00b7-494c-a252-d8e41d8ade3a" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_\theta \log p(\mathcal{D}; \theta) \\
&amp;= \mathrm{argmax}_\theta \log \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log p(\mathcal{D}_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log p_X(x_n; \theta) \\
&amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log \sum\limits_{z_n \in S} p_{X, Z}(x_n, z_n; \theta) \\
\end{align}\]</div>
<p><strong>Optimization.</strong> We can optimize the GMM’s MLE objective using gradient descent, as we’ve done for every other model so far. However, note that in practice, there’s a much better algorithm to perform this optimization. This algorithm is called <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener noreferrer" target="_blank">Expectation Maximization (EM)</a>, and it converges faster and better avoids local optima. We will unfortunately not get into the details of this algorithm.</p>
<div class="admonition-exercise-law-of-total-probability admonition">
<p class="admonition-title">Exercise: Law of Total Probability</p>
<p><strong>Part 1:</strong> Suppose you have three random variables, <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span>, with supports <span class="math notranslate nohighlight">\(S_A\)</span>, <span class="math notranslate nohighlight">\(S_B\)</span>, and <span class="math notranslate nohighlight">\(S_C\)</span>. Using the law of total probability, how would you compute the following probabilities?</p>
<p>a. You are given a distribution, <span class="math notranslate nohighlight">\(p_{A, B, C}(a, b, c)\)</span>. Compute <span class="math notranslate nohighlight">\(p_{A, B | C}(a, b | c)\)</span>.</p>
<p>b. You are given a distribution, <span class="math notranslate nohighlight">\(p_{A, B, C}(a, b, c)\)</span>. Compute <span class="math notranslate nohighlight">\(p_{A | B, C}(a | b, c)\)</span>.</p>
<p>c. You are given a distribution, <span class="math notranslate nohighlight">\(p_{A, B | C}(a, b | c)\)</span>. Compute <span class="math notranslate nohighlight">\(p_{A | B, C}(a | b, c)\)</span>.</p>
<p><strong>Part 2:</strong> Consider the following graphical model, in which the support of <span class="math notranslate nohighlight">\(z_{n, m}\)</span> is <span class="math notranslate nohighlight">\(S = \{ 0 , 1 \}\)</span>.</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMeBRCjYo&#x2F;drP9yh18Xe8SP3N23hfB9g&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>a. Which variable here is a latent variable, and how do you know?</p>
<p>b. Write down the MLE objective when <span class="math notranslate nohighlight">\(M = 2\)</span>, <span class="math notranslate nohighlight">\(M = 3\)</span> and <span class="math notranslate nohighlight">\(M = 4\)</span>.</p>
<p>c. In general, how many elements need to be summed over in the MLE objective (as a function of <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(M\)</span>)?</p>
</div>
</section>
<section id="multivariate-gmms">
<h2><span class="section-number">18.4. </span>Multivariate GMMs<a class="headerlink" href="#multivariate-gmms" title="Permalink to this heading">#</a></h2>
<p>When given low dimensional data, we can often inspect it (by visualizing it) to determine what types of hidden structures (e.g. clusters) exist. However, in higher dimensional data, this is no longer possible. In our GMM model, above, we’ve used 1-dimensional Gaussian distributions (for one-dimensional observed data). To work with higher dimensional data, we can use a <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" rel="noopener noreferrer" target="_blank"><em>multivariate</em> Gaussian distribution</a>. A multivariate Gaussian allows us to write down a single formula for a collection of random variables, whose <em>joint</em> distribution is Gaussian.</p>
<p>A multivariate Gaussian requires two parameters: a mean, <span class="math notranslate nohighlight">\(\mu\)</span>, and a covariance, <span class="math notranslate nohighlight">\(\Sigma\)</span>. The mean plays the same role as it does in the univariate case—it is the center of the distribution. For example, suppose we have a 2-dimensional multivariate Gaussian over random variables <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. And suppose their mean,</p>
<div class="amsmath math notranslate nohighlight" id="equation-e19c7691-4402-42e5-b3f2-de717a7c8116">
<span class="eqno">(18.12)<a class="headerlink" href="#equation-e19c7691-4402-42e5-b3f2-de717a7c8116" title="Permalink to this equation">#</a></span>\[\begin{align}
\mu &amp;= \begin{bmatrix} -1.0 \\ 1.0 \end{bmatrix},
\end{align}\]</div>
<p>then the distribution of <span class="math notranslate nohighlight">\(X_1\)</span> will be centered at <span class="math notranslate nohighlight">\(-1.0\)</span> and the distribution of <span class="math notranslate nohighlight">\(X_2\)</span> will be centered at <span class="math notranslate nohighlight">\(1.0\)</span>.</p>
<p>Next, the covariance plays the same role as the variance (or standard deviation)—it tells us about the spread of the distribution around its mean. In the multivariate case, you can think of the covariance as telling us how pairs of variables correlate. For example,</p>
<div class="amsmath math notranslate nohighlight" id="equation-65438ecb-8c62-4617-84e3-d354c8c145a2">
<span class="eqno">(18.13)<a class="headerlink" href="#equation-65438ecb-8c62-4617-84e3-d354c8c145a2" title="Permalink to this equation">#</a></span>\[\begin{align}
\Sigma &amp;= \begin{bmatrix} 2.0 &amp; 0.9 \\ 0.9 &amp; 1.0 \end{bmatrix}
\end{align}\]</div>
<p>The values along the diagonal, <span class="math notranslate nohighlight">\(2.0\)</span> and <span class="math notranslate nohighlight">\(1.0\)</span>, tell us about the variance (or spread) of <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>, respectively. Specifically, they tell us that <span class="math notranslate nohighlight">\(X_1\)</span> is more spread out than <span class="math notranslate nohighlight">\(X_2\)</span>. The values in the off-diagonal tell us about how the variables correlate. A value of <span class="math notranslate nohighlight">\(0.9\)</span> indicates <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> correlate more strongly than a value of <span class="math notranslate nohighlight">\(0.0\)</span> (which indicates no correlation, i.e. independence). Note that <span class="math notranslate nohighlight">\(\Sigma\)</span> needs to satisfy certain properties; for example, it needs to be <em>symmetric</em> (since <span class="math notranslate nohighlight">\(X_1\)</span> covaries with <span class="math notranslate nohighlight">\(X_2\)</span> as much as <span class="math notranslate nohighlight">\(X_2\)</span> covaries with <span class="math notranslate nohighlight">\(X_1\)</span>).</p>
<p>Let’s get more intuition by creating some multivariate Gaussians and visualizing their samples. But first, let’s import stuff!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jrandom</span>
<span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">D</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will visualize samples from a 2-dimensional multivariate Gaussian with the above mean and covariance:</p>
<div class="amsmath math notranslate nohighlight" id="equation-be85f154-d07b-4b03-bab6-2dc373905cff">
<span class="eqno">(18.14)<a class="headerlink" href="#equation-be85f154-d07b-4b03-bab6-2dc373905cff" title="Permalink to this equation">#</a></span>\[\begin{align}
\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} 
\sim \mathcal{N}\left( 
\begin{bmatrix} -1.0 \\ 1.0 \end{bmatrix},
\begin{bmatrix} 2.0 &amp; 0.9 \\ 0.9 &amp; 1.0 \end{bmatrix}
\right)
\end{align}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define mean and covariance</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="c1"># Create a multivariate gaussian</span>
<span class="n">mvn</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>

<span class="c1"># Sample</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">mvn</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot the joint distribution of X_1 and X_2</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Joint of $X_1$ and $X_2$&#39;</span><span class="p">)</span>

<span class="c1"># Plot the distribution of X_1</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Marginal of $X_1$&#39;</span><span class="p">)</span>

<span class="c1"># Plot the distribution of X_2</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Marginal of $X_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5ed8ea47a8d0014ca23232f449d19a0d631126546721b96499227fc1f5c045a0.png" src="_images/5ed8ea47a8d0014ca23232f449d19a0d631126546721b96499227fc1f5c045a0.png" />
</div>
</div>
<p>To extend GMMs to multivariate data, we can replace the univariate Gaussian components in our model with multivariate Gaussians. Using the MLE, we can learn the parameters, <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span>, of each Gaussian.</p>
</section>
<section id="fitting-gmms-to-data">
<h2><span class="section-number">18.5. </span>Fitting GMMs to Data<a class="headerlink" href="#fitting-gmms-to-data" title="Permalink to this heading">#</a></h2>
<p><strong>GMMs in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</strong> To fit a GMM using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, we have to explicitly marginalize out the discrete latent variable. While it’s possible to do this, we find that it tends to be a little finicky. As a result, we will show you how to write the model in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> instead of implementing it yourself. We’ll then have you play with an existing implementation to gain intuition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">univariate_gmm</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># Parameter for the mixing distribution</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;pi&#39;</span><span class="p">,</span>
        <span class="n">init_value</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">K</span><span class="p">),</span>
        <span class="n">constaint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">simplex</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Parameter for the means of the Gaussian components</span>
    <span class="c1"># For optimization purposes, we initialize the means to some random samples</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;mu&#39;</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,)),</span> 
        <span class="n">constaint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Parameter for the standard deviations of the Gaussian components</span>
    <span class="c1"># For optimization purposes, we initialize the means to some random samples</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;mu&#39;</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,))</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">,</span> 
        <span class="n">constaint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
    <span class="p">)</span>    

    <span class="c1"># We create a plate for the number of observations</span>
    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="c1"># For each observation, we first sample its &quot;cluster&quot; or &quot;type&quot;</span>
        <span class="n">p_z</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">p_z</span><span class="p">)</span>

        <span class="c1"># Using the cluster, we choose a Gaussian component to sample from</span>
        <span class="n">p_x_given_z</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="n">std_dev</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">p_x_given_z</span><span class="p">,</span> <span class="n">obs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>As you can see, this model is a direct translation of our data generating process. Notice that the line that samples <code class="docutils literal notranslate"><span class="pre">z</span></code> does not have a keyword argument <code class="docutils literal notranslate"><span class="pre">obs=</span></code>. This is because we have not observed <code class="docutils literal notranslate"><span class="pre">z</span></code> (it’s latent).</p>
<div class="admonition-exercise-fitting-gmms-to-data admonition">
<p class="admonition-title">Exercise: Fitting GMMs to Data</p>
<p><strong>Part 1:</strong> We’ll use an existing implementation of GMMs from <code class="docutils literal notranslate"><span class="pre">Scikit-Learn</span></code>. You can import the model as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
</pre></div>
</div>
<p>Then, following the example in their <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" rel="noopener noreferrer" target="_blank">documentation</a>, fit the data to the IHH data set above.</p>
<p><strong>Part 2:</strong> Visualize the IHH’s data and samples generated by the fitted model. Do they look the same? If not, figure out what went wrong and retry!</p>
<p><strong>Part 3:</strong> Using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method, determine the most likely “cluster” for each patient. Then compare against the ground-truth <code class="docutils literal notranslate"><span class="pre">Condition</span></code> in the data. How well do the two match? If they do not match perfectly, why?</p>
<p><strong>Part 4:</strong> Unfortunately, an unknown epidemic hit the IHH hard this week. Clinicians at the IHH’s Center for Epidemiology have been working around the clock to try to understand the nature of the disease. They have collected data about patient symptoms—fever and heart-rate—that they were hoping would help them see if there are any underlying patient types. They have obtained an initial sample of data (already normalized): <code class="docutils literal notranslate"><span class="pre">'data/IHH-CE-clustering.csv'</span></code>. Fit a GMM to the data and visualize the model against the data. You can visualize the data by scatter plotting it, with color corresponding to the predicted cluster. How many types of patients did you identify? How do you know?</p>
<p><strong>Part 4:</strong> Clinicians at the IHH’s sister hospital have been trying to understand a similar epidemic that hit their hospital. They have collected a similar data set (already normalized) and have asked you to investigate it: <code class="docutils literal notranslate"><span class="pre">'data/IHH-sister-CE-clustering.csv'</span></code>. Like before, fit a GMM to the data and visualize the model against the data. How many types of patients did you identify? How do you know?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ethics-of-predictive-models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">17. </span>The Ethics of Predictive Models in Sociotechnical Systems</p>
      </div>
    </a>
    <a class="right-next"
       href="factor-analysis.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Factor Analysis (Dimensionality Reduction)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-variable-models-lvms">18.1. Latent Variable Models (LVMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-law-of-total-probability-discrete">18.2. The Law of Total Probability (Discrete)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-gmms">18.3. MLE for GMMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gmms">18.4. Multivariate GMMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-gmms-to-data">18.5. Fitting GMMs to Data</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024 Yaniv Yacoby
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>