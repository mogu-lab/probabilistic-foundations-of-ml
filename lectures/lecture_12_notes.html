

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>7. Lecture #12: Logistic Regression and Gradient Descent &#8212; Introduction to Probabilistic Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_12_notes';</script>
    <link rel="canonical" href="https://ml-collaboratory.github.io/intro-to-prob-ml/lectures/lecture_12_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing" href="lecture_13_notes.html" />
    <link rel="prev" title="3. Lecture #11: Hierarchical Models" href="lecture_11_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction to Probabilistic Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ml-collaboratory/intro-to-prob-ml/blob/master/lectures/lecture_12_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fml-collaboratory%2Fintro-to-prob-ml%2Fblob%2Fmaster%2Flectures/lecture_12_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_12_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #12: Logistic Regression and Gradient Descent</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">7. Lecture #12: Logistic Regression and Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">7.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">7.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">8. Logistic Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss-revisited-modeling-a-bernoulli-variable-with-covariates">8.1. Coin-Toss Revisited: Modeling a Bernoulli Variable with Covariates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">8.2. The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-logistic-regression-and-classification">8.3. The Relationship Between Logistic Regression and Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-classification">What Is Classification?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-classify">How Do We Classify?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-a-logistic-regression-model">8.4. Interpreting a Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#with-great-explanatory-power-comes-great-responsibility">8.5. With Great Explanatory Power Comes Great Responsibility!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-not-to-use-sensitive-protected-attributes">When not to use sensitive/protected attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-you-might-want-to-use-sensitive-protected-attributes">When you might want to use sensitive/protected attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#appropriate-usage-of-sensitive-protected-attributes">Appropriate usage of sensitive/protected attributes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-a-logistic-regression-model-log-odds">8.6. Interpreting a Logistic Regression Model: Log-Odds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-the-logistic-regression-log-likelihood">8.7. Maximizing the Logistic Regression Log-likelihood</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">9. Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-as-directional-information">9.1. Gradient as Directional Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuition-for-gradient-descent">9.2. An Intuition for Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-the-algorithm">9.3. Gradient Descent: the Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-design-choices-with-the-trajectory">9.4. Diagnosing Design Choices with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-issues-with-the-trajectory">9.5. Diagnosing Issues with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">9.6. Diagnosing Issues with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-step-size-matters">9.7. Gradient Descent: Step Size Matters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-for-logistic-regression">9.8. Gradient Descent for Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-did-we-optimize-it">9.9. But Did We Optimize It?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization">10. Convex Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets">10.1. Convex Sets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">10.2. Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-function-first-order-condition">10.3. Convex Function: First Order Condition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-function-second-order-condition">10.4. Convex Function: Second Order Condition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-convex-functions">10.5. Properties of Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">10.6. Convex Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-of-the-logistic-regression-negative-log-likelihood">10.7. Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-does-it-always-converge">10.8. But Does It Always Converge?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-how-quickly-can-we-get-there">10.9. But How Quickly Can We Get There?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-does-it-scale">10.10. But Does It Scale?</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-12-logistic-regression-and-gradient-descent">
<h1><span class="section-number">7. </span>Lecture #12: Logistic Regression and Gradient Descent<a class="headerlink" href="#lecture-12-logistic-regression-and-gradient-descent" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2><span class="section-number">7.1. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1">### Import basic libraries</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;autograd&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h2><span class="section-number">7.2. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Logistic Regression</p></li>
<li><p>Gradient Descent</p></li>
<li><p>Convex Optimization</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="logistic-regression">
<h1><span class="section-number">8. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h1>
<section id="coin-toss-revisited-modeling-a-bernoulli-variable-with-covariates">
<h2><span class="section-number">8.1. </span>Coin-Toss Revisited: Modeling a Bernoulli Variable with Covariates<a class="headerlink" href="#coin-toss-revisited-modeling-a-bernoulli-variable-with-covariates" title="Permalink to this heading">#</a></h2>
<p>Let’s revisit our model for coin-toss: we’d assumed that the outcomes <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> were independently and identically distributed as Bernoulli’s, <span class="math notranslate nohighlight">\(Y^{(n)} \sim Ber(\theta)\)</span>. Today, we will re-examine the <em><strong>identical</strong></em> part of the modeling assumptions.</p>
<p>Realistically, the probability of <span class="math notranslate nohighlight">\(Y^{(n)} = 1\)</span> depends on variables like force, angle, spin etc.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)} \in \mathbb{R}^D\)</span> be <span class="math notranslate nohighlight">\(D\)</span> number of such measurements of the <span class="math notranslate nohighlight">\(n\)</span>-th toss. We model the probability of <span class="math notranslate nohighlight">\(Y^{(n)} = 1\)</span> as a function of these <em><strong>covariates</strong></em> <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Y^{(n)} \sim Ber\left(\mathrm{sigm}\left(f\left(\mathbf{X}^{(n)}; \mathbf{w}\right)\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> are the parameters of <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(\mathrm{sigm}\)</span> is the sigmoid function <span class="math notranslate nohighlight">\(\mathrm{sigm}(z) = \frac{1}{1 + e^{-z}}\)</span>.</p>
<p><strong>Note:</strong> we need the sigmoid function to transform an arbitrary real number <span class="math notranslate nohighlight">\(f\left(\mathbf{X}^{(n)}; \mathbf{w}\right)\)</span> into a probability (i.e. a number in <span class="math notranslate nohighlight">\([0, 1]\)</span>).</p>
</section>
<section id="the-logistic-regression-model">
<h2><span class="section-number">8.2. </span>The Logistic Regression Model<a class="headerlink" href="#the-logistic-regression-model" title="Permalink to this heading">#</a></h2>
<p>Given a set of <span class="math notranslate nohighlight">\(N\)</span> observations <span class="math notranslate nohighlight">\((\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(N)}, y^{(N)})\)</span>. We assume the following model for the data.</p>
<div class="math notranslate nohighlight">
\[
Y^{(n)} \sim Ber\left(\mathrm{sigm}\left(f\left(\mathbf{X}^{(n)}; \mathbf{w}\right)\right)\right).
\]</div>
<p>This is called the <em><strong>logistic regression</strong></em> model.</p>
<p>Fitting this model on the data means <em><strong>inferring</strong></em> the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that best aligns with the observations.</p>
<p>Once we have inferred the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, given a new set of covariates <span class="math notranslate nohighlight">\(\mathbf{x}^{\text{new}}\)</span>, we can <strong>predict</strong> the probability of <span class="math notranslate nohighlight">\(\mathbf{Y}^{\text{new}} = 1\)</span> by computing</p>
<div class="math notranslate nohighlight">
\[\mathrm{sigm}\left(f\left(\mathbf{X}^{(n)}; \mathbf{w}\right)\right).\]</div>
<p>For now, we will assume that <span class="math notranslate nohighlight">\(f\)</span> is a linear function:</p>
<div class="math notranslate nohighlight">
\[
f\left(\mathbf{X}^{(n)}; \mathbf{w}\right) = \mathbf{w}^\top \mathbf{X}^{(n)}.
\]</div>
</section>
<section id="the-relationship-between-logistic-regression-and-classification">
<h2><span class="section-number">8.3. </span>The Relationship Between Logistic Regression and Classification<a class="headerlink" href="#the-relationship-between-logistic-regression-and-classification" title="Permalink to this heading">#</a></h2>
<section id="what-is-classification">
<h3>What Is Classification?<a class="headerlink" href="#what-is-classification" title="Permalink to this heading">#</a></h3>
<p>A regression problem is where we predict a <strong>quantitative</strong> outcome <span class="math notranslate nohighlight">\(y\)</span> based on some covariates <span class="math notranslate nohighlight">\(x\)</span>.</p>
<ul class="simple">
<li><p><strong>Example:</strong> Given the number of room, the number of allowed guests and the neighborhood, predict price of an Airbnb listing.</p></li>
</ul>
<p>A <em><strong>classification</strong></em> problem is a <strong>categorical</strong> outcome <span class="math notranslate nohighlight">\(y\)</span> based on some covariates <span class="math notranslate nohighlight">\(x\)</span>.</p>
<ul class="simple">
<li><p><strong>Example:</strong> Given the age, education and income level of a loan applicant, predict whether or not the loan application will be approved.</p></li>
</ul>
</section>
<section id="how-do-we-classify">
<h3>How Do We Classify?<a class="headerlink" href="#how-do-we-classify" title="Permalink to this heading">#</a></h3>
<p>The data for classification consists of a set of tuples <span class="math notranslate nohighlight">\((\mathbf{x}^{(n)}, y^{(n)})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\in \mathbb{R}^D\)</span> is a set of predictors/covariates (e.g. age, income etc) for the <span class="math notranslate nohighlight">\(n\)</span>-th observation and <span class="math notranslate nohighlight">\(y^{(n)}\)</span> is the binary label (e.g. did the person receive the loan). The set of observations that are labeled <span class="math notranslate nohighlight">\(y=0\)</span> are called class 0 and the set labeled <span class="math notranslate nohighlight">\(y=1\)</span> are called class 1.</p>
<p>The goal is learn a function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> so that given a new set of covariates <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> we can predict the correpsonding label <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Suppose our data has a small number of predictors, by visualizing the data we can intuitively check how easy it is to separate the classes. In the following example, we have two covariates. We scatter plot the covariates (pictured as crosses and bars), we then indicate data belonging to class 1 by visualizing them as blue crosses and those belonging to class 0 as red bars.</p>
<img src="fig/fig0.png" alt="" style="height: 200px;"/>
<p>Ideally, the classes are easily separated by a curve (or surface) in the input space, this curve (or surface) is called the <em><strong>decision boundary</strong></em>.</p>
<p>When the decision boundary is linear (i.e. a line or a plane), it is defined by the equation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^\top \mathbf{x} = w_0x_0 + w_1x_1 + \ldots + w_D x_D = 0
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_0 = 1\)</span>. Often we write <span class="math notranslate nohighlight">\(b = w_0x_0\)</span> and we call <span class="math notranslate nohighlight">\(b\)</span> the <em><strong>bias</strong></em> term.</p>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> allow us to gauge the ‘distance’ of a point from the decision boundary. That is, the magnitude of the inner product <span class="math notranslate nohighlight">\(|\mathbf{w}^\top \mathbf{x}|\)</span> gives us the distance between the point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the decision boundary, whereas the sign of <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}\)</span> gives us the side of the decision boundary on which the point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> lies. The proof of this is short and geometric – you should try to work it out!</p>
<p><em>(In the picture below, they use <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}\)</span> to express <span class="math notranslate nohighlight">\(w_1x_1 + \ldots + w_D x_D + b\)</span>, whereas we pretend that <span class="math notranslate nohighlight">\(x_0 = 1\)</span> and <span class="math notranslate nohighlight">\(b = w_0x_0\)</span>. This is just a difference in notation)</em>
<img src="fig/fig1.png" alt="" style="height: 200px;"/></p>
<p>Once we have a decision boundary should we always classify points on one side of the boundary as 1 and points on the other side as 0?</p>
<img src="fig/fig2.png" alt="" style="height: 200px;"/>
<p>For classes that are not linearly separable (the classes don’t fall cleanly along two sides of a linear decision boundary), we want to make “soft classifications”. That is, rather than declaring <span class="math notranslate nohighlight">\(y=1\)</span> when <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x} &gt; 0\)</span> and <span class="math notranslate nohighlight">\(y=0\)</span> when <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x} &lt;0\)</span>, we want to encode uncertainty in our prediction of the label <span class="math notranslate nohighlight">\(y\)</span>. We want to say the following:</p>
<blockquote>
<div><p>When <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is very far from the decision boundary, i.e. when <span class="math notranslate nohighlight">\(|\mathbf{w}^\top \mathbf{x}|\)</span> is very large, and when <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}&gt;0\)</span> then we want to say <span class="math notranslate nohighlight">\(y=1\)</span> with <strong>high probability</strong> (analogously, when <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is very far from the decision boundary and <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}&lt;0\)</span>, we want to say <span class="math notranslate nohighlight">\(y=0\)</span> with high probability). When <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is very close from the decision boundary, i.e. when <span class="math notranslate nohighlight">\(|\mathbf{w}^\top \mathbf{x}|\approx 0\)</span>, we want to be maximally uncertain about how predict <span class="math notranslate nohighlight">\(y\)</span>, i.e. we predict <span class="math notranslate nohighlight">\(y\)</span> randomly by a 50-50 chance.</p>
</div></blockquote>
<p>Formalizing this: to model the <strong>probability of labeling a point a certain class</strong>, we have to convert distance, <span class="math notranslate nohighlight">\(\mathbf{w}^\top\mathbf{x}\)</span> (which is unbounded) into a number (probability) between 0 and 1, using the <em><strong>sigmoid function</strong></em>:
$<span class="math notranslate nohighlight">\(
\text{Prob}(y = 1 | \mathbf{x}) = \text{sigm}(\underbrace{\mathbf{w}^\top\mathbf{x}}_{\text{distance}})
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\text{sigm}(z) = \frac{1}{1 + e^{-z}}<span class="math notranslate nohighlight">\(. This way, when \)</span>\mathbf{w}^\top\mathbf{x}<span class="math notranslate nohighlight">\( is large and positive then \)</span>\text{sigm}(\underbrace{\mathbf{w}^\top\mathbf{x}}<em>{\text{distance}})<span class="math notranslate nohighlight">\( is close to 1, when \)</span>\mathbf{w}^\top\mathbf{x}<span class="math notranslate nohighlight">\( is large and negative then \)</span>\text{sigm}(\underbrace{\mathbf{w}^\top\mathbf{x}}</em>{\text{distance}})<span class="math notranslate nohighlight">\( is close to 0; when \)</span>\mathbf{w}^\top\mathbf{x}<span class="math notranslate nohighlight">\( is near 0 then \)</span>\text{sigm}(\underbrace{\mathbf{w}^\top\mathbf{x}}_{\text{distance}})<span class="math notranslate nohighlight">\( is close to 0.5, i.e. \)</span>\text{Prob}(y = 1 | \mathbf{x}) = 0.5$. This is exactly what we wanted!</p>
</section>
</section>
<section id="interpreting-a-logistic-regression-model">
<h2><span class="section-number">8.4. </span>Interpreting a Logistic Regression Model<a class="headerlink" href="#interpreting-a-logistic-regression-model" title="Permalink to this heading">#</a></h2>
<p>Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three covariates:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x_1</span></code> representing gender: 0 for male, 1 for female</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_2</span></code> for the income</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_3</span></code> for the loan amount</p></li>
</ol>
<p>Suppose that the parameters you found are:
$<span class="math notranslate nohighlight">\(
p(y=1 | x_1, x_2, x_3) = \mathrm{sigm}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3).
\)</span>$</p>
<p>What are the parameters telling us about the most influential attribute for predicting loan approval? What does this say about our data?</p>
</section>
<section id="with-great-explanatory-power-comes-great-responsibility">
<h2><span class="section-number">8.5. </span>With Great Explanatory Power Comes Great Responsibility!<a class="headerlink" href="#with-great-explanatory-power-comes-great-responsibility" title="Permalink to this heading">#</a></h2>
<p>So in the previous predictve model for loan application model we’d use the follwing covariates:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x_1</span></code> representing gender: 0 for male, 1 for female</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_2</span></code> for the income</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_3</span></code> for the loan amount</p></li>
</ol>
<p>We were pleased that using covariates we can hypothesize <strong>why</strong> the outcome <span class="math notranslate nohighlight">\(y\)</span> is a certain way - why is the loan approved or denied. But just because we can collect the data for the covariates does it mean that we should use them? It particular, in our logistic model on the previous slide, the most impactful factor on a loan decision was gender, does this seem reasonable? Should we use this model in real-life loan decisions?</p>
<p>In models that are built on human data, covariates containing information that infringes on the rights of the subject to remain anonymous or to keep potentially non-relevant and biasing information out of the decision making process are called <strong>sensitive</strong> or <strong>protected attributes</strong>.</p>
<section id="when-not-to-use-sensitive-protected-attributes">
<h3>When not to use sensitive/protected attributes<a class="headerlink" href="#when-not-to-use-sensitive-protected-attributes" title="Permalink to this heading">#</a></h3>
<p>Gender, in the case of loan decisions, is a protected attribute, since under the <a class="reference external" href="https://www.consumerfinance.gov/fair-lending/">Equal Credit Opportunity Act</a> makes it illegal for a creditor to discriminate in any aspect of credit transaction based on certain characteristics including gender. Thus, any decision process using gender to inform a loan decision can be potentially considered discriminatory.</p>
<p>In fact, most credit models used in industry are trained on data that are stripped of protected attributes - meaning that these models never see covariates like gender during training or deployment!</p>
<p>If we had deployed the logistic model to make real life loan decisions, not only would our decisions be potentially unfair and possibly questionable in terms of financial soundness (since our decisions are heavily influenced by a covariate not directly related to the financial qualifications of a loan applicant), using such a model exposes our company to regulatory sanctions and law-suits.</p>
</section>
<section id="when-you-might-want-to-use-sensitive-protected-attributes">
<h3>When you might want to use sensitive/protected attributes<a class="headerlink" href="#when-you-might-want-to-use-sensitive-protected-attributes" title="Permalink to this heading">#</a></h3>
<p>So should we never collect data on protected attributes? Well, unfortunately, just because we are blind to protected attributes it does not mean that our decisions are fair with respect to these attributes! That is, just because we don’t see gender when making decisions, it does not mean that our decisions impact men and women equally. In fact, the Equal Credit Opportunity Act contains explicit language that protects consumers from the <a class="reference external" href="https://www.americanbanker.com/opinion/dont-ditch-disparate-impact">disparate impact</a> of credit decision systems. Disparate impact is defined as the unequal impact of a credit policy on a legally protected class:</p>
<blockquote>
<div><p>“A lender’s policies, even when applied equally to all its credit applicants, may have a negative effect on certain applicants. For example, a lender may have a policy of not making single family home loans for less than $60,000. This policy might exclude a high number of applicants who have lower income levels or lower home values than the rest of the applicant pool. That uneven effect of the policy is called disparate impact.”</p>
</div></blockquote>
<p>How do modelers and engineers prevent their models from creating disparate impact for protected classes of people? One of the key ways to check for disparate impact is to compare model decisions on protected classes against model decisions against the population (for example, we can compare the percentages of loans the model approves for men and for women). But in order perform these checks, we need access to protected attributes!</p>
<p>Often times, the models we are auditing for disparate impact are “black-boxes”, that is, the inner workings of the model (as well as details like the training procedure) are all proprietary information and we only have access to the models inputs and outputs, <span class="math notranslate nohighlight">\((x_2, x_3, y)\)</span> (note that the black-box model is not using the protected attribute gender <span class="math notranslate nohighlight">\(x_1\)</span>). In these cases, we can train a proxy model on covariates including protected attributes as well as the predictions of the black-box model, and then analyze our proxy model as an approximation to the black-box model. If we suppose that our logistic regression model in the previous slide was a proxy model that is trained to approximate a black-box model, then our proxy model is telling us that the black-box model’s decisions are highly correlated with gender and hence it’s decisions may cause disparate impact! This is a case where we needed to look at protected attributes in order to check for regulartory compliance!</p>
<p>So why is this happening - why is a black-box model that is trained on data stripped of gender information making decisions that are highly correlated with gender? There are many possible explanations for this, but one common reason for this is that gender is a <strong>confounding variable</strong>, that is, some combination of income and loan amount is secretly encoding for gender (e.g. let’s say that in your data set, all the men earn 50k-60k and apply for exactly $10,000 of loans) and the black-box model is <em><strong>implicitly</strong></em> relying on gender to make loan decisions.</p>
</section>
<section id="appropriate-usage-of-sensitive-protected-attributes">
<h3>Appropriate usage of sensitive/protected attributes<a class="headerlink" href="#appropriate-usage-of-sensitive-protected-attributes" title="Permalink to this heading">#</a></h3>
<p>So now we see that protected attributes can be extremely useful when we are auditing models for regulatory compliance, should we collect sensitive or protected attributes all the time and as many of them as possible? Unfortunately, it’s not so simple: some industries are required by law to collect sensitive attribute data, while others are prohibited from doing so, still others infer sensitive attributes from collected data for compliance checking purposes (e.g. using income and other covariates to infer gender or zip codes to infer race). While useful for antidiscrimnatory purposes, collecting or infering sensitive protected attribute is nonetheless full of challenges and potential pitfalls (for example, protected attributes you infered can violate the subject’s right to non-disclosure!) - references in this section addresses the issues of appropriate usage of sensitive attributes.</p>
<p>Furthermore, <strong>how</strong> you collect and ecnode ata can deeply impact your fairness/compliance analysis. For example, application forms that include two options for gender and three boxes for race may cause us to incorrectly aggregate subjects who do not fall neatly into these boxes under categories that are inappropriate (and by the way, we’ve not been carefully distinguishing gender and sex in this discussion, but they maybe treated differently under different bodies of laws. Also, are they equivalent concepts to our human subjects from whom we are collecting the data? What are we really trying to measure by collecting gender or sex data?). Such misleading categorization can cause us to miss cases of disparate impact in our analysis, since we have simply missed out on important subgroups of the population.</p>
<p>If you feel like we’re now treading in the unfamiliar territories of social science or law, you are not mistaken! When we work with human data, the interpretation of the data and hence our analysis hinges on our ability to meaningfully understand the data in social contexts and in human terms. Just as we need to consult physicist domain experts when we are building a statistical model for physical data, medical experts when we are building models for medical data, when we build models that have social impact we have to proceed carefully under the advisement of experts on our social/legal systems.</p>
<p><strong>References for Issues Involving Proper Use of Protected Attributes</strong>
0. <a href="https://journals.sagepub.com/doi/10.1177/0891243215584758">New Categories Are Not Enough: Rethinking the Measurement of Sex and Gender in Social Surveys</a></p>
<ol class="arabic simple">
<li><p><a href="https://www.pewsocialtrends.org/2015/11/06/chapter-1-estimates-of-multiracial-adults-and-other-racial-and-ethnic-groups-across-various-question-formats/"> Estimates of Multiracial Adults and Other Racial and Ethnic Groups Across Various Question Formats</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1806.03281.pdf">Blind Justice: Fairness with Encrypted Sensitive Attributes</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1912.06171.pdf">Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination</a></p></li>
</ol>
</section>
</section>
<section id="interpreting-a-logistic-regression-model-log-odds">
<h2><span class="section-number">8.6. </span>Interpreting a Logistic Regression Model: Log-Odds<a class="headerlink" href="#interpreting-a-logistic-regression-model-log-odds" title="Permalink to this heading">#</a></h2>
<p>A more formal way of interpreting the parameters of the logistic regression model is through the <em><strong>log-odds</strong></em>. That is, we solve for <span class="math notranslate nohighlight">\(\mathbf{w}^\top\mathbf{x}\)</span> in terms of <span class="math notranslate nohighlight">\(\text{Prob}(y = 1 | \mathbf{x})\)</span>.</p>
<p>\begin{aligned}
\text{Prob}(y = 1 | \mathbf{x}) &amp;= \text{sigm}(\mathbf{w}^\top\mathbf{x})\
\text{sigm}^{-1}(\text{Prob}(y = 1 | \mathbf{x})) &amp;= \mathbf{w}^\top\mathbf{x}\
\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{1 - \text{Prob}(y = 1 | \mathbf{x})}\right)&amp;= \mathbf{w}^\top\mathbf{x}\
\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\right)&amp;= \mathbf{w}^\top\mathbf{x}
\end{aligned}
where we used the fact that <span class="math notranslate nohighlight">\(\text{sigm}^{-1}(z) = \frac{z}{1 - z}\)</span>.</p>
<p>The term <span class="math notranslate nohighlight">\(\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\right)\)</span> is essentially a ratio of the probability of <span class="math notranslate nohighlight">\(y=1\)</span> and the probability of <span class="math notranslate nohighlight">\(y=0\)</span>, we can interpret this quantity as the (log) odds of you winning if you’d bet that <span class="math notranslate nohighlight">\(y=1\)</span>. Thus, we can imagine the parameter <span class="math notranslate nohighlight">\(w_d\)</span> for the covariate <span class="math notranslate nohighlight">\(x_d\)</span> as telling us if the odd of winning a bet on <span class="math notranslate nohighlight">\(y=1\)</span> is good.</p>
<ol class="arabic simple">
<li><p>if <span class="math notranslate nohighlight">\(w_d &lt; 0\)</span>, then by increasing <span class="math notranslate nohighlight">\(x_d\)</span> (while holding all other covariates constant) we make <span class="math notranslate nohighlight">\(\mathbf{w}^\top\mathbf{x}\)</span> more negative, and hence the ratio <span class="math notranslate nohighlight">\(\frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\)</span> closer to 0. That is, when <span class="math notranslate nohighlight">\(w_d &lt; 0\)</span>, increasing <span class="math notranslate nohighlight">\(x_d\)</span> decreases our odds.<br><br></p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_d &gt; 0\)</span>, then by increasing <span class="math notranslate nohighlight">\(x_d\)</span> (while holding all other covariates constant) we make <span class="math notranslate nohighlight">\(\mathbf{w}^\top\mathbf{x}\)</span> more positive, and hence the ratio <span class="math notranslate nohighlight">\(\frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\)</span> larger. That is, when <span class="math notranslate nohighlight">\(w_d &gt; 0\)</span>, increasing <span class="math notranslate nohighlight">\(x_d\)</span> decreases our odds.</p></li>
</ol>
</section>
<section id="maximizing-the-logistic-regression-log-likelihood">
<h2><span class="section-number">8.7. </span>Maximizing the Logistic Regression Log-likelihood<a class="headerlink" href="#maximizing-the-logistic-regression-log-likelihood" title="Permalink to this heading">#</a></h2>
<p>Given a set of <span class="math notranslate nohighlight">\(N\)</span> observations <span class="math notranslate nohighlight">\((\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(N)}, y^{(N)})\)</span>. We want to find <span class="math notranslate nohighlight">\(\mathbf{w}_{\text{MLE}}\)</span> that maximizes the log (joint) likelihood:</p>
<p>\begin{aligned}
\small
\mathbf{w}<em>{\text{MLE}} =&amp; \underset{\mathbf{w}}{\mathrm{argmax}};\ell(\mathbf{w}) \equiv \underset{\mathbf{w}}{\mathrm{argmin}};-\ell(\mathbf{w}) =\underset{\mathbf{w}}{\mathrm{argmin}}-\log \prod</em>{n=1}^N p(y^{(n)} | \mathbf{x}^{(n)}) \
=&amp; \underset{\mathbf{w}}{\mathrm{argmin}}\sum_{n=1}^N -\log\left( \mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)})^{y^{(n)}}(1 -\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}))^{1 - y^{(n)}}   \right) \
=&amp; \underset{\mathbf{w}}{\mathrm{argmin}}\sum_{n=1}^N -y^{(n)},\log,\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}) \
&amp;- (1 - y^{(n)}) \log (1 -\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}))
\end{aligned}</p>
<p>Optimizing the likelihood requires us to find the stationary points of the gradient of <span class="math notranslate nohighlight">\(\ell(\mathbf{w})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{w}} \ell(\mathbf{w}) = -\sum_{n=1}^N \left(y^{(n)} - \frac{1}{1 + e^{-\mathbf{w}^\top\mathbf{x}^{(n)}}} \right) \mathbf{x}^{(n)} =\mathbf{0}
\]</div>
<p>Can we solve for <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gradient-descent">
<h1><span class="section-number">9. </span>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this heading">#</a></h1>
<section id="gradient-as-directional-information">
<h2><span class="section-number">9.1. </span>Gradient as Directional Information<a class="headerlink" href="#gradient-as-directional-information" title="Permalink to this heading">#</a></h2>
<p>The gradient is orthogonal to the level curve of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x^*\)</span> and hence, <em>when it is not zero</em>, points in the direction of the greatest instantaneous increase in <span class="math notranslate nohighlight">\(f\)</span>.
<img src="fig/levelcurves.jpg" style="height:300px;"></p>
</section>
<section id="an-intuition-for-gradient-descent">
<h2><span class="section-number">9.2. </span>An Intuition for Gradient Descent<a class="headerlink" href="#an-intuition-for-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>The intuition behind various flavours of gradient descent  is as follows:</p>
<img src="./fig/fig9.pdf" style='height:400px;'></section>
<section id="gradient-descent-the-algorithm">
<h2><span class="section-number">9.3. </span>Gradient Descent: the Algorithm<a class="headerlink" href="#gradient-descent-the-algorithm" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>start at random place: <span class="math notranslate nohighlight">\(\mathbf{w}^{(0)}\leftarrow \textbf{random}\)</span></p></li>
<li><p>until (stopping condition satisfied):</p></li>
</ol>
<p>a. compute gradient at <span class="math notranslate nohighlight">\(\mathbf{w}^{(t)}\)</span>:
gradient (<span class="math notranslate nohighlight">\(\mathbf{w}^{(t)}\)</span>) = <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}}\)</span> loss_function (<span class="math notranslate nohighlight">\(\mathbf{w}^{(t)}\)</span>)</p>
<p>b. take a step in the negative gradient direction:
<span class="math notranslate nohighlight">\(\mathbf{w}^{(t+1)} \leftarrow \mathbf{w}^{(t)}\)</span> - <span class="math notranslate nohighlight">\(\eta\)</span> * gradient (<span class="math notranslate nohighlight">\(\mathbf{w}^{(t)}\)</span>)</p>
<p>Here <span class="math notranslate nohighlight">\(\eta\)</span> is called the <em><strong>learning rate</strong></em>.</p>
</section>
<section id="diagnosing-design-choices-with-the-trajectory">
<h2><span class="section-number">9.4. </span>Diagnosing Design Choices with the Trajectory<a class="headerlink" href="#diagnosing-design-choices-with-the-trajectory" title="Permalink to this heading">#</a></h2>
<p>If this is your objective function during training, what can you conclude about your step-size?
<img src="./fig/fig13.png" style='height:400px;'></p>
</section>
<section id="diagnosing-issues-with-the-trajectory">
<h2><span class="section-number">9.5. </span>Diagnosing Issues with the Trajectory<a class="headerlink" href="#diagnosing-issues-with-the-trajectory" title="Permalink to this heading">#</a></h2>
<p>If this is your objective function during training, what can you conclude about your step-size?
<img src="./fig/fig14.png" style='height:400px;'></p>
</section>
<section id="id1">
<h2><span class="section-number">9.6. </span>Diagnosing Issues with the Trajectory<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>If this is your objective function during training, what can you conclude about your step-size?
<img src="./fig/fig15.png" style='height:400px;'></p>
</section>
<section id="gradient-descent-step-size-matters">
<h2><span class="section-number">9.7. </span>Gradient Descent: Step Size Matters<a class="headerlink" href="#gradient-descent-step-size-matters" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig10.jpg" style='height:400px;'></section>
<section id="gradient-descent-for-logistic-regression">
<h2><span class="section-number">9.8. </span>Gradient Descent for Logistic Regression<a class="headerlink" href="#gradient-descent-for-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>When diagnosing our gradient descent learning, we can:</p>
<ol class="arabic simple">
<li><p>Visualize the log-likelihood. <strong>What does this tell us?</strong></p></li>
<li><p>Visualize the norm of the gradients. <strong>What does this tell us?</strong></p></li>
</ol>
<p>What else should we visualize to check that our learned model aligns with the data?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate 2d classification dataset</span>
<span class="n">centers</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))))</span>

<span class="c1">#define the sigmoid function</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="c1">#define the negative log-likelihood</span>
<span class="n">nll</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span>  <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">))))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1">#define logistic regression MLE inference by gradient descent</span>
<span class="k">def</span> <span class="nf">logistic_regression_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">total_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="c1">#implement the gradient</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="o">-</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)))[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1">#initialize parameters w</span>
    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">w_current</span> <span class="o">=</span> <span class="n">init</span>
    <span class="c1">#store the negative log-likelihood</span>
    <span class="n">nlls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1">#store the norm of the gradients </span>
    <span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1">#initialize the difference between current and new parameters</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1">#gradient descent </span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">total_iterations</span> <span class="ow">and</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="c1">#compute the gradient</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span>
        <span class="c1">#update parameter by taking gradient step</span>
        <span class="n">w_next</span> <span class="o">=</span> <span class="n">w_current</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="c1">#compute the difference between current parameters and new parameters</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_next</span> <span class="o">-</span> <span class="n">w_current</span><span class="p">)</span>
        <span class="c1">#compute the norm of the gradient</span>
        <span class="n">grad_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>
        <span class="c1">#compute the negative log-likelihood</span>
        <span class="n">nlls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nll</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_current</span><span class="p">))</span>
        
        <span class="n">w_current</span> <span class="o">=</span> <span class="n">w_next</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        
    <span class="k">return</span> <span class="n">w_current</span><span class="p">,</span> <span class="n">nlls</span><span class="p">,</span> <span class="n">grad_norms</span>

<span class="k">def</span> <span class="nf">plot_diagnostics</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">nlls</span><span class="p">,</span> <span class="n">grad_norms</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nlls</span><span class="p">)),</span> <span class="n">nlls</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;negative log-likelihood over iterations of gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;negative log-likelihood&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iterations of gradient descent&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">)),</span> <span class="n">grad_norms</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;norms of gradients over iterations of gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;norm of gradient&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iterations of gradient descent&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>

<span class="n">w_MLE</span><span class="p">,</span> <span class="n">nlls</span><span class="p">,</span> <span class="n">grad_norms</span> <span class="o">=</span> <span class="n">logistic_regression_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">total_iterations</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_diagnostics</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">nlls</span><span class="p">,</span> <span class="n">grad_norms</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a75b6c54a7cd03fe9c2637c28f89810f5353c9b572d42d16c0093a09ed48390d.png" src="../_images/a75b6c54a7cd03fe9c2637c28f89810f5353c9b572d42d16c0093a09ed48390d.png" />
</div>
</div>
</section>
<section id="but-did-we-optimize-it">
<h2><span class="section-number">9.9. </span>But Did We Optimize It?<a class="headerlink" href="#but-did-we-optimize-it" title="Permalink to this heading">#</a></h2>
<img src="fig/optima.jpg" style="height:450px;"></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convex-optimization">
<h1><span class="section-number">10. </span>Convex Optimization<a class="headerlink" href="#convex-optimization" title="Permalink to this heading">#</a></h1>
<section id="convex-sets">
<h2><span class="section-number">10.1. </span>Convex Sets<a class="headerlink" href="#convex-sets" title="Permalink to this heading">#</a></h2>
<p>A <em><strong>convex set</strong></em> <span class="math notranslate nohighlight">\(S\subset \mathbb{R}^D\)</span> is a set that contains the line segment between any two points in <span class="math notranslate nohighlight">\(S\)</span>. Formally, if <span class="math notranslate nohighlight">\(x, y \in S\)</span> then <span class="math notranslate nohighlight">\(S\)</span> contains all convex combinations of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
tx + (1-t) y \in S,\quad t\in [0, 1].
\]</div>
<img src="./fig/convexset.jpg" style='height:250px;'></section>
<section id="convex-functions">
<h2><span class="section-number">10.2. </span>Convex Functions<a class="headerlink" href="#convex-functions" title="Permalink to this heading">#</a></h2>
<p>A function <span class="math notranslate nohighlight">\(f\)</span> is a <em><strong>convex function</strong></em> if domain of <span class="math notranslate nohighlight">\(f\)</span> is a convex set, and the line segment between the points <span class="math notranslate nohighlight">\((x, f(x))\)</span> and <span class="math notranslate nohighlight">\((y, f(y))\)</span> lie above the graph of <span class="math notranslate nohighlight">\(f\)</span>. Formally, for any <span class="math notranslate nohighlight">\(x, y\in \mathrm{dom}(f)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\underbrace{f(tx + (1-t)y)}_{\text{height of graph of $f$ at a point between $x$ and $y$}} \quad \leq \underbrace{tf(x) + (1-t)f(y)}_{\text{height of point on line segment between $(x, f(x))$ and $(y, f(y))$}},\quad t\in [0, 1]
\]</div>
<img src="./fig/convex.jpg" style='height:300px;'></section>
<section id="convex-function-first-order-condition">
<h2><span class="section-number">10.3. </span>Convex Function: First Order Condition<a class="headerlink" href="#convex-function-first-order-condition" title="Permalink to this heading">#</a></h2>
<p>How do we check that a function <span class="math notranslate nohighlight">\(f\)</span> is convex? If <span class="math notranslate nohighlight">\(f\)</span> is differentiable then <span class="math notranslate nohighlight">\(f\)</span> is convex if the graph of <span class="math notranslate nohighlight">\(f\)</span> lies above every tangent plane.</p>
<p><strong>Theorem:</strong> If <span class="math notranslate nohighlight">\(f\)</span> is differentiable then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if for every <span class="math notranslate nohighlight">\(x \in \mathrm{dom}(f)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\underbrace{f(y)}_{\text{height of graph of $f$ over $y$}} \geq \underbrace{f(x) + \nabla f(x)^\top (y - x)}_{\text{height of plane tangent to $f$ at $x$, evaluated over $y$}},\quad \forall y\in \mathrm{dom}(f)
\]</div>
<img src="./fig/convex_first_order.jpg" style='height:250px;'></section>
<section id="convex-function-second-order-condition">
<h2><span class="section-number">10.4. </span>Convex Function: Second Order Condition<a class="headerlink" href="#convex-function-second-order-condition" title="Permalink to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is twice-differentiable then <span class="math notranslate nohighlight">\(f\)</span> is convex if the “second derivative is positive”.</p>
<p><strong>Theorem:</strong> If <span class="math notranslate nohighlight">\(f\)</span> is twice-differentiable then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if the Hessian <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is positive semi-definite for every <span class="math notranslate nohighlight">\(x\in \mathrm{dom}(f)\)</span>.</p>
<img src="./fig/convex_nonconvex.jpg" style='height:300px;'></section>
<section id="properties-of-convex-functions">
<h2><span class="section-number">10.5. </span>Properties of Convex Functions<a class="headerlink" href="#properties-of-convex-functions" title="Permalink to this heading">#</a></h2>
<p>How to build complex convex functions from simple convex functions:</p>
<ol class="arabic simple">
<li><p>if <span class="math notranslate nohighlight">\(w_1, w_2 \geq 0\)</span> and <span class="math notranslate nohighlight">\(f_1, f_2\)</span> are convex, then <span class="math notranslate nohighlight">\(h = w_1 f_1 + w_2 f_2\)</span> is convex<br><br></p></li>
<li><p>if <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex, and <span class="math notranslate nohighlight">\(g\)</span> is univariate and non-decreasing then <span class="math notranslate nohighlight">\(h = g \circ f\)</span> is convex<br><br></p></li>
<li><p>Log-sum-exp functions are convex: <span class="math notranslate nohighlight">\(f(x) = \log \sum_{k=1}^K e^{x}\)</span></p></li>
</ol>
<p><strong>Note:</strong> there are many other convexity preserving operations on functions.</p>
</section>
<section id="id2">
<h2><span class="section-number">10.6. </span>Convex Optimization<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>A <em><strong>convex optimization problem</strong></em> is an optimization of the following form:</p>
<p>\begin{aligned}
\mathrm{min}; &amp;f(x) &amp; (\text{convex objective function})\
\text{subject to}; &amp; h_i(x) \leq 0, i=1, \ldots, i &amp; (\text{convex inequality constraints}) \
&amp; a_j^\top x - b_j = 0, j=1, \ldots, J &amp; (\text{affine equality constraints}) \
\end{aligned}</p>
<p>The set of points that satisfy the constraints is called the <em><strong>feasible set</strong></em>.</p>
<p>You can prove that the a convex optimization problem optimizes a convex objective function over a convex feasible set. But why should we care about convex optimization problems?</p>
<p><strong>Theorem:</strong> Let <span class="math notranslate nohighlight">\(f\)</span> be a convex function defined over a convex feasible set <span class="math notranslate nohighlight">\(\Omega\)</span>. Then if <span class="math notranslate nohighlight">\(f\)</span> has a local minimum at <span class="math notranslate nohighlight">\(x\in \Omega\)</span> – <span class="math notranslate nohighlight">\(f(y) \geq f(x)\)</span> for <span class="math notranslate nohighlight">\(y\)</span> in a small neighbourhood of <span class="math notranslate nohighlight">\(x\)</span> – then <span class="math notranslate nohighlight">\(f\)</span> has a global minimum at <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p><strong>Corollary:</strong> Let <span class="math notranslate nohighlight">\(f\)</span> be a differentiable convex function:</p>
<ol class="arabic simple">
<li><p>if <span class="math notranslate nohighlight">\(f\)</span> is unconstrained, then <span class="math notranslate nohighlight">\(f\)</span> has a <strong>local minimum</strong> and hence <strong>global minimum</strong> at <span class="math notranslate nohighlight">\(x\)</span> if <span class="math notranslate nohighlight">\(\nabla f(x) = 0\)</span>.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(f\)</span> is constrained by equalities, then <span class="math notranslate nohighlight">\(f\)</span> has a global minimum at <span class="math notranslate nohighlight">\(x\)</span> if <span class="math notranslate nohighlight">\(\nabla J(x, \lambda) = 0\)</span>, where <span class="math notranslate nohighlight">\(J(x, \lambda)\)</span> is the Lagrangian of the constrained optimization problem.</p></li>
</ol>
<p><strong>Note:</strong> we can also characterize the global minimum of inequalities constrained convex optimization problems using the Lagrangian, but the formulation is more complicated.</p>
</section>
<section id="convexity-of-the-logistic-regression-negative-log-likelihood">
<h2><span class="section-number">10.7. </span>Convexity of the Logistic Regression Negative Log-Likelihood<a class="headerlink" href="#convexity-of-the-logistic-regression-negative-log-likelihood" title="Permalink to this heading">#</a></h2>
<p>But why do we care about convex optimization problems? Let’s connect the theory of convex optimization to MLE inference for logistic regression. Recall that the negative log-likelihood of the logistic regression model is</p>
<p>\begin{aligned}
-\ell(\mathbf{w}) &amp;= -\sum_{n=1}^N y^{(n)},\log,\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}) + (1 - y^{(n)}) \log (1 -\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}))\
&amp;=\sum_{n=1}^N y^{(n)} \log(e^0 + e^{\mathbf{w}^\top \mathbf{x}^{(n)}}) + (1 - y^{(n)})(- \mathbf{w}^\top \mathbf{x}^{(n)})
\end{aligned}</p>
<p><strong>Proposition:</strong> The negative log-likelihood of logistic regression <span class="math notranslate nohighlight">\(-\ell(\mathbf{w})\)</span> is convex.</p>
<p><strong>What does this mean for gradient descent?</strong> If gradient descent finds that <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span> is a stationary point of <span class="math notranslate nohighlight">\(-\nabla_{\mathbf{w}}\ell(\mathbf{w})\)</span> then <span class="math notranslate nohighlight">\(-\ell(\mathbf{w})\)</span> has a global minimum at <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span>. Hence, <span class="math notranslate nohighlight">\(\ell(\mathbf{w})\)</span> is maximized at <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span>.</p>
<p><em><strong>Proof of the Proposition:</strong></em> Note that</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(- \mathbf{w}^\top \mathbf{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\((1 - y^{(n)})(- \mathbf{w}^\top \mathbf{x}^{(n)})\)</span> are convex, since they are linear</p></li>
<li><p><span class="math notranslate nohighlight">\(\log(e^0 + e^{\mathbf{w}^\top \mathbf{x}^{(n)}})\)</span> is convex since it is the composition of a log-sum-exp function (which is convex) and a convex function <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}^{(n)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{n=1}^N y^{(n)} \log(e^0 + e^{\mathbf{w}^\top \mathbf{x}^{(n)}})\)</span> is convex since it is a nonnegative linear combination of convex functions</p></li>
<li><p><span class="math notranslate nohighlight">\(-\ell(\mathbf{w})\)</span> is convex since it is the sum of two convex functions</p></li>
</ol>
</section>
<section id="but-does-it-always-converge">
<h2><span class="section-number">10.8. </span>But Does It Always Converge?<a class="headerlink" href="#but-does-it-always-converge" title="Permalink to this heading">#</a></h2>
<p>We’ve seen that if we choose the learning rate to be too large (say for example <code class="docutils literal notranslate"><span class="pre">1e10</span></code> then gradient descent can fail to converge even if the function <span class="math notranslate nohighlight">\(f\)</span> is convex. But how large is “too large”. There are two cases to consider</p>
<ol class="arabic simple">
<li><p>You have some prior knowledge about how smooth the function <span class="math notranslate nohighlight">\(f\)</span> is – i.e. how quickly <span class="math notranslate nohighlight">\(f\)</span> can increase or decrease. Then using this you can choose a learning rate that will provably guaratee convergence<br><br></p></li>
<li><p>In most cases, the objective function (like the log-likelihood) may be too complex to reason about. In which case,</p></li>
<li><p>we do a scientific “guess-and-check” to determine the learning rate:
1. we find a learning rate that is large enough to cause gradient descent to diverge
2. we find a leanring rate that is small enough to cause gradient descent to converge too slowly
3. we choose a range of values between the large rate and the small rate and try them all to determine the optimal rate<br><br></p></li>
<li><p>alternatively, we can choose the step-size <span class="math notranslate nohighlight">\(\eta\)</span> adaptively (e.g. when the gradient is large we can set <span class="math notranslate nohighlight">\(\eta\)</span> to be moderate to small and when the gradient is small we can set <span class="math notranslate nohighlight">\(\eta\)</span> to be larger). There are a number of adaptive step-size regimes that you may want to look up and implement for your specific problem.</p></li>
</ol>
<p>The prior knowledge required to choose <span class="math notranslate nohighlight">\(\eta\)</span> for provable convergence is called Lipschitz continuity. If we knew that <span class="math notranslate nohighlight">\(f\)</span> is convex, differentiable and that there is a constant <span class="math notranslate nohighlight">\(L&gt;0\)</span> such that <span class="math notranslate nohighlight">\(\|\nabla f(x) - \nabla f(y)\|_2 \leq L\|x -y\|_2\)</span>, then if we choose a fixed step size to be <span class="math notranslate nohighlight">\(\eta \leq \frac{1}{L}\)</span> then gradient descent <strong>provably</strong> converges to the global minimum of <span class="math notranslate nohighlight">\(f\)</span> as the number of iterations <span class="math notranslate nohighlight">\(N\)</span> goes to infinity. The constant <span class="math notranslate nohighlight">\(L\)</span> is called the <em><strong>Lipschitz constant</strong></em>.</p>
</section>
<section id="but-how-quickly-can-we-get-there">
<h2><span class="section-number">10.9. </span>But How Quickly Can We Get There?<a class="headerlink" href="#but-how-quickly-can-we-get-there" title="Permalink to this heading">#</a></h2>
<p>Just because we know gradient descent will converge it doesn’t mean that it will give us a good enough approximation of the global minimum within our time limit. This is why studying the <em><strong>rate of convergence</strong></em> of gradient descent is extremely important. Again there are two cases to consider</p>
<ol class="arabic simple">
<li><p>You have prior knowledge that <span class="math notranslate nohighlight">\(f\)</span> is convex, differentiable and its Lipschitz constant is <span class="math notranslate nohighlight">\(L\)</span> and suppose that <span class="math notranslate nohighlight">\(f\)</span> has a global minimum at <span class="math notranslate nohighlight">\(x^*\)</span>, then for gradient descent to get within <span class="math notranslate nohighlight">\(\epsilon\)</span> of <span class="math notranslate nohighlight">\(f(x^*)\)</span>, we need <span class="math notranslate nohighlight">\(O(1/\epsilon)\)</span> number of iterations.<br><br></p></li>
<li><p>In most cases, the objective function will fail to be convex and its Lipschitz constant may be too difficult to compute. In this case, we simply stop the gradient descent when the gradient is sufficiently small.</p></li>
</ol>
</section>
<section id="but-does-it-scale">
<h2><span class="section-number">10.10. </span>But Does It Scale?<a class="headerlink" href="#but-does-it-scale" title="Permalink to this heading">#</a></h2>
<p>Gradient is such a simple algorithm that can be applied to <strong>any optimization problem</strong> for which you can compute the gradient of the objective function.</p>
<p><strong>Question:</strong> Does this mean that maximum likelihood inference for statistical models is now an easy task (i.e. just use gradient descent)?</p>
<p>For every likelihood optimization problem, evaluating the gradient at a set of parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> requires evaluating the likelihood of the entire dataset using <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{w}} \ell(\mathbf{w}) = -\sum_{n=1}^N \left(y^{(n)} - \frac{1}{1 + e^{-\mathbf{w}^\top\mathbf{x}^{(n)}}} \right) \mathbf{x}^{(n)} =\mathbf{0}
\]</div>
<p>Imagine if the size of your dataset <span class="math notranslate nohighlight">\(N\)</span> is in the millions. Naively evaluating the gardient <strong>just once</strong> may take up to seconds or minutes, thus running gradient descent until convergence may be unachievable in practice!</p>
<p><strong>Idea:</strong> Maybe we don’t need to use the entire data set to evaluate the gradient during each step of gradient descent. Maybe we can approximate the gradient at <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> well enough with just a subset of the data.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_11_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Lecture #11: Hierarchical Models</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_13_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Lecture #13: Stochastic Gradient Descent and Simulated Annealing</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">7. Lecture #12: Logistic Regression and Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">7.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">7.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">8. Logistic Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss-revisited-modeling-a-bernoulli-variable-with-covariates">8.1. Coin-Toss Revisited: Modeling a Bernoulli Variable with Covariates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">8.2. The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-logistic-regression-and-classification">8.3. The Relationship Between Logistic Regression and Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-classification">What Is Classification?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-classify">How Do We Classify?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-a-logistic-regression-model">8.4. Interpreting a Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#with-great-explanatory-power-comes-great-responsibility">8.5. With Great Explanatory Power Comes Great Responsibility!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-not-to-use-sensitive-protected-attributes">When not to use sensitive/protected attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-you-might-want-to-use-sensitive-protected-attributes">When you might want to use sensitive/protected attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#appropriate-usage-of-sensitive-protected-attributes">Appropriate usage of sensitive/protected attributes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-a-logistic-regression-model-log-odds">8.6. Interpreting a Logistic Regression Model: Log-Odds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-the-logistic-regression-log-likelihood">8.7. Maximizing the Logistic Regression Log-likelihood</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">9. Gradient Descent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-as-directional-information">9.1. Gradient as Directional Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuition-for-gradient-descent">9.2. An Intuition for Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-the-algorithm">9.3. Gradient Descent: the Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-design-choices-with-the-trajectory">9.4. Diagnosing Design Choices with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-issues-with-the-trajectory">9.5. Diagnosing Issues with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">9.6. Diagnosing Issues with the Trajectory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-step-size-matters">9.7. Gradient Descent: Step Size Matters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-for-logistic-regression">9.8. Gradient Descent for Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-did-we-optimize-it">9.9. But Did We Optimize It?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization">10. Convex Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets">10.1. Convex Sets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">10.2. Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-function-first-order-condition">10.3. Convex Function: First Order Condition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-function-second-order-condition">10.4. Convex Function: Second Order Condition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-convex-functions">10.5. Properties of Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">10.6. Convex Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-of-the-logistic-regression-negative-log-likelihood">10.7. Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-does-it-always-converge">10.8. But Does It Always Converge?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-how-quickly-can-we-get-there">10.9. But How Quickly Can We Get There?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-does-it-scale">10.10. But Does It Scale?</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Weiwei Pan and Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>