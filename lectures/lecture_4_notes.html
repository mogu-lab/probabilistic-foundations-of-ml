

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>21. Lecture #4: Bayesian versus Frequentist Inference &#8212; Introduction to Probabilistic Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_4_notes';</script>
    <link rel="canonical" href="https://ml-collaboratory.github.io/intro-to-prob-ml/lectures/lecture_4_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Lecture #5: Sampling for Posterior Simulation" href="lecture_5_notes.html" />
    <link rel="prev" title="14. Lecture #3: Bayesian Modeling" href="lecture_3_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction to Probabilistic Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1 current active"><a class="current reference internal" href="#">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_4_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #4: Bayesian versus Frequentist Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">21. Lecture #4: Bayesian versus Frequentist Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">21.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">21.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-bayesian-modeling">22. Review of Bayesian Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-process">22.1. The Bayesian Modeling Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-update">22.2. Bayesian Update</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">22.3. Model Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-bayesian-inference">22.4. Components of Bayesian Inference</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-conjugate-and-non-conjugate-models">23. Examples of Conjugate and Non-Conjugate Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-variance">23.1. Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-model">The Bayesian Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">23.2. Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-mean">23.3. Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The Bayesian Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">23.4. Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-unknown-mean-and-variance">23.5. Bayesian Model for (Univariate) Gaussian Likelihood with Unknown Mean and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-poisson-likelihood">23.6. Bayesian Model for Poisson Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The Bayesian Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-conjugate-models">23.7. Non-Conjugate Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#connections-with-frequentist-inference">24. Connections with Frequentist Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-estimates-from-the-posterior">24.1. Point Estimates from the Posterior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-estimates-can-be-misleading">24.2. Point Estimates Can Be Misleading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">24.3. Point Estimates Can Be Misleading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-posterior-point-estimates-and-mle">24.4. Comparison of Posterior Point Estimates and MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coin-toss-example-revisited-yet-again">24.5. The Coin Toss Example: Revisited Yet Again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#law-of-large-numbers-for-bayesian-inference">24.6. Law of Large Numbers for Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-comparisons">24.7. Computational Comparisons</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-4-bayesian-versus-frequentist-inference">
<h1><span class="section-number">21. </span>Lecture #4: Bayesian versus Frequentist Inference<a class="headerlink" href="#lecture-4-bayesian-versus-frequentist-inference" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2><span class="section-number">21.1. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h2><span class="section-number">21.2. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Review of Bayesian Modeling</p></li>
<li><p>Examples of Conjugate and Non-Conjugate Models</p></li>
<li><p>Connections to Frequentist Inference</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="review-of-bayesian-modeling">
<h1><span class="section-number">22. </span>Review of Bayesian Modeling<a class="headerlink" href="#review-of-bayesian-modeling" title="Permalink to this heading">#</a></h1>
<section id="the-bayesian-modeling-process">
<h2><span class="section-number">22.1. </span>The Bayesian Modeling Process<a class="headerlink" href="#the-bayesian-modeling-process" title="Permalink to this heading">#</a></h2>
<p>In order to make statements about <span class="math notranslate nohighlight">\(Y\)</span>, the outcome, and <span class="math notranslate nohighlight">\(\theta\)</span>, parameters of the distribution generating the data, we form the joint distribution over both variables and use the various marginals/conditional distributions to reason about <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ol class="arabic simple">
<li><p>we form the <em><strong>joint distribution</strong></em> over both variables <span class="math notranslate nohighlight">\(p(Y, \theta) = p(Y | \theta) p(\theta)\)</span>.</p></li>
<li><p>we can condition on the observed outcome to make inferences about <span class="math notranslate nohighlight">\(\theta\)</span>,
$<span class="math notranslate nohighlight">\(
p(\theta | Y) = \frac{p(Y, \theta)}{p(Y)}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>p(\theta | Y)<span class="math notranslate nohighlight">\( is called the ***posterior distribution*** and \)</span>p(Y)$ is called the <em><strong>evidence</strong></em>.</p></li>
<li><p>before any data is observed, we can simulate data by using our prior
$<span class="math notranslate nohighlight">\(
p(Y^*) = \int_\Theta p(Y^*, \theta) d\theta = \int_\Theta p(Y^* | \theta) p(\theta) d\theta
\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y^<em><span class="math notranslate nohighlight">\( represents new data and \)</span>p(Y^</em>)$ is called the <em><strong>prior predictive</strong></em>.</p></li>
<li><p>after observing data, we can simulate new data simliar to the observed data by using our posterior
$<span class="math notranslate nohighlight">\(
p(Y^*|Y) = \int_\Theta p(Y^*, \theta|Y) d\theta = \int_\Theta p(Y^* | \theta) p(\theta | Y) d\theta
\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y^<em><span class="math notranslate nohighlight">\( represents new data and \)</span>p(Y^</em>|Y)$ is called the <em><strong>posterior predictive</strong></em>.</p></li>
</ol>
</section>
<section id="bayesian-update">
<h2><span class="section-number">22.2. </span>Bayesian Update<a class="headerlink" href="#bayesian-update" title="Permalink to this heading">#</a></h2>
<img src="fig/bayes.jpg" style="height:300px;"></section>
<section id="model-evaluation">
<h2><span class="section-number">22.3. </span>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this heading">#</a></h2>
<p>How do we know that our Bayesian model is a good fit for the data?</p>
<ol class="arabic simple">
<li><p><strong>(Log-likelihood)</strong> We can compute the marginal log-likelihood of the data under our posterior. That is, give a set of test data <span class="math notranslate nohighlight">\(\{y^*_1, \ldots, y^*_M\}\)</span>, compute<br><br>
\begin{align}
\log \prod_{m=1}^M p(\mathbf{y}^<em><em>m | \text{Data}) &amp;= \sum</em>{m=1}^M \log p(\mathbf{y}^</em><em>m | \text{Data}) = \sum</em>{m=1}^M \log \int_\Theta p(\mathbf{y}^*_m | \theta) p(\theta| \text{Data}) d\theta
\end{align}
<br>This is simply called the <strong>log-likelihood</strong> of the data under the Bayesian model.<br><br></p></li>
<li><p><strong>(Posterior Predictive Check)</strong> We can also compare the synthetic data generated from our posterior predictive:</p></li>
<li><p>sample from the posterior <span class="math notranslate nohighlight">\(\theta_s \sim p(\theta | Y)\)</span></p></li>
<li><p>plug the posterior samples into the likelihood, and sample synthetic data from the likelihood <span class="math notranslate nohighlight">\(Y_s \sim p(Y | \theta_s)\)</span>.</p></li>
</ol>
<p>We can then compare the synthetic data from the posterior predictive to the observed data.</p>
</section>
<section id="components-of-bayesian-inference">
<h2><span class="section-number">22.4. </span>Components of Bayesian Inference<a class="headerlink" href="#components-of-bayesian-inference" title="Permalink to this heading">#</a></h2>
<p>We see that in order to evaluate Bayesian models we need to be able to perform two tasks:</p>
<ol class="arabic simple">
<li><p>integration over the posterior (required by log-likelihood)</p></li>
<li><p>sampling from the posterior (required by the posterior predictive check).</p></li>
</ol>
<p>Both requirements becomes easier if know the closed form expression for the posterior, e.g. if the prior is conjugate to the likelihood.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="examples-of-conjugate-and-non-conjugate-models">
<h1><span class="section-number">23. </span>Examples of Conjugate and Non-Conjugate Models<a class="headerlink" href="#examples-of-conjugate-and-non-conjugate-models" title="Permalink to this heading">#</a></h1>
<section id="bayesian-model-for-univariate-gaussian-likelihood-with-known-variance">
<h2><span class="section-number">23.1. </span>Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance<a class="headerlink" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-variance" title="Permalink to this heading">#</a></h2>
<section id="the-bayesian-model">
<h3>The Bayesian Model<a class="headerlink" href="#the-bayesian-model" title="Permalink to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu, \sigma^2)\)</span>, with <span class="math notranslate nohighlight">\(\sigma^2\)</span> known. We place a normal prior on <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\mu\sim\mathcal{N}(m, s^2)\)</span>.</p>
<p><strong>Question:</strong> is our choice of prior appropriate?</p>
</section>
</section>
<section id="id1">
<h2><span class="section-number">23.2. </span>Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p><strong>Inference:</strong> The posterior <span class="math notranslate nohighlight">\(p(\mu|Y)\)</span> is then:
\begin{aligned}
p(\mu | Y) = \frac{p(Y| \mu)p(\mu)}{p(Y)} = \frac{\overbrace{\frac{1}{\sqrt{2\pi \sigma^2}} \mathrm{exp} \left{-\frac{(Y - \mu)^2}{2\sigma^2}\right}}^{\text{likelihood}} \overbrace{\frac{1}{\sqrt{2\pi s^2}} \mathrm{exp} \left{-\frac{(m - \mu)^2}{2s^2}\right}}^{\text{prior}}}{p(Y)}
\end{aligned}</p>
<p>We can simplify the posterior as:
\begin{aligned}
p(\mu | Y) &amp;= const *\frac{\mathrm{exp} \left{ -\frac{s^2(Y - \mu)^2 + \sigma^2(m - \mu)^2}{2s^2\sigma^2}\right}}{p(Y)} \
&amp;= const <em>\mathrm{exp} \left{ \frac{s^2Y^2 + \sigma^2m^2}{\sigma^2 s^2}\right}\mathrm{exp} \left{ -\frac{(s^2 + \sigma^2)\mu^2 - 2(s^2Y + \sigma^2m)\mu}{2s^2\sigma^2}\right}\
&amp;= const</em> \mathrm{exp} \left{ -\frac{\left(\mu - \frac{s^2Y + \sigma^2m}{s^2 + \sigma^2} \right)^2}{2s^2\sigma^2}\right}\quad \text{(Completing the square)}
\end{aligned}
Thus, we see that the posterior is a normal distribution, <span class="math notranslate nohighlight">\(\mathcal{N}\left(\frac{s^2Y + \sigma^2m}{s^2 + \sigma^2}, s^2\sigma^2\right)\)</span>.</p>
</section>
<section id="bayesian-model-for-univariate-gaussian-likelihood-with-known-mean">
<h2><span class="section-number">23.3. </span>Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean<a class="headerlink" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-mean" title="Permalink to this heading">#</a></h2>
<section id="id2">
<h3>The Bayesian Model<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu, \sigma^2)\)</span>, with <span class="math notranslate nohighlight">\(\mu\)</span> known. We place an inverse-gamma prior on <span class="math notranslate nohighlight">\(\sigma^2\)</span>, <span class="math notranslate nohighlight">\(\sigma^2\sim IG(\alpha, \beta)\)</span>.</p>
<p><strong>Question:</strong> is our choice of prior appropriate?</p>
</section>
</section>
<section id="id3">
<h2><span class="section-number">23.4. </span>Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p><strong>Inference:</strong> The posterior <span class="math notranslate nohighlight">\(p(\sigma^2|Y)\)</span> is then:
\begin{aligned}
p(\sigma^2 | Y) = \frac{p(Y| \sigma^2)p(\sigma^2)}{p(Y)} = \frac{\overbrace{\frac{1}{\sqrt{2\pi \sigma^2}} \mathrm{exp} \left{-\frac{(Y - \mu)^2}{2\sigma^2}\right}}^{\text{likelihood}} \overbrace{\frac{\beta^\alpha}{\Gamma(\alpha)} \left( \sigma^2\right)^{-\alpha -1}\mathrm{exp} \left{-\frac{\beta}{\sigma^2}\right}}^{\text{prior}}}{p(Y)}
\end{aligned}</p>
<p>We can simplify the posterior as:</p>
<p>\begin{aligned}
p(\sigma^2 | Y) &amp;= const * \left( \sigma^2\right)^{-(\alpha + 0.5) -1}\mathrm{exp} \left{-\frac{\frac{(Y-\mu)^2}{2} + \beta}{\sigma^2}\right}
\end{aligned}</p>
<p>Thus, we see that the posterior is an inverse gamma distribution, <span class="math notranslate nohighlight">\(IG\left(\alpha + 0.5, \frac{(Y-\mu)^2}{2} + \beta\right)\)</span>.</p>
</section>
<section id="bayesian-model-for-univariate-gaussian-likelihood-with-unknown-mean-and-variance">
<h2><span class="section-number">23.5. </span>Bayesian Model for (Univariate) Gaussian Likelihood with Unknown Mean and Variance<a class="headerlink" href="#bayesian-model-for-univariate-gaussian-likelihood-with-unknown-mean-and-variance" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu, \sigma^2)\)</span>, with both parameters unknown. We place a normal prior on <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\mu\sim\mathcal{N}(m, s^2)\)</span>, and an inverse-gamma prior on <span class="math notranslate nohighlight">\(\sigma^2\)</span>, <span class="math notranslate nohighlight">\(\sigma^2\sim IG(\alpha, \beta)\)</span>.</p>
<p>The posterior <span class="math notranslate nohighlight">\(p(\sigma^2|Y)\)</span> is then:</p>
<p>\begin{aligned}
p(\mu, \sigma^2 | Y)  = \frac{\overbrace{\frac{1}{\sqrt{2\pi \sigma^2}} \mathrm{exp} \left{-\frac{(Y - \mu)^2}{2\sigma^2}\right}}^{\text{likelihood}} \overbrace{\frac{1}{\sqrt{2\pi s^2}} \mathrm{exp} \left{-\frac{(m - \mu)^2}{2s^2}\right}}^{\text{prior on <span class="math notranslate nohighlight">\(\mu\)</span>}}\overbrace{\frac{\beta^\alpha}{\Gamma(\alpha)} \left( \sigma^2\right)^{-\alpha -1}\mathrm{exp} \left{-\frac{\beta}{\sigma^2}\right}}^{\text{prior on <span class="math notranslate nohighlight">\(\sigma^2\)</span>}}}{p(Y)}
\end{aligned}</p>
<p>Can the posterior be simplified so that we recognize the form of the distribution?</p>
</section>
<section id="bayesian-model-for-poisson-likelihood">
<h2><span class="section-number">23.6. </span>Bayesian Model for Poisson Likelihood<a class="headerlink" href="#bayesian-model-for-poisson-likelihood" title="Permalink to this heading">#</a></h2>
<section id="id4">
<h3>The Bayesian Model<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>With the kidney cancer dataset in mind, let <span class="math notranslate nohighlight">\(Y\sim Poi(N\theta)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the total population and <span class="math notranslate nohighlight">\(\theta\)</span> is the underlying cancer rate. We place a gamma prior on <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\theta \sim Ga(\alpha, \beta)\)</span>.</p>
<p><strong>Question:</strong> is our choice of prior appropriate?</p>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">#</a></h3>
<p>The posterior <span class="math notranslate nohighlight">\(p(\theta|Y)\)</span> is then:
\begin{aligned}
p(\theta | Y) = \frac{p(Y| \theta)p(\theta)}{p(Y)} = \frac{\overbrace{\frac{(N\theta)^Y}{Y!} \mathrm{exp} \left{-N\theta\right}}^{\text{likelihood}} \overbrace{\frac{\beta^\alpha}{\Gamma(\alpha)} \left( \theta\right)^{\alpha -1}\mathrm{exp} \left{-\beta\theta\right}}^{\text{prior}}}{p(Y)}
\end{aligned}</p>
<p>We can simplify the posterior as:</p>
<p>\begin{aligned}
p(\theta | Y) &amp;= const * \theta^{\alpha + Y - 1}\mathrm{exp}\left{-(N+ \beta)\theta \right}
\end{aligned}</p>
<p>Thus, we see that the posterior is a gamma distribution, <span class="math notranslate nohighlight">\(Ga\left(\alpha + Y, N + \beta\right)\)</span>.</p>
</section>
</section>
<section id="non-conjugate-models">
<h2><span class="section-number">23.7. </span>Non-Conjugate Models<a class="headerlink" href="#non-conjugate-models" title="Permalink to this heading">#</a></h2>
<p>We know that conjugate priors yield closed-form expressions for the posterior. In all our examples, these posteriors have been both easy to sample from and easy to integrate over. That is, we can evaluate our Bayesian models: can compute the log-likelihood of the data and perform posterior predictive checks.</p>
<p>So why would we ever consider non-conjugate priors?</p>
<p>Suppose that <span class="math notranslate nohighlight">\(Y\sim \mathcal{N}(\mu, 2)\)</span> represent the height of a person randomly selected from a population. Would the conjugate prior <span class="math notranslate nohighlight">\(\mu\sim \mathcal{N}(5.7, 1)\)</span> be appropriate for this application?</p>
<p>If we wanted to use the prior <span class="math notranslate nohighlight">\(\mu \sim Ga(5.7, 1)\)</span>, would we be able to derive a closed form expression for the posterior?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="connections-with-frequentist-inference">
<h1><span class="section-number">24. </span>Connections with Frequentist Inference<a class="headerlink" href="#connections-with-frequentist-inference" title="Permalink to this heading">#</a></h1>
<section id="point-estimates-from-the-posterior">
<h2><span class="section-number">24.1. </span>Point Estimates from the Posterior<a class="headerlink" href="#point-estimates-from-the-posterior" title="Permalink to this heading">#</a></h2>
<p>If you absolutely wanted to derive a point estimate for the parameters <span class="math notranslate nohighlight">\(\theta\)</span> in the likelihood from your Bayesian model, there are two common ways to do it:</p>
<ol class="arabic simple">
<li><p><em><strong>the posterior mean</strong></em>
$<span class="math notranslate nohighlight">\(
\theta_{\text{post mean}} = \mathbb{E}_{\theta\sim p(\theta|Y)}\left[ \theta|Y \right] = \int \theta p(\theta|Y) d\theta
\)</span>$</p></li>
<li><p><em><strong>the posterior mode</strong></em> or <em><strong>maximum a posterior (MAP)</strong></em> estimate
$<span class="math notranslate nohighlight">\(
\theta_{\text{MAP}} = \mathrm{argmax}_{\theta} p(\theta|Y)
\)</span>$</p></li>
</ol>
<p><strong>Question:</strong> is it better to summarize the entire posterior using a point estimate? I.e. why should we keep the posterior distribution around?</p>
</section>
<section id="point-estimates-can-be-misleading">
<h2><span class="section-number">24.2. </span>Point Estimates Can Be Misleading<a class="headerlink" href="#point-estimates-can-be-misleading" title="Permalink to this heading">#</a></h2>
<p>The posterior mode can be an atypical point:
<img src="fig/map.jpg" style="height:250px; width:450px"></p>
</section>
<section id="id5">
<h2><span class="section-number">24.3. </span>Point Estimates Can Be Misleading<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h2>
<p>The posterior mean can be an unlikely point:</p>
<img src="fig/mean.jpg" style="height:250px; width:450px"></section>
<section id="comparison-of-posterior-point-estimates-and-mle">
<h2><span class="section-number">24.4. </span>Comparison of Posterior Point Estimates and MLE<a class="headerlink" href="#comparison-of-posterior-point-estimates-and-mle" title="Permalink to this heading">#</a></h2>
<p><strong>Beta-Binomial Model for Coin Flips</strong></p>
<ul class="simple">
<li><p>Likelihood: <span class="math notranslate nohighlight">\(Bin(N, \theta)\)</span></p></li>
<li><p>Prior: <span class="math notranslate nohighlight">\(Beta(\alpha, \beta)\)</span></p></li>
<li><p>MLE: <span class="math notranslate nohighlight">\(\frac{Y}{N}\)</span></p></li>
<li><p>MAP: <span class="math notranslate nohighlight">\(\frac{Y + \alpha - 1}{N + \alpha + \beta - 2}\)</span></p></li>
<li><p>Posterior Mean: <span class="math notranslate nohighlight">\(\frac{Y + \alpha}{N + \alpha + \beta}\)</span></p></li>
</ul>
<p><strong>Question:</strong> What is the effect of the prior on the posterior point estimates? Imagine if <span class="math notranslate nohighlight">\(Y=10\)</span>, <span class="math notranslate nohighlight">\(N=11\)</span>, <span class="math notranslate nohighlight">\(\alpha=100\)</span>, <span class="math notranslate nohighlight">\(\beta=300\)</span>. What if <span class="math notranslate nohighlight">\(Y=1,000\)</span>, <span class="math notranslate nohighlight">\(N=11,000\)</span>, <span class="math notranslate nohighlight">\(\alpha=1\)</span>, <span class="math notranslate nohighlight">\(\beta=3\)</span>?</p>
</section>
<section id="the-coin-toss-example-revisited-yet-again">
<h2><span class="section-number">24.5. </span>The Coin Toss Example: Revisited Yet Again<a class="headerlink" href="#the-coin-toss-example-revisited-yet-again" title="Permalink to this heading">#</a></h2>
<p>Recall that one way to prevent the MLE from overfitting is to add <em><strong>regularization terms</strong></em>:
$<span class="math notranslate nohighlight">\(
\theta_{\text{MLE Reg}} = \frac{Y + \alpha}{N + \beta}.
\)</span>$
This is very similar to the MAP and posterior mean estimates:</p>
<ul class="simple">
<li><p>MAP: <span class="math notranslate nohighlight">\(\frac{Y + \alpha - 1}{N + \alpha + \beta - 2}\)</span></p></li>
<li><p>Posterior Mean: <span class="math notranslate nohighlight">\(\frac{Y + \alpha}{N + \alpha + \beta}\)</span></p></li>
</ul>
<p>In fact, we have see that one effect of adding a prior is that it <strong>regularizes</strong> our inference about <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>Question:</strong> What happens to the MAP and posterior mean estimates as <span class="math notranslate nohighlight">\(N\)</span> (and hence <span class="math notranslate nohighlight">\(Y\)</span>) becomes very large?</p>
<div class="math notranslate nohighlight">
\[
\lim_{N\to \infty} \frac{Y + \alpha - 1}{N + \alpha + \beta - 2} = ?
\]</div>
</section>
<section id="law-of-large-numbers-for-bayesian-inference">
<h2><span class="section-number">24.6. </span>Law of Large Numbers for Bayesian Inference<a class="headerlink" href="#law-of-large-numbers-for-bayesian-inference" title="Permalink to this heading">#</a></h2>
<p>In general, in Bayesian inference we are <strong>less interested asymptotic behavior</strong>. But the properties of the asymptotic distribution of the posterior can be useful.</p>
<p><strong>Theorem: (Berstein-von Mises</strong>)</p>
<p>“Under some conditions, as <span class="math notranslate nohighlight">\(N\to \infty\)</span> the posterior distribution converges to a Gaussian distribution centred at the MLE with covariance matrix given by a function of the Fisher information matrix at the true population parameter value.”</p>
<p><strong>Consequences</strong></p>
<ol class="arabic simple">
<li><p>The posterior point estimates approach the MLE, with large samples sizes. <br><br></p></li>
<li><p>It may be valid to approximate the posterior with a Gaussian, with large samples sizes. This will become a very important idea during the second half of the course!</p></li>
</ol>
</section>
<section id="computational-comparisons">
<h2><span class="section-number">24.7. </span>Computational Comparisons<a class="headerlink" href="#computational-comparisons" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Computation of the MLE is an optimization problem.</strong> Although difficult, there are many established methods for performing optimization (even when the objective function is not convex – i.e. many local optima).</p></li>
</ol>
<p>More importantly, there are algorithms to perform general, automatic optimization (e.g. gradient descent) on a large class of functions – that is, we do not need to artisanally solve an optimization problem for each statistical model.<br><br></p>
<ol class="arabic simple" start="2">
<li><p><strong>Computation of the posterior (thus far) is an process of choosing the right priors and noting that the posterior distribution is of the same type as the prior.</strong> The derivation is simple so long as we use conjugate priors. But many intuitively appropriate priors (like the inverse gamma and normal priors for a univariate gaussian) are not conjugate. In those cases, it becomes intractable to</p></li>
</ol>
<ul class="simple">
<li><p>compute posterior mean</p></li>
<li><p>simulate samples from the posterior (and hence simulate samples from the posterior predictive)</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_3_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Lecture #3: Bayesian Modeling</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_5_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Lecture #5: Sampling for Posterior Simulation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">21. Lecture #4: Bayesian versus Frequentist Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">21.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">21.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-bayesian-modeling">22. Review of Bayesian Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-process">22.1. The Bayesian Modeling Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-update">22.2. Bayesian Update</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">22.3. Model Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-bayesian-inference">22.4. Components of Bayesian Inference</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-conjugate-and-non-conjugate-models">23. Examples of Conjugate and Non-Conjugate Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-variance">23.1. Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-model">The Bayesian Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">23.2. Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-known-mean">23.3. Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The Bayesian Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">23.4. Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-univariate-gaussian-likelihood-with-unknown-mean-and-variance">23.5. Bayesian Model for (Univariate) Gaussian Likelihood with Unknown Mean and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-for-poisson-likelihood">23.6. Bayesian Model for Poisson Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The Bayesian Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-conjugate-models">23.7. Non-Conjugate Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#connections-with-frequentist-inference">24. Connections with Frequentist Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-estimates-from-the-posterior">24.1. Point Estimates from the Posterior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-estimates-can-be-misleading">24.2. Point Estimates Can Be Misleading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">24.3. Point Estimates Can Be Misleading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-posterior-point-estimates-and-mle">24.4. Comparison of Posterior Point Estimates and MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coin-toss-example-revisited-yet-again">24.5. The Coin Toss Example: Revisited Yet Again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#law-of-large-numbers-for-bayesian-inference">24.6. Law of Large Numbers for Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-comparisons">24.7. Computational Comparisons</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Weiwei Pan and Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>