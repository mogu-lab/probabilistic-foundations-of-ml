
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Maximum Likelihood: Theory &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'mle-theory';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/mle-theory.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="9. Maximum Likelihood: Code" href="mle-code.html" />
    <link rel="prev" title="7. The Ethics of Data" href="ethics-of-data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/mle-theory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Maximum Likelihood: Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-notation-and-formalism">8.1. MLE: Notation and Formalism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphically-representing-i-i-d-observations-and-model-parameters">8.2. Graphically Representing I.I.D Observations and Model Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-properties-of-the-mle">8.3. Theoretical Properties of the MLE</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="maximum-likelihood-theory">
<h1><span class="section-number">8. </span>Maximum Likelihood: Theory<a class="headerlink" href="#maximum-likelihood-theory" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> At this point, our modeling toolkit is already getting quite expressive.</p>
<ol class="arabic simple">
<li><p>We can develop simple <em>predictive models</em> using <em>conditional distributions</em>: we can specify models of the form <span class="math notranslate nohighlight">\(p_{A | B}(a | b)\)</span>, which allow us to predict the probability that <span class="math notranslate nohighlight">\(A = a\)</span> given that <span class="math notranslate nohighlight">\(B = b\)</span>. We do this by specifying a distribution over random variable (RV) <span class="math notranslate nohighlight">\(A\)</span>, whose parameters are a <em>function</em> of <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>We can develop simple <em>generative models</em> using <em>joint distributions</em>: we can specify models of the form <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span>, which allow us to sample (or generate) data. We do this by factorizing this joint probability into a product of conditional and marginal distributions, e.g. <span class="math notranslate nohighlight">\(p_{A, B}(a, b) = p_{A | B}(a | b) \cdot p_B(b)\)</span>, which we already know how to specify, sample from, and evaluate.</p></li>
</ol>
<p>Of course, the predictive and generative models you may have heard about in the news are capable of doing more than the instances we’ve covert so far—we will build up to these fancy models over the course of the semester. What’s important for now, though, is that you understand how such models can be represented using probability distributions.</p>
<p><strong>Challenge:</strong> So what stands in our way of applying our modeling tools to real-world data? First, we’ve only instantiated our models with <em>discrete</em> distributions. Many real-world data, however, requires <em>continuous</em> distributions; that is, distributions over real numbers (e.g. blood pressure, body-mass index, time spent in REM sleep, etc.). We’ll get more into the details of continuous modeling a bit later. Our second obstacle is: we still don’t have a way of <em>automatically</em> fitting a model to data. So far, you’ve fit all models to data by hand via inspection—you looked at the data and tried to match the model to the data. With increasing model and data complexity, it becomes prohibitively difficult to fit the model to the data by hand. Today, we’ll introduce one technique for doing this: maximum likelihood estimation (MLE). This is the first algorithm we cover that allows the machine to “learn” from data.</p>
<p>The idea behind MLE is to find a model under which the probability of the data is highest. The intuition behind the MLE is that a model that scores the observed data as likely could have reasonably generated the data.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Formally introduce and motivate the MLE.</p></li>
<li><p>Extend notation of directed graphical models (DGMs) to represent a full data-set instead of just one observation.</p></li>
<li><p>Understand theoretical properties of the MLE.</p></li>
</ul>
<p>This will give us the framing we need in order to implement MLE in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</p>
<section id="mle-notation-and-formalism">
<h2><span class="section-number">8.1. </span>MLE: Notation and Formalism<a class="headerlink" href="#mle-notation-and-formalism" title="Link to this heading">#</a></h2>
<p>The idea behind the MLE is to find the model parameters that maximize the probability of the data. Let’s introduce some notation to help us formalize what this means mathematically.</p>
<p><strong>Notation for Data.</strong> Let <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> denote our <em>all</em> of our observed data (<span class="math notranslate nohighlight">\(\mathcal{D}\)</span> represents the entirety of the above table). Let <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> represent observation number <span class="math notranslate nohighlight">\(n\)</span> (i.e. row <span class="math notranslate nohighlight">\(n\)</span>) from the table. <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is a tuple of values at each of the columns: <span class="math notranslate nohighlight">\(\mathcal{D}_n = (d_n, c_n, h_n, a_n)\)</span>. Recall that we define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D\)</span>: Day-of-Week</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span>: Condition</p></li>
<li><p><span class="math notranslate nohighlight">\(H\)</span>: Hospitalized</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span>: Antibiotics</p></li>
</ul>
<p><strong>Notation for Parameters.</strong> For simplicity, we’ve omitted the notation for each distribution’s parameters from the notation so far. From now on, we’ll explicitly write out the parameters as arguments to the distribution by listing them after a <em>semi-colon</em>.</p>
<p>Consider our running example: modeling the joint probability of a patient arriving on a specific day, <span class="math notranslate nohighlight">\(D = d\)</span>, with/without intoxication <span class="math notranslate nohighlight">\(I = i\)</span> via <span class="math notranslate nohighlight">\(p_{I, D}(i, d) = p_{I | D}(i | d) \cdot p_D(d)\)</span>. Here, <span class="math notranslate nohighlight">\(p_D(\cdot)\)</span> is a categorical, which means it relies on a parameter, <span class="math notranslate nohighlight">\(\pi\)</span>, a 7-dimensional array consisting of the probabilities of patients arriving on each day of the week. To denote this explicitly, we write:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c692ef59-376a-4920-a15f-86e4b372e778">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-c692ef59-376a-4920-a15f-86e4b372e778" title="Permalink to this equation">#</a></span>\[\begin{align}
p_D(\cdot; \pi) = \mathrm{Cat}(\pi),
\end{align}\]</div>
<p>wherein, we explicitly write <span class="math notranslate nohighlight">\(\pi\)</span> after a semi-colon. We can then do the same for the conditional <span class="math notranslate nohighlight">\(p_{I | D}(\cdot | d)\)</span>, which relies on a parameter we’ll call <span class="math notranslate nohighlight">\(\rho\)</span>, a 7-dimensional array consisting of the probability of intoxication on each day of the week. We can denote this parameter explicitly via:</p>
<div class="amsmath math notranslate nohighlight" id="equation-703a5d0c-521d-4e2a-9de4-3f1a2c2ac69a">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-703a5d0c-521d-4e2a-9de4-3f1a2c2ac69a" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{I | D}(\cdot | d; \rho) &amp;= \mathrm{Ber}(\rho_d) \\
\end{align}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\rho_d\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-th element of <span class="math notranslate nohighlight">\(\rho\)</span>, representing the probability of intoxication on day <span class="math notranslate nohighlight">\(d\)</span>. (Note that previously we used <span class="math notranslate nohighlight">\(\rho(d)\)</span> instead of <span class="math notranslate nohighlight">\(\rho_d\)</span>—these are two different ways of expressing the same idea).</p>
<p>Finally, to explicitly denote the parameters of the <em>joint</em> distribution, we will list all of them behind the semi-colon. That is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-529ef4bd-640c-49bc-a49a-016d0cb5ba70">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-529ef4bd-640c-49bc-a49a-016d0cb5ba70" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{I, D}(i, d; \underbrace{\rho, \pi}_{\theta}) = p_{I | D}(\cdot | d; \rho) \cdot p_D(\cdot; \pi).
\end{align}\]</div>
<p>We will group them together <span class="math notranslate nohighlight">\(\theta = \{ \rho, \pi \}\)</span> and say we want to <em>learn <span class="math notranslate nohighlight">\(\theta\)</span> from the data</em>.</p>
<p><strong>The MLE Objective.</strong>
Let <span class="math notranslate nohighlight">\(\theta\)</span> denote the set of all parameters used in our model for the IHH ER data. Using the above notation, <span class="math notranslate nohighlight">\(p(\mathcal{D}; \theta)\)</span> denotes the probability of the observed data; we will call it the <em>joint data likelihood</em>, since it is the <em>joint</em> distribution of all observations. Our goal is then to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize the probability of having observed <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2ee73de6-f29b-4d87-acc4-df471295bc63">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-2ee73de6-f29b-4d87-acc4-df471295bc63" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_{\theta} \text{ } p(\mathcal{D}; \theta),
\end{align}\]</div>
<p>wherein “argmax” denotes the value of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes the joint probability. So what does it mean to evaluate the probability of the <em>whole data</em>, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, under our model, <span class="math notranslate nohighlight">\(p_{D, C, H, A}\)</span>? It means evaluating the <em>joint distribution of all observations</em>, <span class="math notranslate nohighlight">\(\mathcal{D}_n = (d_n, c_n, h_n, a_n)\)</span> for every <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7ebcc32b-0a7c-4cd8-a51d-1561bc587871">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-7ebcc32b-0a7c-4cd8-a51d-1561bc587871" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_{\theta} \text{ }  p(\mathcal{D}; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \text{ } p(\mathcal{D}_1, \cdots, \mathcal{D}_N; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the total number of observations.</p>
<p>Now, recall that every joint distribution can be factorized into a product of conditional and marginal distributions, and that the number of possible factorizations grows unwieldy very quickly with the number of variables. Since the number of variables in this joint distribution is a function of the number of observations, <span class="math notranslate nohighlight">\(N\)</span>, which is large (e.g. thousands), we need some way to select a reasonable factorization. As typical, we are going to assume that the observations are independent, and identically distributed (i.i.d). This means that one patient coming to the ER does not tell us anything about how likely other patients are to come to the ER. Now, recall that when two RVs are independent, their joint distribution equals a product of their marginals. We can therefore factorize the joint distribution as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1430c7ac-5aea-4e2f-b458-f6d6d7d361ce">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-1430c7ac-5aea-4e2f-b458-f6d6d7d361ce" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_{\theta} \text{ } p(\mathcal{D}; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \text{ } p(\mathcal{D}_1, \cdots, \mathcal{D}_N; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \text{ } p(\mathcal{D}_1; \theta) \cdot p(\mathcal{D}_2; \theta) \cdots p(\mathcal{D}_N; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \prod\limits_{n=1}^N \underbrace{p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta)}_{\text{We already know how to compute this!}}
\end{align}\]</div>
<p>We have now arrived at a formula for the joint distribution that we know how to compute—we’ve even written code to evaluate it in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</p>
<p><strong>Numerical Stability:</strong> Notice that since our joint is a discrete probability distribution, it outputs probabilities between 0 and 1: <span class="math notranslate nohighlight">\(p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta) \in [0, 1]\)</span>. In other words, it outputs <em>fractions</em>. In the above formula, we then multiply these fractions times one another <span class="math notranslate nohighlight">\(N\)</span> times. But what happens when you multiply fractions together many times? Answer: the results shrinks towards 0 very quickly (try it yourself!). This is a problem, because our computer can only represent small numbers up to a finite precision. For a large <span class="math notranslate nohighlight">\(N\)</span>, our computer will round down the answer to <span class="math notranslate nohighlight">\(0\)</span>, which will prevent us from performing the argmax. Because of this issue, we have to <em>transform</em> our original MLE objective into a problem that a computer can numerically solve. We do this by maximizing the <span class="math notranslate nohighlight">\(\log\)</span> of the joint probability for two reasons:</p>
<p><strong>(a)</strong> Logs turn products into sums: <span class="math notranslate nohighlight">\(\log (x \cdot y) = \log x + \log y\)</span>. Applying this formula to our MLE objective results in a <em>sum</em> of fractions, which is numerically stable:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9e947ab9-40c3-4239-b6c6-11a55f2aa83d">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-9e947ab9-40c3-4239-b6c6-11a55f2aa83d" title="Permalink to this equation">#</a></span>\[\begin{align}
\log \prod\limits_{n=1}^N p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta) &amp;= \sum\limits_{n=1}^N \log p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta)
\end{align}\]</div>
<p>Instead of taking a <em>product</em> of a huge number of fractions, we take a <em>sum</em>, overcoming the issue of numerical instability.</p>
<p><strong>(b)</strong> But now we’re no longer solving the original problem—we’re maximizing the <span class="math notranslate nohighlight">\(\log\)</span> of the joint probability—so will we still get the right answer? Yes: because the <span class="math notranslate nohighlight">\(\log\)</span> function is a <em>strictly increasing function</em>, our maxima will remain in the same location. That is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-117ba8d3-82b5-4f20-ba29-659b90232236">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-117ba8d3-82b5-4f20-ba29-659b90232236" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_{\theta} \prod\limits_{n=1}^N p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta) \\
&amp;= \mathrm{argmax}_{\theta} \sum\limits_{n=1}^N \log p_{D, C, H, A}(d_n, c_n, h_n, a_n; \theta)
\end{align}\]</div>
<p>To illustrate point (b), check out the graph below, which shows that the argmax of a function doesn’t change if a <span class="math notranslate nohighlight">\(\log\)</span> is applied to it.</p>
<figure class="align-center" id="fig-invariange-of-argmax-to-log">
<a class="reference internal image-reference" href="_images/invariance_of_argmax_to_log.png"><img alt="_images/invariance_of_argmax_to_log.png" src="_images/invariance_of_argmax_to_log.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">The <span class="math notranslate nohighlight">\(\log\)</span>-transform does not impact the <span class="math notranslate nohighlight">\(\mathrm{argmax}\)</span>.</span><a class="headerlink" href="#fig-invariange-of-argmax-to-log" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Optimization:</strong> So at this point, we can compute the MLE objective for specific choices of <span class="math notranslate nohighlight">\(\theta\)</span>, but we don’t know yet how to perform the argmax operation. We’ll introduce this concept a bit later in the course. For now, we’ll provide you with a function that can perform the maximization.</p>
</section>
<section id="graphically-representing-i-i-d-observations-and-model-parameters">
<h2><span class="section-number">8.2. </span>Graphically Representing I.I.D Observations and Model Parameters<a class="headerlink" href="#graphically-representing-i-i-d-observations-and-model-parameters" title="Link to this heading">#</a></h2>
<p>Before implementing the MLE in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, we will extend our Directed Graphical Model (DGM) representation to include i.i.d observations and model parameters. This will help us in the translation process from math to code. Suppose we have a simple joint distribution over two RVs, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, whose conditional controlled by a parameter, <span class="math notranslate nohighlight">\(\theta\)</span>, as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-24feb906-7979-4c67-9b26-760e255a72b1">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-24feb906-7979-4c67-9b26-760e255a72b1" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B}(a, b; \theta) &amp;= p_{B | A}(b | a; \theta) \cdot p_A(a)
\end{align}\]</div>
<p>Suppose further that we have <span class="math notranslate nohighlight">\(N\)</span> i.i.d observations from this joint distribution. That is, we have <span class="math notranslate nohighlight">\(\mathcal{D}_n = (a_n, b_n)\)</span> for <span class="math notranslate nohighlight">\(n \in 1, \dots, N\)</span>. This gives us the following joint data likelihood:</p>
<div class="amsmath math notranslate nohighlight" id="equation-783c550c-38e3-431d-b64b-45e5fd6ebb84">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-783c550c-38e3-431d-b64b-45e5fd6ebb84" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}; \theta) &amp;= \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \quad \text{since the observations are i.i.d} \\
&amp;= \prod\limits_{n=1}^N p_{A, B}(a_n, b_n; \theta) \\
&amp;= \prod\limits_{n=1}^N p_{B | A}(b_n | a_n; \theta) \cdot p_A(a_n)
\end{align}\]</div>
<p>How would we represent this graphically? The answer is a little messy:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIriW55zE&#x2F;pT0n6N3Vb4gqonaV360vkQ&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>Each pair <span class="math notranslate nohighlight">\((A_n, B_n)\)</span> get its own arrow to signify the conditional dependence of <span class="math notranslate nohighlight">\(B_n\)</span> on <span class="math notranslate nohighlight">\(A_n\)</span>. And since every pair depends on the same parameter, <span class="math notranslate nohighlight">\(\theta\)</span> has an arrow pointing into every <span class="math notranslate nohighlight">\(B_n\)</span>.</p>
<p><strong>Representing Parameters.</strong> In the above, notice that circles are only used for RVs. Since <span class="math notranslate nohighlight">\(\theta\)</span> is not an RV, it is not inside a circle—it’s represented by a dot instead.</p>
<p><strong>Representing I.I.D Observations:</strong> For more complicated models, like the IHH ER you’ve already developed, this graphical representation becomes too difficult to read. As a result, we use the following short-hand:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIrp4DCB8&#x2F;jGLx4K70R3JIS_kgnlL8Dg&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>In this representation, we introduce a “plate” (the rectangle surrounding <span class="math notranslate nohighlight">\(A_n\)</span> and <span class="math notranslate nohighlight">\(B_n\)</span>). The plate denotes that what’s inside should be repeated <span class="math notranslate nohighlight">\(N\)</span> times, where <span class="math notranslate nohighlight">\(N\)</span> is written in the bottom-right corner. Why is this called a plate? Do you eat off of rectangular plates at home? This shall remain a mystery to us all…</p>
<p><strong>A note on conditional independence:</strong> We note that in this example model, for the IHH ER model you’ve developed, and generally for the models we consider in this class, the observations are only i.i.d given the model parameters. That is, given <span class="math notranslate nohighlight">\(\theta\)</span>, we can factorize <span class="math notranslate nohighlight">\(p(\mathcal{D}; \theta)\)</span> into <span class="math notranslate nohighlight">\(\prod_{n=1}^N p(\mathcal{D}_n; \theta)\)</span>. However, if we do not “condition” on the parameters, <span class="math notranslate nohighlight">\(\theta\)</span>, the observations do carry knowledge about one another. That is, having observed <span class="math notranslate nohighlight">\(\mathcal{D}_1\)</span> can tell me something about <span class="math notranslate nohighlight">\(\mathcal{D}_2\)</span> because it tells me something about <span class="math notranslate nohighlight">\(\theta\)</span>, which is shared across all observations. More on that later in the course.</p>
<div class="admonition-exercise-translate-between-dgms-and-joint-distributions admonition">
<p class="admonition-title">Exercise: Translate between DGMs and joint distributions</p>
<p><strong>Tip:</strong> For the problems below that contain a <strong>plate</strong>, we recommend <strong>expanding</strong> out the plate first. That is, if you have a plate of size <span class="math notranslate nohighlight">\(N\)</span>, meaning you have <span class="math notranslate nohighlight">\(N\)</span> i.i.d RVs, pick a small <span class="math notranslate nohighlight">\(N\)</span> (e.g. <span class="math notranslate nohighlight">\(N=3\)</span>) and write out what the graphical model would look like <strong>without</strong> the plate. Then, write the joint distribution for this version of the model (with <span class="math notranslate nohighlight">\(N=3\)</span>) to gain intuition. Finally, this will help you write the joint distribution for the model abstractly for any <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p><strong>Part 1:</strong> Extend the DGM for the IHH ER below to represent the joint distribution of the data. Additionally, include all parameters of all distributions.</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsbt2Mvc&#x2F;_p7bxsayZ9Qc6k9X8G25bw&#x2F;view?embed">
  </iframe>
</div>
</div> 
<p><strong>Part 2:</strong> For each of the following DGMs, write down the joint distribution of all RVs. We’ve specifically selected models that are commonly used in ML. At this point in the course, we have not covered enough materials to make the connection between the model and their actual use—so don’t expect to understand exactly what they mean yet!</p>
<p>(i) Predictive Models (including regression and classification).</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIs__Ja5w&#x2F;PVlm2FrOU1HjUyF96XXFhw&#x2F;view?embed">
  </iframe>
</div>
</div> 
<p>(ii) Bayesian Gaussian Mixture Models (used for clustering).</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIs1vP9i0&#x2F;-IWykjjF-dWy5DOBnqfudA&#x2F;view?embed">
  </iframe>
</div>
</div> 
<p>(iii) Latent Dirichlet Allocation (used for automatically extracting topics from text).</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIs1zW89w&#x2F;AV6ehWRFRXZ4Y0xpddDcAw&#x2F;view?embed">
  </iframe>
</div>
</div> 
<p><strong>Part 3:</strong> Draw the DGM for each of the following models.</p>
<p>(iv) Conditional Subspace Variational Autoencoder (used for generating synthetic data, like pictures of celebrity faces)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\prod\limits_{n=1}^N p_{X | Z, W}(x_n | z_n, w_n; \theta) \cdot p_Z(z_n) \cdot p_{W | Y}(w_n | y_n) \cdot p_Y(y_n)
\end{align*}\]</div>
<p>(v) Hidden Markov Models (used for modeling time-series data).</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{Z_1}(z_1; \psi) \cdot p_{X_1 | Z_1}(x_1 | z_1; \theta) \cdot \prod\limits_{t=2}^T p_{Z_t | Z_{t-1}}(z_t | z_{t - 1}; \psi) \cdot p_{X_t | Z_t}(x_t | z_t; \theta)
\end{align*}\]</div>
<p>Note: for this DGM, you will not be able to use plate notation. Instead, please use “<span class="math notranslate nohighlight">\(\dots\)</span>” to indicate a repeating pattern.</p>
</div>
</section>
<section id="theoretical-properties-of-the-mle">
<h2><span class="section-number">8.3. </span>Theoretical Properties of the MLE<a class="headerlink" href="#theoretical-properties-of-the-mle" title="Link to this heading">#</a></h2>
<p>In this course, our main goal is to focus on <em>model specification</em>—the process of creating a probabilistic model, and understanding the consequences of our modeling assumptions on downstream tasks. As such, we will not get into the nitty gritty of how models are fit to data. Nonetheless, it is important to informally highlight several theoretical properties of the MLE. The purpose of this is to help us understand, what can we expect of the MLE—will it behave like we want in different situations?</p>
<p><strong>Desiderata:</strong> Let’s first highlight three properties we typically want for our learning algorithms. Informally, we define:</p>
<ol class="arabic simple">
<li><p><em>Consistency:</em> As the number of observations approaches infinity, <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>, the parameters we learned from the data <span class="math notranslate nohighlight">\(\theta^\text{learned}\)</span> approach the true parameters of the model that generated the data <span class="math notranslate nohighlight">\(\theta^\text{true}\)</span>.</p></li>
<li><p><em>Unbiasedness:</em> Suppose we were able to collect several data sets from the same random phenomenon. For each of the data sets, suppose we were to then fit the same model and obtain <span class="math notranslate nohighlight">\(\theta^\text{learned}\)</span>. For an unbiased learning algorithm, averaging all of the <span class="math notranslate nohighlight">\(\theta^\text{learned}\)</span>’s would yield <span class="math notranslate nohighlight">\(\theta^\text{true}\)</span>.</p></li>
<li><p><em>Low-variance:</em> Suppose we were able to collect several data sets from the same random phenomenon. For each of the data sets, suppose we were to then fit the same model and obtain <span class="math notranslate nohighlight">\(\theta^\text{learned}\)</span>. For a low-variance learning algorithm, the average distance between the <span class="math notranslate nohighlight">\(\theta^\text{learned}\)</span>’s and <span class="math notranslate nohighlight">\(\theta^\text{true}\)</span> is small.</p></li>
</ol>
<p><strong>Why should we care about these properties?</strong> Consistency tells us that with sufficient data, our learning algorithm should converge to the true parameters. This is important! Would you want to use a learning algorithm that, with more data, becomes more and more wrong? Unbiasedness and low-variance, together, give us a notion of how <em>quickly</em> our learning algorithm converges to the true parameters with more data. These two often come at a tradeoff with one another. We won’t get into this for now.</p>
<p><strong>Properties of the MLE:</strong> In order for the MLE to satisfy the above desiderata, we must make several assumptions, including that,</p>
<ul class="simple">
<li><p>The model is <em>well-specified</em>—the observed data was generated from the same model we are fitting.</p></li>
<li><p>The model is <em>identifiable</em>—there are no two different sets of parameters that represent the same model.</p></li>
<li><p>The data was generated i.i.d.</p></li>
</ul>
<p>For <em>any model satisfying these assumptions</em>, the MLE is:</p>
<ol class="arabic simple">
<li><p>Consistent</p></li>
<li><p>Asymptotically (as <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>) unbiased</p></li>
<li><p>Asymptotically (as <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>) minimum-variance</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ethics-of-data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>The Ethics of Data</p>
      </div>
    </a>
    <a class="right-next"
       href="mle-code.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Maximum Likelihood: Code</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-notation-and-formalism">8.1. MLE: Notation and Formalism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphically-representing-i-i-d-observations-and-model-parameters">8.2. Graphically Representing I.I.D Observations and Model Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-properties-of-the-mle">8.3. Theoretical Properties of the MLE</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>