

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>16. Model Selection &amp; Evaluation &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'model-selection';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/model-selection.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="17. The Ethics of Predictive Models in Sociotechnical Systems" href="ethics-of-predictive-models.html" />
    <link rel="prev" title="15. Neural Networks" href="neural-networks.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Syllabus</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="goals-and-expectations.html">Goals and Expectations</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="policies.html">Course Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="skills-check.html">Skills Check</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule &amp; Office Hours</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prior-and-posterior.html">21. Bayesian Inference: Prior and Posterior</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictive.html">22. Bayesian Inference: Posterior Predictive</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human/AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Special Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/model-selection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Selection & Evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preventing-over-and-under-fitting">16.1. Preventing Over- and Under-fitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">16.2. Log-Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-regression">16.3. Metrics for Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-classification">16.4. Metrics for Classification</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-selection-evaluation">
<h1><span class="section-number">16. </span>Model Selection &amp; Evaluation<a class="headerlink" href="#model-selection-evaluation" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> At this point, we have a general framework for developing probabilistic models, as well as one way of fitting them to data, MLE. We’ve then instantiated this framework to develop two types of predictive models—regression and classification. We further learned how to build in expressivity using tools from deep learning—namely, neural networks—into these models. Are we finally ready to apply these models to real-life tasks?</p>
<p>Unfortunately, there’s one key piece we’re still missing: so far, we’ve only used 1- and 2-dimensional input data and 1-dimensional output data. While in principle, we already have the tools to implement predictive models for higher dimensional data, we don’t yet have the tools to <em>evaluate</em> them. We’ve purposefully worked with lower dimensional data because it is easy to visualize, and therefore easy to qualitatively evaluate. But as data becomes higher dimensional, it’s much more difficult to get intuition using visualizations. As a result, we will have to rely on <em>metrics</em>.</p>
<p><strong>Challenge:</strong> There are many ways of measuring model performance. Which metrics should we use? What are the pros and cons of each metric? We will answer these questions here. Even though our motivation for developing evaluation metrics is our inability to visualize high-dimensional data, we will in fact focus on low-dimensional data, again. This is because we need to gain intuition about each metric we introduce.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>What’s underfitting/overfitting? How do we prevent it?</p></li>
<li><p>Introduce log-likelihood</p></li>
<li><p>Introduce metrics specific to regression</p></li>
<li><p>Introduce metrics specific to classification</p></li>
</ul>
<p><strong>Data.</strong> You’ve started a collaboration with doctors from IHH’s Center for Rare Disorders. The doctors are interested in better understanding Antenna Inflammation, in which a being’s antennas sporadically inflame for weeks at a time. This causes them to malfunction and is quite painful. Because the disease is so rare, it’s been difficult to gather enough information to develop a treatment. Currently, the only known treatment is to expose the inflamed antennas to high-energy space beams. While these beams do not cure the disease, they do alleviate the pain. Doctors are interested in better understanding what beam intensity to use; they suspect that a low-intensity beam might not help much, but too high of an intensity might also contribute to pain. They have collected data for you to analyze. Let’s take a look:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a bunch of libraries we&#39;ll be using below</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jrandom</span>
<span class="kn">from</span> <span class="nn">cs349</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Load the data into a pandas dataframe</span>
<span class="n">csv_fname</span> <span class="o">=</span> <span class="s1">&#39;data/IHH-CRD-train.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_fname</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Patient ID&#39;</span><span class="p">)</span>

<span class="c1"># Print a random sample of patients, just to see what&#39;s in the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intensity</th>
      <th>Comfort</th>
    </tr>
    <tr>
      <th>Patient ID</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>45</th>
      <td>0.308006</td>
      <td>0.977233</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.502982</td>
      <td>1.359451</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.125683</td>
      <td>0.405900</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.520344</td>
      <td>1.185005</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.404359</td>
      <td>0.955264</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.471006</td>
      <td>1.138226</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.767512</td>
      <td>1.025494</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.534193</td>
      <td>0.828949</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.661706</td>
      <td>1.179636</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.183754</td>
      <td>0.742592</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.378024</td>
      <td>0.670647</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.416132</td>
      <td>0.664669</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.239045</td>
      <td>0.871169</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.449077</td>
      <td>0.933577</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.216425</td>
      <td>0.722563</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Models.</strong> In addition to the data the doctors have collected, they’ve gone ahead and fit some models to the data. They haven’t told you much about the modeling choices they made, but still want your help selecting which model best to use. Let’s load up the models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="n">load_all_regression_models_of_comfort_vs_intensity</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of models loaded:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of models loaded: 6
</pre></div>
</div>
</div>
</div>
<p>We’ve created a helper function for you to visualize all of the models. Let’s have a look:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_all_regression_models_of_comfort_vs_intensity</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b8ce20a34c186e11ab20e302e1db51c799b9f83ab87caa543c423949275592e5.png" src="_images/b8ce20a34c186e11ab20e302e1db51c799b9f83ab87caa543c423949275592e5.png" />
</div>
</div>
<section id="preventing-over-and-under-fitting">
<h2><span class="section-number">16.1. </span>Preventing Over- and Under-fitting<a class="headerlink" href="#preventing-over-and-under-fitting" title="Permalink to this heading">#</a></h2>
<p><strong>Decomposition into Signal vs. Noise.</strong> In probabilistic models, we often term one part of the model the “trend” or the “signal,” and we term the rest “noise” or “observation error.” For example, recall that in regression, our trend is captured by a function of the inputs, <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span>, around which we assume a Gaussian observation error. Similarly, in the classification model we introduced, our trend is captured by a function of the inputs, <span class="math notranslate nohighlight">\(\rho(\cdot; W)\)</span>, which computes the probability of belonging to class 1 (as opposed to class 0). We then sample from a Bernoulli distribution with that probability—this represents the “noise” or “observation error.” When fitting a probabilistic model, we’re always at risk of learning models that mix up what’s signal vs. what’s noise. This can happen in two ways:</p>
<ul class="simple">
<li><p><strong>Overfitting:</strong> When the “signal” part of the model memorizes the “noise” part, and as a result, interpolates poorly. By <em>interpolation</em>, we mean, how well the model fits data points <em>near</em> our training data.</p></li>
<li><p><strong>Underfitting:</strong> When a model doesn’t capture enough of the “signal” in the data (assuming it’s noise), and as a result, interpolates poorly.</p></li>
</ul>
<p>Let’s illustrate what overfitting and underfitting each look like, in both classification and regression:</p>
<figure class="align-center" id="model-fitting-illustration">
<a class="reference internal image-reference" href="_images/model-fitting-illustration.png"><img alt="_images/model-fitting-illustration.png" src="_images/model-fitting-illustration.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16.1 </span><span class="caption-text">Illustration of overfitting and underfitting, adapted from <a class="reference external" href="https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png" rel="noopener noreferrer" target="_blank">this image</a>.</span><a class="headerlink" href="#model-fitting-illustration" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Preventing Inappropriate Decomposition into Signal vs. Noise.</strong> How can we tell if a model overfit or underfit? Well, looking at the definition above, we said a model over/under-fitts if it <em>interpolates</em> poorly. This means that, to determine if a model over/under-fit, we should check its fit <em>on data it has not seen yet</em>—specifically, on data that’s similar to our training data.</p>
<p>But where can we get such extra data points? Oftentimes, it’s hard to collect additional data. For example, at the IHH, data collection is expensive; it’s funded by large government grants and requires approval of an <a class="reference external" href="https://en.wikipedia.org/wiki/Institutional_review_board" rel="noopener noreferrer" target="_blank">Institutional Review Board (IRB)</a> to ensure data collection and analysis is ethical. As a result, we often cannot collect additional data. Instead, however, we can <em>split our data into parts</em>—one part for training and one part for validating the model’s interpolation. Specifically, after fitting the model on the training data, we compare the model’s fit on the training data with its fit on the validation data:</p>
<ul class="simple">
<li><p>If the fit on the training data is better than that on the validation data, we <em>overfit</em>.</p></li>
<li><p>If the fit on neither the training data nor the validation data is good, we <em>underfit.</em></p></li>
<li><p>If the fit on both the training and validation data is comparably good, we fit just right!</p></li>
</ul>
<p>The bottom row of <a class="reference internal" href="#model-fitting-illustration"><span class="std std-numref">Fig. 16.1</span></a> above shows the loss function evaluated on both the training data and the validation set over iterations (or “epochs”) of gradient descent.</p>
<p><strong>Reporting Model Performance.</strong> In practice, we often aren’t just interested in the fitted model—we also want to report some <em>metric</em> to quantify how well it fits. To accomodate this need, we split the data into three parts: a training set, a validation set, and a test set.</p>
<ul class="simple">
<li><p>We fit the model to the <em>training</em> data.</p></li>
<li><p>We use the <em>validation</em> set to determine over/under-fitting (by comparing the fit on the validation vs. training data, as described above).</p></li>
<li><p>Finally, we report the model’s performance on the <em>test</em> set.</p></li>
</ul>
<p>Often, we use <span class="math notranslate nohighlight">\(70\%\)</span> of the data for the training, <span class="math notranslate nohighlight">\(20\%\)</span> for validation, and <span class="math notranslate nohighlight">\(10\%\)</span> for testing.</p>
<p><strong>But which metric should we use?</strong> That’s a million-dollar question! We will next present several common metrics for evaluating model performance numerically. As you will see, model performance is multi-faceted—there’s more to consider than just how well the model fits. For example, is the model fair? and will the model be understandable to a human? If we have multiple factors to consider, how can we possibly design a metric that gives us a single ranking of models from best to worst? The answer is: we can’t. We will have to use multiple metrics to evaluate our models. At times, these metrics will present us with contradictory information, and we will have to determine what to do.</p>
</section>
<section id="log-likelihood">
<h2><span class="section-number">16.2. </span>Log-Likelihood<a class="headerlink" href="#log-likelihood" title="Permalink to this heading">#</a></h2>
<p><strong>A General Purpose Metric.</strong> Since so far, we fit our models by finding parameters that maximize the data log-likelihood (MLE). Can we also use log-likelihood as an evaluation metric? If we trust this metric for <em>fitting</em> our model, surely we should trust it for <em>evaluation</em> as well. And what’s nice about log-likelihood is that, unlike some of the other metrics we’ll introduce next, it’s not model-specific. We can evaluate the log-likelihood of <em>any</em> probabilistic model.</p>
<p><strong>Log-Likelihood as an Evaluation Metric.</strong> Recall that the data log-likelihood is,</p>
<div class="amsmath math notranslate nohighlight" id="equation-2562a0f7-4a32-484c-a2f0-c65f6179f44b">
<span class="eqno">(16.1)<a class="headerlink" href="#equation-2562a0f7-4a32-484c-a2f0-c65f6179f44b" title="Permalink to this equation">#</a></span>\[\begin{align}
\log p(\mathcal{D}; \theta) &amp;= \sum\limits_{n=1}^N \log p(\mathcal{D}_n; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is our entire data set of <span class="math notranslate nohighlight">\(N\)</span> observations, each <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is an observation, and <span class="math notranslate nohighlight">\(\theta\)</span> is the model parameters. However, notice that in this definition, the data log-likelihood grows as <span class="math notranslate nohighlight">\(N\)</span> increases due to the sum. This means that, if our train, validation and test sets each have a different number of points, we cannot compare their log-likelihoods directly. To fix this problem, we instead compute the <em>average</em> log-likelihood (this is just the above equation divided by <span class="math notranslate nohighlight">\(N\)</span>):</p>
<div class="amsmath math notranslate nohighlight" id="equation-e1b90b3f-898c-454a-9e5a-9cfec0e2884e">
<span class="eqno">(16.2)<a class="headerlink" href="#equation-e1b90b3f-898c-454a-9e5a-9cfec0e2884e" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{Avg. Log-Likelihood} = \frac{1}{N} \sum\limits_{n=1}^N \log p(\mathcal{D}_n; \theta).
\end{align}\]</div>
<div class="admonition-exercise-getting-intuition-for-log-likelihood admonition">
<p class="admonition-title">Exercise: Getting Intuition for Log-Likelihood</p>
<p><strong>Part 1:</strong> Looking at the plots of the models above, rank them from what you’d consider best to worst. How did you arrive at this ranking? Please relate your answer to the broader context of the problem.</p>
<p><strong>Part 2:</strong> For each model, evaluate the average log-likelihood on the training, validation, and test set. Do this by,</p>
<ul>
<li><p>Loading in the list of models (using the function we provided above). Each model in the list has the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Expects x to be of shape (N, 1)</span>
    <span class="c1"># Expects y to be of shape (N, 1)</span>
    <span class="k">pass</span> <span class="c1"># already implemented</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of points, <code class="docutils literal notranslate"><span class="pre">x</span></code> are the inputs, and <code class="docutils literal notranslate"><span class="pre">y</span></code> are the outputs.</p>
</li>
<li><p>Loading the training, validation, and test data sets from <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-train.csv</span></code>, <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-val.csv</span></code>, and <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-test.csv</span></code>.</p></li>
<li><p>Finally, compute the average log-likelihood. Please use the helper function we created, <code class="docutils literal notranslate"><span class="pre">cs349_joint_data_log_likelihood(model,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>, where <code class="docutils literal notranslate"><span class="pre">*args,</span> <span class="pre">**kwargs</span></code> capture all arguments your model takes in: <code class="docutils literal notranslate"><span class="pre">N,</span> <span class="pre">x,</span> <span class="pre">y=None</span></code>. Note that our helper function computes the <em>joint data</em> log-likelihood, not the average log-likelihood.</p></li>
</ul>
<p><strong>Part 3:</strong> Rank the models using log-likelihood on the training set and report log-likelihood test set. Did this method succeed in ranking the best-fitting model on top? Next, rank the models using validation set and report the test log-likelihood. Did this method successfully rank the best-fitting model on top?</p>
<p><strong>Part 4:</strong> How did the ranking you got from using the validation set compare with your initial ranking from part (1)? What does log-likelihood penalize models for?</p>
</div>
<p><strong>Human-meaningful Notions of Model-fit.</strong> So log-likelihood seems to work well when <em>comparing</em> models; given a collection of trained models, we can use log-likelihood on the validation set to select a model that is likely to fit the test set well. However, suppose we’re given a single model—how do we know if this model is good? For example, if our model’s log-likelihood is <span class="math notranslate nohighlight">\(0.123\)</span>, is that good? is that bad? The <em>units</em> of log-likelihood hold little meaning to us. Because of this, log-likelihood is difficult to interpret on its own, leading us to search for other metrics we can more easily interpret.</p>
</section>
<section id="metrics-for-regression">
<h2><span class="section-number">16.3. </span>Metrics for Regression<a class="headerlink" href="#metrics-for-regression" title="Permalink to this heading">#</a></h2>
<p><strong>Mean Squared Error (MSE).</strong> Since log-likelihood is hard to interpret on its own, we will now introduce another metric, MSE, specific to regression. This metric can be used together with log-likelihood to paint a clearer picture of model fit. This is, of course, not the only evaluation metric for regression, but it’s a common one, so we’ll focus on it here. For 1-dimensional output data, the MSE is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-01c7e884-6c7f-4ffe-8063-a73c6f3eb26e">
<span class="eqno">(16.3)<a class="headerlink" href="#equation-01c7e884-6c7f-4ffe-8063-a73c6f3eb26e" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{MSE} &amp;= \frac{1}{N} \sum\limits_{n=1}^N (\underbrace{y_n - \widehat{y}_n}_{\text{error}})^2,
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_n\)</span> is an observation and <span class="math notranslate nohighlight">\(\widehat{y}_n\)</span> the model’s prediction. As the name suggests, we compute the error between our model’s prediction and the observation, square it, and average. The MSE can be thought of as a measure of “spread” (or, in statistical lingo, the <em>variance</em>) of the data around the model’s prediction. Hopefully, in contrast to log-likelihood, it’s a little easier to understand, since we may have domain knowledge of what error, <span class="math notranslate nohighlight">\(y_n - \widehat{y}_n\)</span>, is considered high/low.</p>
<p>But for a probabilistic model, what is the model’s “prediction”? In a probabilistic regression model, we have a <em>distribution</em> of outputs given inputs, <span class="math notranslate nohighlight">\(p_{Y | X}\)</span>, and a distribution is <em>not</em> a single prediction. Because of this, we have to summarize the conditional distribution using a single value. Here, we will use the mean (which also happens to be the mode—or the most likely value of <span class="math notranslate nohighlight">\(y\)</span>). For our regression model, the mean of the distribion is the “trend,” so</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d081433-f553-4f92-b5ae-11e1b4dd884d">
<span class="eqno">(16.4)<a class="headerlink" href="#equation-4d081433-f553-4f92-b5ae-11e1b4dd884d" title="Permalink to this equation">#</a></span>\[\begin{align}
\hat{y}_n &amp;= \mu(x_n; W^\text{MLE}).
\end{align}\]</div>
<div class="admonition-exercise-implement-and-use-mse admonition">
<p class="admonition-title">Exercise: Implement and Use MSE</p>
<p><strong>Part 1:</strong> Write a function to compute the MSE. The models are set up such calling <code class="docutils literal notranslate"><span class="pre">cs349_sample_generative_process</span></code> will return a Python dictionary with key <code class="docutils literal notranslate"><span class="pre">mu</span></code>, containing <span class="math notranslate nohighlight">\(\mu(x_n; W^\text{MLE})\)</span>.</p>
<p><strong>Part 2:</strong> For each model, evaluate the MSE on the training, validation, and test set.</p>
<p><strong>Part 3:</strong> Rank the models using validation MSE and report the test MSE. Does this ranking disagree with the log-likelihood ranking? If so, where? <em>Hint: look at the observation noise.</em></p>
<p><strong>Part 4:</strong> Are there any other factors we should have considered in our evaluation of the models? Let’s load the training, validation, and test data sets from <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-train-augmented.csv</span></code>, <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-val-augmented.csv</span></code>, and <code class="docutils literal notranslate"><span class="pre">data/IHH-CRD-test-augmented.csv</span></code>. These data sets are the same as before, just with a previously omitted feature: race. As it turns out, the intervention (the space beam), was only tested on one of these races and not the other. Create a scatter plot of the training data using color to distinguish between the two races.</p>
<ul class="simple">
<li><p>What’s different about the trend for the two races? What’s the same?</p></li>
<li><p>How do you think this will affect the model’s predictive performance on each of the two races? Why?</p></li>
</ul>
<p><strong>Part 5:</strong> Re-compute the log-likelihood and MSE of each model on each of these data sets, this time evaluating each metric separately for each race. For which race does the model perform better on?</p>
</div>
</section>
<section id="metrics-for-classification">
<h2><span class="section-number">16.4. </span>Metrics for Classification<a class="headerlink" href="#metrics-for-classification" title="Permalink to this heading">#</a></h2>
<p><strong>Accuracy.</strong> Since log-likelihood is hard to interpret on its own, we will now introduce another metric, accuracy, that’s specific to classification. Simply put, accuracy is the fraction of predictions the model got correct:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b985e5bc-1968-4c37-ba44-526f50da4d4a">
<span class="eqno">(16.5)<a class="headerlink" href="#equation-b985e5bc-1968-4c37-ba44-526f50da4d4a" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{Accuracy} &amp;= \frac{1}{N} \underbrace{\sum\limits_{n=1}^N \mathbb{I}(y_n = \widehat{y}_n),}_{\text{count correct predictions}}
\end{align}\]</div>
<p>where, as before, <span class="math notranslate nohighlight">\(\widehat{y}_n\)</span> is the model’s prediction, and <span class="math notranslate nohighlight">\(\mathbb{I}(\cdot)\)</span> is an indicator function—a function that evaluates to <span class="math notranslate nohighlight">\(1\)</span> when the condition inside is true and <span class="math notranslate nohighlight">\(0\)</span> otherwise. The sum over the indicators counts the number of correct predictions. When divided by <span class="math notranslate nohighlight">\(N\)</span>, we get the fraction of correct predictions.</p>
<p>Just like before, a probabilistic model does not make “predictions;” it models a full distribution over outputs given inputs. We, again, have to choose how to summarize this distribution using a single value. Remember that in our classification model, <span class="math notranslate nohighlight">\(\rho(x_n)\)</span> outputted the probability of <span class="math notranslate nohighlight">\(x_n\)</span> belonging to class 1 (vs. class 0). To turn this into a prediction, we will threshold it: if <span class="math notranslate nohighlight">\(\rho(x_n) \geq 0.5\)</span>, we’ll say <span class="math notranslate nohighlight">\(x_n\)</span> belongs to class 1—otherwise, it belongs to class 0. We can formalize this using an indicator function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cdf08f33-2cdb-4826-a8ac-8cf589a557bb">
<span class="eqno">(16.6)<a class="headerlink" href="#equation-cdf08f33-2cdb-4826-a8ac-8cf589a557bb" title="Permalink to this equation">#</a></span>\[\begin{align}
\widehat{y}_n &amp;= \mathbb{I}(\rho(x_n) \geq 0.5)
\end{align}\]</div>
<div class="admonition-exercise-the-shortcomings-of-accuracy admonition">
<p class="admonition-title">Exercise: The Shortcomings of Accuracy</p>
<p><strong>Context.</strong> Since the disease is incredibly rare, occurring only in <span class="math notranslate nohighlight">\(0.1\%\)</span> of the population, doctors don’t yet know what are its causes. The doctors have managed to acquire a large fund to collect data about a large swath of the population, ranging from genetic factors to environmental factors (demographics, occupation, diet, physiology, etc.). They just finished collecting the data and have run a preliminary analysis to determine which factors contribute most.</p>
<p><strong>Challenge.</strong> Even though their model is <span class="math notranslate nohighlight">\(99.9\%\)</span> accurate, they have found that none of the factors seem to matter to the model—how is this possible? They recruited you to help understand what’s going on with their model.</p>
<p><strong>Part 1:</strong> You decided to inspect the parameters of the model. In the model, <span class="math notranslate nohighlight">\(1\)</span> represents having the disease and <span class="math notranslate nohighlight">\(0\)</span> represents no disease. Lucky for you, the doctors used a linear classification model (so interpreting it is possible):</p>
<div class="amsmath math notranslate nohighlight" id="equation-20272ca1-a5c8-47f5-95eb-ea6addb2dca2">
<span class="eqno">(16.7)<a class="headerlink" href="#equation-20272ca1-a5c8-47f5-95eb-ea6addb2dca2" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{Y | X}(y_n | x_n; \theta) &amp;= \mathrm{Ber}\left( \rho(x_n; \theta) \right) 
\end{align}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-6bffca03-77ab-4702-ac1b-a23934cae046">
<span class="eqno">(16.8)<a class="headerlink" href="#equation-6bffca03-77ab-4702-ac1b-a23934cae046" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho(x_n; \theta) &amp;= \mathrm{sigmoid}\left( \theta^{(0)} + \sum\limits_{d=1}^{D_x} \theta^{(d)} \cdot x_n^{(d)} \right),
\end{align}\]</div>
<p>where the <span class="math notranslate nohighlight">\((d)\)</span>-superscript notation represents the <span class="math notranslate nohighlight">\(d\)</span>th dimension, and <span class="math notranslate nohighlight">\(D_x\)</span> is the dimensionality of the inputs <span class="math notranslate nohighlight">\(x_n\)</span>. When inspecting the parameters, you found that,</p>
<div class="amsmath math notranslate nohighlight" id="equation-8c477227-b35b-4987-8336-50f605e60a92">
<span class="eqno">(16.9)<a class="headerlink" href="#equation-8c477227-b35b-4987-8336-50f605e60a92" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;= [\underbrace{-6.90675}_{\theta^{(0)}}, \underbrace{0.0, 0.0, \dots, 0.0}_{\theta^{(1)}, \dots, \theta^{(D_x)}}].
\end{align}\]</div>
<p>Looking at the parameters, <span class="math notranslate nohighlight">\(\theta\)</span>, which dimensions of the input matter most?</p>
<p><strong>Part 2:</strong> How can such a model achieve such high accuracy?</p>
<p><strong>Part 3:</strong> Suppose the IHH were to deploy this model as a screening tool for beings coming into the IHH’s emergency room. The model would be used to identify beings at high risk of Antenna Infection for early treatment. What problems may arise?</p>
</div>
<p><strong>Confusion Matrices.</strong> Although accuracy is easy to evaluate and understand, it has some major shortcomings, as you’ve shown in the exercise above. Confusion matrices aim to address this issue by looking at the ways in which the model makes mistakes when it’s incorrect. Why are they called “confusion” matrices? Maybe to alleviate our confusion? Unclear. Anyways, we define a confusion matrix as follows:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Predicted: Disease</strong></p></th>
<th class="head"><p><strong>Predicted: No Disease</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actual: Disease</strong></p></td>
<td><p><em>True Positives:</em> The number of patients with the disease who were correctly classified as having the disease.</p></td>
<td><p><em>False Negatives:</em> The number of patients with the disease who were incorrectly classified as not having the disease.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actual: No Disease</strong></p></td>
<td><p><em>False Positives:</em> The number of patients without the disease who were incorrectly classified as having the disease.</p></td>
<td><p><em>True Negatives:</em> The number of patients without the disease who were correctly classified as having no disease.</p></td>
</tr>
</tbody>
</table>
<p>As you can see, a confusion matrix presents a breakdown of the model’s correct and incorrect predictions.</p>
<div class="admonition-exercise-understanding-confusion-matrices admonition">
<p class="admonition-title">Exercise: Understanding Confusion Matrices</p>
<p><strong>Part 1:</strong> Write down the confusion matrix for the classifier presented to you by the doctors in the previous problems (in terms of the number of patients in the data, <span class="math notranslate nohighlight">\(N\)</span>).</p>
<p><strong>Part 2:</strong> To try to fix the problem, the doctors trained two more classifiers on their data set. As per your suggestion, they are now computing the confusion matrices for both (instead of looking at accuracy alone):</p>
<ul>
<li><p>Model 1:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{Accuracy} = 80.1\%, \quad \text{Confusion Matrix} = \begin{bmatrix} 500 &amp; 9500 \\ 190000 &amp; 800000 \\ \end{bmatrix}.
    \end{align*}\]</div>
</li>
<li><p>Model 2:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{Accuracy} = 67.1\%, \quad \text{Confusion Matrix} = \begin{bmatrix} 700 &amp; 324300 \\ 5000 &amp; 670000 \\ \end{bmatrix}.
    \end{align*}\]</div>
</li>
</ul>
<p>For the screening tool described in the previous problem, which model would you prefer? Why?</p>
<p><strong>Part 3:</strong> Make up a scenario in which you would prefer the model you didn’t choose? What differentiates your scenario from the one about the screening tool?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="neural-networks.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="ethics-of-predictive-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>The Ethics of Predictive Models in Sociotechnical Systems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preventing-over-and-under-fitting">16.1. Preventing Over- and Under-fitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">16.2. Log-Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-regression">16.3. Metrics for Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-classification">16.4. Metrics for Classification</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024 Yaniv Yacoby
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>