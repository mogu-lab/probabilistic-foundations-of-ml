

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Optimization &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optimization';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/optimization.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="11. Probability (Continuous)" href="probability-continuous.html" />
    <link rel="prev" title="9. Maximum Likelihood: Code" href="mle-code.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Syllabus</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="goals-and-expectations.html">Goals and Expectations</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="policies.html">Course Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="skills-check.html">Skills Check</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule &amp; Office Hours</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prior-and-posterior.html">21. Bayesian Inference: Prior and Posterior</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictive.html">22. Bayesian Inference: Posterior Predictive</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human/AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Special Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optimization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analytic-solutions-to-optimization-problems">10.1. Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-analytically-solving-for-the-mle">10.2. An Example: Analytically Solving for the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-analytic-solutions-to-optimization-problems">10.3. Challenges with Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-solutions-to-optimization-problems">10.4. Numeric Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-numeric-optimization">10.5. Challenges with Numeric Optimization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<h1><span class="section-number">10. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> We can use <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> to perform the MLE on a class of models, composed of discrete distributions. The MLE involves solving an optimization problem—finding the parameters that maximize the joint data likelihood. So far, we’ve let <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> take care of this optimization problem for us. But what is <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> doing under the hood?</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce (exact) analytical optimization, which rely on person to do most of the work via pen-and-paper math.</p></li>
<li><p>Introduce (approximate) numerical optimization, which rely on the machine to do the work for us, like <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</p></li>
<li><p>Discuss tradeoffs between the two.</p></li>
</ul>
<section id="analytic-solutions-to-optimization-problems">
<h2><span class="section-number">10.1. </span>Analytic Solutions to Optimization Problems<a class="headerlink" href="#analytic-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p><strong>Goal:</strong> Recall that our goal is to maximize the joint data log-likelihood:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d2811ee1-1a56-4f37-bef4-77b6eaf28df1">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-d2811ee1-1a56-4f37-bef4-77b6eaf28df1" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_\theta \text{ } p(\mathcal{D}; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> represents our model parameters. We will remain consistent with the convention of ML literature; we re-write the above problem as a minimization problem of our <em>negative</em> log-likelihood.</p>
<div class="amsmath math notranslate nohighlight" id="equation-21073ac7-aec0-4423-b2fc-4e3b62b55e54">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-21073ac7-aec0-4423-b2fc-4e3b62b55e54" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmin}_\theta \underbrace{-p(\mathcal{D}; \theta)}_{\text{Our loss function, } \mathcal{L}(\theta)}
\end{align}\]</div>
<p>We call the <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> our “loss function,” since our goal is now to minimize our losses.</p>
<p><strong>Intuition:</strong> So how can we identify the minima of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>? A good place to start is to determine what makes <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> different at its minima. Let’s see if we can get some intuition by looking at some intuition by looking at loss functions we made up.</p>
<figure class="align-center" id="fig-example-loss-functions">
<a class="reference internal image-reference" href="_images/example_loss_functions.png"><img alt="_images/example_loss_functions.png" src="_images/example_loss_functions.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.1 </span><span class="caption-text">Two “made-up” loss functions—what properties do the minima share?</span><a class="headerlink" href="#fig-example-loss-functions" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Looking at the left plot above, we see that the minimum has a unique property: the loss function is <em>flat at the minimum</em>. In other words, at the minimum, the derivative of the loss function equals zero:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f22b9e38-129a-4e3e-8904-9405a5a499da">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-f22b9e38-129a-4e3e-8904-9405a5a499da" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{d \mathcal{L}(\theta)}{d \theta} = 0
\end{align}\]</div>
<p>Does the same hold for the right plot? Not exactly… At the minimum, we do have <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span>. But there are other points for which we also have <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span>.
This shows us that there are two <em>types</em> of optima: <em>global</em> and <em>local</em>. The <em>global minima</em> are the <span class="math notranslate nohighlight">\(\theta\)</span>’s that make <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> smaller than all other <span class="math notranslate nohighlight">\(\theta\)</span>’s. On the other hand, <em>local minima</em> are <span class="math notranslate nohighlight">\(\theta\)</span>’s that make <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> smaller than all other <span class="math notranslate nohighlight">\(\theta\)</span>’s in their neighborhood. Let’s make this clear with the following illustration.</p>
<figure class="align-center" id="fig-global-vs-local-optima">
<a class="reference internal image-reference" href="_images/global-vs-local-optima.png"><img alt="_images/global-vs-local-optima.png" src="_images/global-vs-local-optima.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.2 </span><span class="caption-text">Types of optima. Adapted from <a class="reference external" href="https://ludovicarnold.com/teaching/optimization/problem-statement/" rel="noopener noreferrer" target="_blank">this website</a>.</span><a class="headerlink" href="#fig-global-vs-local-optima" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>So if our idea of setting the gradient of the loss to zero gives us both global- and local- minima and maxima, is it still useful? Yes, looking at the <span class="math notranslate nohighlight">\(\theta\)</span>’s for which the loss is flat is still helpful! Instead of looking at every possible value of <span class="math notranslate nohighlight">\(\theta\)</span> (in these plots, <span class="math notranslate nohighlight">\(\theta\)</span> can take on infinite different values), we just need to examine the points for which the loss is flat. For the plot on the left, this strategy directly found the global minimum. For the plot on the right, this strategy found a small set of points that <em>includes</em> the global minimum. To find the global minimum within this set, all we need to do is evaluate the loss at each point and select the one that yields the smallest loss.</p>
<p><strong>Procedure:</strong> We can turn this intuition into the following four-step process.</p>
<ol class="arabic simple">
<li><p>Compute the derivative of the loss function: <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta}\)</span></p></li>
<li><p>Set the derivative of the loss function equal to zero: <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span></p></li>
<li><p>Solve the equation for <em>all possible values</em> of <span class="math notranslate nohighlight">\(\theta\)</span> analytically (i.e. on pen-and-paper)—this is the difficult part!</p></li>
<li><p>Plug each possible value of <span class="math notranslate nohighlight">\(\theta\)</span> into our loss function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and select the one(s) that minimize it.</p></li>
</ol>
</section>
<section id="an-example-analytically-solving-for-the-mle">
<h2><span class="section-number">10.2. </span>An Example: Analytically Solving for the MLE<a class="headerlink" href="#an-example-analytically-solving-for-the-mle" title="Permalink to this heading">#</a></h2>
<p><strong>The Model.</strong> Let’s see how this works by analytically performing the MLE on a simple example. Suppose we want to model the probability of a patient being hospitalized overnight. We can do this using a Bernoulli distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a2b69d6e-00b6-4059-b483-57bb30d47ae2">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-a2b69d6e-00b6-4059-b483-57bb30d47ae2" title="Permalink to this equation">#</a></span>\[\begin{align}
H \sim p_H(\cdot; \rho) = \mathrm{Bern}(\rho).
\end{align}\]</div>
<p>Recall that the PMF of a Bernoulli RV is,</p>
<div class="amsmath math notranslate nohighlight" id="equation-7be1c60d-b072-496f-873a-887d141e00ae">
<span class="eqno">(10.5)<a class="headerlink" href="#equation-7be1c60d-b072-496f-873a-887d141e00ae" title="Permalink to this equation">#</a></span>\[\begin{align}
p_H(h; \rho) = \rho^{\mathbb{I}(h = 1)} \cdot (1 - \rho)^{\mathbb{I}(h = 0)},
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{I}(\cdot)\)</span> is an <em>indicator variable</em>—it evaluates to 1 if the condition in parentheses is true and 0 otherwise.</p>
<p><strong>The Joint Data Likelihood.</strong> Now, let’s write the joint data log-likelihood for our model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-30d80303-eebd-4883-a570-ff42cf21e871">
<span class="eqno">(10.6)<a class="headerlink" href="#equation-30d80303-eebd-4883-a570-ff42cf21e871" title="Permalink to this equation">#</a></span>\[\begin{align}
\log p(\mathcal{D}; \rho) &amp;= \log \prod\limits_{n=1}^N p(\mathcal{D}_n; \rho) \quad (\text{since observations are i.i.d}) \\
&amp;= \log \prod\limits_{n=1}^N p_H(h_n; \rho) \\
&amp;= \log \prod\limits_{n=1}^N \rho^{\mathbb{I}(h_n = 1)} \cdot (1 - \rho)^{\mathbb{I}(h_n = 0)} \quad (\text{using the definition of Bernoulli PMF}) \\
&amp;= \sum\limits_{n=1}^N \log \rho^{\mathbb{I}(h_n = 1)} + \log (1 - \rho)^{\mathbb{I}(h_n = 0)} \quad (\text{using the fact that } \log (x \cdot y) = \log x + \log y) \\
&amp;= \sum\limits_{n=1}^N \mathbb{I}(h_n = 1) \cdot \log \rho + \mathbb{I}(h_n = 0) \cdot \log (1 - \rho) \quad (\text{using the fact that } \log x^y = y \cdot \log x)  \\
&amp;= \underbrace{\left( \sum\limits_{n=1}^N \mathbb{I}(h_n = 1) \right)}_{\text{Total number of times $H = 1$}} \cdot \log \rho + \underbrace{\left( \sum\limits_{n=1}^N \mathbb{I}(h_n = 0) \right)}_{\text{Total number of times $H = 0$}} \cdot \log (1 - \rho) \quad (\text{moving terms that do not depend on the sums out}) \\
&amp;= T \cdot \log \rho + (N - T) \cdot \log (1 - \rho)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(T = \sum\limits_{n=1}^N \mathbb{I}(h_n = 1)\)</span> is the total number of hospitalizations.</p>
<p><strong>The MLE Objective.</strong> Our MLE objective is therefore:</p>
<div class="amsmath math notranslate nohighlight" id="equation-40784d90-c167-453b-8746-6d7e9ae73ed6">
<span class="eqno">(10.7)<a class="headerlink" href="#equation-40784d90-c167-453b-8746-6d7e9ae73ed6" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} &amp;= \mathrm{argmax}_{\rho} \text{ } \log p(\mathcal{D}; \rho) \\
&amp;= \mathrm{argmax}_{\rho} \left( T \cdot \log \rho - (N - T) \cdot \log (1 - \rho) \right) \\
&amp;= \mathrm{argmin}_{\rho} \underbrace{\left( -T \cdot \log \rho + (N - T) \cdot \log (1 - \rho) \right)}_{\text{Our loss function: } \mathcal{L}(\rho)} \quad (\text{maximizing a function is equivalent to minimizing its negative})
\end{align}\]</div>
<p><strong>Analytic Optimization.</strong> We take the gradient of the above loss <span class="math notranslate nohighlight">\(\mathcal{L}(\rho)\)</span> with respect to <span class="math notranslate nohighlight">\(\rho\)</span>, set it to <span class="math notranslate nohighlight">\(0\)</span> and solve:</p>
<div class="amsmath math notranslate nohighlight" id="equation-95248db4-6a68-4885-ad44-f67fd3abddd3">
<span class="eqno">(10.8)<a class="headerlink" href="#equation-95248db4-6a68-4885-ad44-f67fd3abddd3" title="Permalink to this equation">#</a></span>\[\begin{align}
0 &amp;= \frac{d \mathcal{L}(\rho)}{d \rho} \\
&amp;= -\frac{T}{\rho} + \frac{N - T}{1 - \rho} \quad (\text{taking the derivative of } \mathcal{L}(\rho)) \\
&amp;= \frac{T - N \cdot \rho}{\rho \cdot (\rho - 1)} \quad (\text{bringing fractions under common denominator}) \\
&amp;= T - N \cdot \rho \quad (\text{multiplying both sides by } \rho \cdot (\rho - 1))
\end{align}\]</div>
<p>Solving the above gives us the solution,</p>
<div class="amsmath math notranslate nohighlight" id="equation-66a81238-7362-4072-beaf-ca1e2087cc8e">
<span class="eqno">(10.9)<a class="headerlink" href="#equation-66a81238-7362-4072-beaf-ca1e2087cc8e" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho &amp;= \frac{T}{N}.
\end{align}\]</div>
<p>The solution is exactly the proportion of hospitalizations out of the total number of hospital visits!</p>
<p><strong>A Note on Constraint Optimization.</strong> Oftentimes, when performing the MLE analytically, we need to constrain the parameters to lie within valid ranges. For example, <span class="math notranslate nohighlight">\(\rho\)</span> should only be allowed to take on values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Formally, we express such a constraint as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-fd5b397e-fc42-4e13-9b54-4694a174bdbb">
<span class="eqno">(10.10)<a class="headerlink" href="#equation-fd5b397e-fc42-4e13-9b54-4694a174bdbb" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} &amp;= \mathrm{argmax}_{\rho} \text{ } \log p(\mathcal{D}; \rho) \quad \text{subject to} \quad 0 \leq \rho \leq 1 \\
\end{align}\]</div>
<p>In our derivation, it just so happens that the solution already satisfies this constraint. However, for different models, we may have to enforce such constraints explicitly. To enforce these constraints, one can use <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener noreferrer" target="_blank">lagrange multipliers</a>. We will not cover this method in here, but just want to point out that this class of methods exists.</p>
</section>
<section id="challenges-with-analytic-solutions-to-optimization-problems">
<h2><span class="section-number">10.3. </span>Challenges with Analytic Solutions to Optimization Problems<a class="headerlink" href="#challenges-with-analytic-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p>As you can see from the above example, performing optimization analytically suffers from two challenges:</p>
<ol class="arabic simple">
<li><p><strong>Analytic optimization needs a specialized solution for every model.</strong> As you can see from the above example, performing the optimization analytically will require a new derivation for each model. However, when working with real data, we rarely know what’s the “right” model a priori. We typically start with an exploratory data analysis, try different models, evaluate them using different metrics, make hypotheses for why the models don’t fit well, revise the models to fit better, and repeat. If we had to derive a new solution for every model we wish to test, our modeling process will become quite cumbersome. Moreover, we are more prone to make errors in the derivation and write bugs in our code.</p></li>
<li><p><strong>Analytic optimization cannot solve for the parameters of every model.</strong> Since the above example is for a simple Bernoulli model, an analytic solution to the MLE exists. However, for more complex problems, that may not be the case. In fact, for modern ML models, it is <em>rare</em> for there to exist an analytic solution.</p></li>
</ol>
<p>This motivates us to look into alternative optimization methods: numeric optimization.</p>
</section>
<section id="numeric-solutions-to-optimization-problems">
<h2><span class="section-number">10.4. </span>Numeric Solutions to Optimization Problems<a class="headerlink" href="#numeric-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p>Numeric optimizations algorithms address both shortcomings of analytic optimization:</p>
<ol class="arabic simple">
<li><p>They can be easily applied to different models without cumbersome derivations. This is because they can be conveniently implemented behind abstractions. Moreover, these abstractions allow us to pair different numeric optimization algorithms with different models <em>without having to write much code</em>. They even make it easy to incorporate constraints over the parameters into the optimization. This helps us write bug-free, error-free optimization code.</p></li>
<li><p>They can be applied to optimization problems for which there exists no analytic optimization solution. They are also fast and work well in practice, making them extremely popular for complicated modern ML models.</p></li>
</ol>
<p>Of course, these numerical algorithms have their own challenges—we’ll look into some challenges they face in a bit. Let us introduce the simplest and most popular numerical optimization algorithm: <em>gradient descent</em>.</p>
<p><strong>Gradient Descent.</strong> The gradient (or “multivariate derivative”) is the direction of the steepest ascent. Similarly, the negative gradient is the direction of the steepest descent. The idea behind the <em>gradient descent</em> algorithm is to take little steps in the direction of the negative gradient in hope that, after taking enough steps, we’ll reach the minimum. Let’s illustrate this with an animation:</p>
<figure class="align-center" id="fig-gradient-descent-gif">
<a class="reference internal image-reference" href="_images/gradient-descent-2d.gif"><img alt="_images/gradient-descent-2d.gif" src="_images/gradient-descent-2d.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.3 </span><span class="caption-text">Animation of gradient descent on a 2-dimensional function. GIF from <a class="reference external" href="https://towardsai.net/p/machine-learning/deep-learning-from-scratch-in-modern-c-gradient-descent" rel="noopener noreferrer" target="_blank">this website</a>.</span><a class="headerlink" href="#fig-gradient-descent-gif" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In the animation, the vertical axis represents <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>. The two horizontal axes represent the dimensions of <span class="math notranslate nohighlight">\(\theta\)</span> (in this case, it’s 2-dimensional). Each arrow represents the negative gradient. As you can see, in each iteration of the algorithm, <span class="math notranslate nohighlight">\(\theta\)</span> (the red point) moves in the direction of the gradient, progressively minimizing the loss.</p>
<p>Now, let’s write the gradient descent algorithm. For clarity, we’ll write it out with the notation for a 1-dimensional <span class="math notranslate nohighlight">\(\theta\)</span> (the multivariate version is pretty much the same).</p>
<div class="proof algorithm admonition" id="gradient-descent">
<p class="admonition-title"><span class="caption-number">Algorithm 10.1 </span> (Gradient Descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs.</strong> A loss function, <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, a choice for the learning rate, <span class="math notranslate nohighlight">\(\alpha\)</span>, and an initialization for the parameters, <span class="math notranslate nohighlight">\(\theta^\text{current} \leftarrow \text{initial value}\)</span>.</p>
<p><strong>Output.</strong> Return a parameter <span class="math notranslate nohighlight">\(\theta\)</span> that (hopefully) minimizes the loss <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p>
<p><strong>Algorithm.</strong> Repeat until the loss doesn’t change much from iteration to iteration:</p>
<ol class="arabic simple">
<li><p>Compute the gradient of the loss with respect to the parameters, evaluated at the current value of the parameters:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-befe6d65-5824-49d4-8f16-832aadb1dcae">
<span class="eqno">(10.11)<a class="headerlink" href="#equation-befe6d65-5824-49d4-8f16-832aadb1dcae" title="Permalink to this equation">#</a></span>\[\begin{align}
    u &amp;\leftarrow \frac{d \mathcal{L}(\theta)}{d \theta} \Bigg|_{\theta = \theta^\text{current}}
  \end{align}\]</div>
<ol class="arabic simple" start="2">
<li><p>Take a step in the direction of the negative gradient (steepest descent):</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-cc750427-777d-4921-8a17-2795a1f6382a">
<span class="eqno">(10.12)<a class="headerlink" href="#equation-cc750427-777d-4921-8a17-2795a1f6382a" title="Permalink to this equation">#</a></span>\[\begin{align}
    \theta^\text{next} &amp;\leftarrow \theta^\text{current} - \alpha \cdot u
  \end{align}\]</div>
<ol class="arabic simple" start="3">
<li><p>Update the model parameters:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-1d413e89-b995-4034-a7a3-553e5a0ed16f">
<span class="eqno">(10.13)<a class="headerlink" href="#equation-1d413e89-b995-4034-a7a3-553e5a0ed16f" title="Permalink to this equation">#</a></span>\[\begin{align}
    \theta^\text{current} &amp;\leftarrow \theta^\text{next}
  \end{align}\]</div>
</section>
</div><p>In this algorithm, notice that there’s one variable we have yet to define: <span class="math notranslate nohighlight">\(\alpha\)</span>. Here, <span class="math notranslate nohighlight">\(\alpha\)</span> represents the size of the step we plan to take in the direction of the gradient. It is typically called the <em>learning rate</em>. You will have to play with this parameter to determine what value works best for minimizing your loss. According to lore, a good place to start is with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>. Why? Who knows…</p>
<p><strong>Simple Implementation of Gradient Descent.</strong> Even though <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> already comes with <a class="reference external" href="https://num.pyro.ai/en/stable/optimizers.html" rel="noopener noreferrer" target="_blank">several different gradient-based optimization algorithms</a>, let’s implement the above univariate gradient descent algorithm. While we’re at it, let’s have the algorithm keep track of how our parameters <span class="math notranslate nohighlight">\(\theta\)</span> change with each iteration to get some more intuition:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>

<span class="k">def</span> <span class="nf">univariate_gradient_descent</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">theta_init</span><span class="p">):</span>
    <span class="c1"># Initialize theta     </span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_init</span>

    <span class="c1"># For each iteration of the algorithm...</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="c1"># Use Jax to compute the gradient of the loss with respect to theta</span>
        <span class="n">gradient_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span>

        <span class="c1"># Evaluate the gradient at the current value of theta</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">gradient_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Take a step in the direction of the gradient</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">u</span>

    <span class="k">return</span> <span class="n">theta</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s see if it can successfully find the minimum of a quadratic loss function: <span class="math notranslate nohighlight">\(\mathcal{L}(\theta) = \theta^2\)</span>. Since this is a parabola, we know the minimum of this formula should be at <span class="math notranslate nohighlight">\(\theta = 0\)</span>. Will our algorithm find it?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our loss function</span>
<span class="k">def</span> <span class="nf">quadratic_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="c1"># Run gradient descent</span>
<span class="n">minimum</span> <span class="o">=</span> <span class="n">univariate_gradient_descent</span><span class="p">(</span><span class="n">quadratic_loss</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">))</span>

<span class="c1"># Print out the minimum</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Minimum at theta =&#39;</span><span class="p">,</span> <span class="n">minimum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum at theta = -4.0740719526689754e-10
</pre></div>
</div>
</div>
</div>
<p>As you can see, gradient descent successfully found the minima, approximately—the resultant number is very close to 0. Let’s animate the algorithm to see how it finds the minima. We’ll do this using two different learning rates: first with <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span>, and then with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>.</p>
<figure class="align-center" id="fig-dg-quad-lr0p1">
<a class="reference internal image-reference" href="_images/gradient_descent_quadratic_fn_lr0p1.gif"><img alt="_images/gradient_descent_quadratic_fn_lr0p1.gif" src="_images/gradient_descent_quadratic_fn_lr0p1.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.4 </span><span class="caption-text">For this loss function, a large learning rate causes gradient descent to converge quickly to the minimum.</span><a class="headerlink" href="#fig-dg-quad-lr0p1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-dg-quad-lr0p01">
<a class="reference internal image-reference" href="_images/gradient_descent_quadratic_fn_lr0p01.gif"><img alt="_images/gradient_descent_quadratic_fn_lr0p01.gif" src="_images/gradient_descent_quadratic_fn_lr0p01.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.5 </span><span class="caption-text">For this loss function, a small learning rate causes gradient descent to converge quickly to the minimum.</span><a class="headerlink" href="#fig-dg-quad-lr0p01" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-exercise-intuition-for-gradient-descent-on-nice-loss-functions admonition">
<p class="admonition-title">Exercise: Intuition for Gradient Descent on “Nice” Loss Functions</p>
<p>Look at the two animations above and answer the following questions:</p>
<ol class="arabic simple">
<li><p>Why did a higher learning rate speed up convergence?</p></li>
<li><p>Notice that gradient descent slowed down with the number of iterations (for both animations). Why does this happen?</p></li>
</ol>
</div>
<p><strong>Ensuring Parameters Satisfy Constraints.</strong> Sometimes we need our parameters to satisfy certain properties. For example, in the above example, in which we computed the MLE for the Bernoulli model, we need to ensure our parameter, <span class="math notranslate nohighlight">\(\rho\)</span>, lies on the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>. How can we satisfy this constraint using gradient descent? We define <span class="math notranslate nohighlight">\(\rho\)</span> as a <em>function</em> of another variable, <span class="math notranslate nohighlight">\(\theta\)</span>, such that the unconstrained <span class="math notranslate nohighlight">\(\theta\)</span> (i.e. <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}\)</span>) is transformed into a constrained <span class="math notranslate nohighlight">\(\rho\)</span> (i.e. <span class="math notranslate nohighlight">\(\rho \in [0, 1]\)</span>):</p>
<div class="amsmath math notranslate nohighlight" id="equation-13410f53-2101-4420-b3b7-5c66c0d8873d">
<span class="eqno">(10.14)<a class="headerlink" href="#equation-13410f53-2101-4420-b3b7-5c66c0d8873d" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho &amp;= g(\theta).
\end{align}\]</div>
<p>We then perform the optimization over <span class="math notranslate nohighlight">\(\theta\)</span>. To make this possible, we need an invertible function <span class="math notranslate nohighlight">\(g(\cdot)\)</span> that maps the real-line to the unit interval (i.e. <span class="math notranslate nohighlight">\(g: \mathbb{R} \rightarrow [0, 1]\)</span>). For this, we can use a function known as a sigmoid:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c143ebcc-9329-4c48-86b6-c5fa456b8f05">
<span class="eqno">(10.15)<a class="headerlink" href="#equation-c143ebcc-9329-4c48-86b6-c5fa456b8f05" title="Permalink to this equation">#</a></span>\[\begin{align}
g(\theta) = \frac{1}{1 + e^{-\theta}}.
\end{align}\]</div>
<p>Check out its <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener noreferrer" target="_blank">Wikipedia page</a> to see what it looks like.</p>
<p>Putting this all together, instead of solving our original optimization problem,</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d3bfd5d-73fa-4f4c-bf53-1458c7997d44">
<span class="eqno">(10.16)<a class="headerlink" href="#equation-8d3bfd5d-73fa-4f4c-bf53-1458c7997d44" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} &amp;= \mathrm{argmin}_{\rho} \text{ } \mathcal{L}(\rho) \quad \text{subject to} \quad 0 \leq \rho \leq 1,
\end{align}\]</div>
<p>we solve,</p>
<div class="amsmath math notranslate nohighlight" id="equation-ff11f8a5-60eb-45f5-bf81-79c1200bafe7">
<span class="eqno">(10.17)<a class="headerlink" href="#equation-ff11f8a5-60eb-45f5-bf81-79c1200bafe7" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmin}_{\theta} \text{ } \mathcal{L}(g(\theta)).
\end{align}\]</div>
<p>We can then get <span class="math notranslate nohighlight">\(\rho^\text{MLE}\)</span> from <span class="math notranslate nohighlight">\(\theta^\text{MLE}\)</span> by using the inverse of <span class="math notranslate nohighlight">\(g(\cdot)\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4f4a9a28-e855-4806-9841-8fc93ccb5bb2">
<span class="eqno">(10.18)<a class="headerlink" href="#equation-4f4a9a28-e855-4806-9841-8fc93ccb5bb2" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} = g^{-1}(\theta^\text{MLE}).
\end{align}\]</div>
<p>This is how constraints (such as <code class="docutils literal notranslate"><span class="pre">C.unit_interval</span></code>) in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> are implemented internally. Of course, each constraint relies on a different transform, <span class="math notranslate nohighlight">\(g(\cdot)\)</span>.</p>
<p><strong>Automatic Differentiation.</strong> So how does <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> know to compute gradients automatically for us? The magic is in the function <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>, which uses an algorithm called “automatic differentiation” or “backpropagation” to compute exact (not approximate) gradients. We will not get into this topic in the course, but if you’re interested, check out <a class="reference external" href="https://www.youtube.com/watch?v=_-D0DiN49fc&amp;t=415s" rel="noopener noreferrer" target="_blank">this video</a> to learn more.</p>
</section>
<section id="challenges-with-numeric-optimization">
<h2><span class="section-number">10.5. </span>Challenges with Numeric Optimization<a class="headerlink" href="#challenges-with-numeric-optimization" title="Permalink to this heading">#</a></h2>
<p>While so far, gradient descent seems like magic, it definitely has its own drawbacks. Let’s look at how gradient descent performs on a more “wiggly” loss function—what do you notice about it?</p>
<figure class="align-center" id="fig-dg-quad-plus-sin-lr0p01">
<a class="reference internal image-reference" href="_images/gradient_descent_quadratic_plus_sin_fn_lr0p01.gif"><img alt="_images/gradient_descent_quadratic_plus_sin_fn_lr0p01.gif" src="_images/gradient_descent_quadratic_plus_sin_fn_lr0p01.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.6 </span><span class="caption-text">For this loss function, a small learning rate causes gradient descent to get stuck in a local minimum near its initialization.</span><a class="headerlink" href="#fig-dg-quad-plus-sin-lr0p01" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-dg-quad-plus-sin-lr0p1">
<a class="reference internal image-reference" href="_images/gradient_descent_quadratic_plus_sin_fn_lr0p1.gif"><img alt="_images/gradient_descent_quadratic_plus_sin_fn_lr0p1.gif" src="_images/gradient_descent_quadratic_plus_sin_fn_lr0p1.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.7 </span><span class="caption-text">For this loss function, a large learning rate helps step out of local minima, but prevents gradient descent from honing in on and ultimately “sticking” to the minimum.</span><a class="headerlink" href="#fig-dg-quad-plus-sin-lr0p1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As you can see from these animations, gradient descent struggles with a few challenges:</p>
<ol class="arabic simple">
<li><p><strong>Gradient Descent is Prone to Local Optima.</strong> Gradient descent is prone to getting stuck in local optima. Moreover, it’s not possible for us to determine whether an optima is local or global. For some complicated ML models, such local optima can be “good enough” for use, and for others they are not.</p></li>
<li><p><strong>Gradient Descent is Sensitive to Initialization.</strong> Since gradient descent tends to get stuck in local optima that are relatively close to its initialization, it’s important that we try many different initializations to empirically find one that works well. This can be time-consuming.</p></li>
<li><p><strong>Gradient Descent is Sensitive to Hyper-parameters.</strong> As you saw from the above animations, the choice of learning rate and number of iterations change the behavior of gradient descent drastically. This means than whenever we use such numeric optimization algorithm, we will have to try a bunch of different settings of these “hyper-parameters” to empirically see what works best. This can sometimes be time-consuming and cumbersome.</p></li>
<li><p><strong>Gradient Descent Adds Diagnosic Challenges.</strong> Now that we’ve introduced an approximate component to our modeling toolkit—numeric optimization—it will be difficult to diagnose <em>why</em> our ML method performs poorly. For example, if our model fits our data poorly, is it because our modeling assumptions (e.g. our choice of distributions) are inappropriate, or is it because our optimizer got stuck in some local minima?</p></li>
</ol>
<p>Given these challenges, it is important for us to use numeric optimization algorithms responsibly: to be aware of all the ways they might fail us, and to think critically about how these failures impact the downstream effects of our ML models.</p>
<div class="admonition-exercise-intuition-for-gradient-descent-on-non-nice-loss-functions admonition">
<p class="admonition-title">Exercise: Intuition for Gradient Descent on Non-“Nice” Loss Functions</p>
<p><strong>Part 1:</strong> Use <a class="reference external" href="https://uclaacm.github.io/gradient-descent-visualiser/#playground" rel="noopener noreferrer" target="_blank">this online gradient descent simulator</a> to answer the questions below.</p>
<ol class="arabic simple">
<li><p>Simulate gradient descent for <code class="docutils literal notranslate"><span class="pre">x^2</span></code> and then for <code class="docutils literal notranslate"><span class="pre">x^2</span> <span class="pre">+</span> <span class="pre">sin(2</span> <span class="pre">*</span> <span class="pre">3.14</span> <span class="pre">*</span> <span class="pre">x)</span></code>, each with a starting point of <code class="docutils literal notranslate"><span class="pre">-4</span></code> and a learning rate of <code class="docutils literal notranslate"><span class="pre">0.1</span></code>. Notice that both functions look similar, but the latter is a “wiggly” version of the former. How do the trajectories differ? Why does this happen?</p></li>
<li><p>Simulate gradient descent for <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">exp(-x^2</span> <span class="pre">/</span> <span class="pre">0.1)</span> <span class="pre">/</span> <span class="pre">0.1</span></code> with a starting point of <code class="docutils literal notranslate"><span class="pre">2</span></code> and a learning rate of <code class="docutils literal notranslate"><span class="pre">0.1</span></code>. What is the trajectory like? Why does this happen?</p></li>
<li><p>Simulate gradient descent for <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">exp(-x^2</span> <span class="pre">/</span> <span class="pre">0.1)</span> <span class="pre">/</span> <span class="pre">0.1</span> <span class="pre">+</span> <span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">x^2</span></code> with a starting point of <code class="docutils literal notranslate"><span class="pre">2</span></code> and a learning rate of <code class="docutils literal notranslate"><span class="pre">0.1</span></code>. What’s the trajectory like? Why does this happen?</p></li>
</ol>
<p><strong>Part 2:</strong> For each of the situations below, describe what you think the loss function looks like that caused the problematic behavior. Then, describe one way you can mitigate the issue.</p>
<ol class="arabic simple">
<li><p>When training your model, you noticed that initially, your loss function decreased steadily, but after a while, it started bouncing up and down erratically, unable to converge on a minima.</p></li>
<li><p>When training your model, you noticed your loss function initially decreased steadily, but after a while, it stopped decreasing. You then decide to see how well your model fit your data, and notice that it fits it very poorly.</p></li>
<li><p>You tried training your model, but the loss didn’t decrease at all.</p></li>
</ol>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="mle-code.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Maximum Likelihood: Code</p>
      </div>
    </a>
    <a class="right-next"
       href="probability-continuous.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Probability (Continuous)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analytic-solutions-to-optimization-problems">10.1. Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-analytically-solving-for-the-mle">10.2. An Example: Analytically Solving for the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-analytic-solutions-to-optimization-problems">10.3. Challenges with Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-solutions-to-optimization-problems">10.4. Numeric Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-numeric-optimization">10.5. Challenges with Numeric Optimization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024 Yaniv Yacoby
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>