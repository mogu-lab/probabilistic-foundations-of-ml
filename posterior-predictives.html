
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>22. Posterior Predictives &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posterior-predictives';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/posterior-predictives.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="23. The Ethics of Uncertainty and Interpretability in Human-AI Systems" href="ethics-of-uncertainty-and-interpretability.html" />
    <link rel="prev" title="21. Priors and Posteriors" href="priors-and-posteriors.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/posterior-predictives.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Posterior Predictives</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-model-averaging">22.1. Intuition: Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-posterior-predictive">22.2. Derivation of the Posterior Predictive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laws-of-conditional-independence">22.3. Laws of Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-of-different-models">22.4. Posterior Predictive of Different Models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="posterior-predictives">
<h1><span class="section-number">22. </span>Posterior Predictives<a class="headerlink" href="#posterior-predictives" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> For safety-critical applications of ML, it’s important that our model captures two notions of uncertainty. Aleatoric uncertainty captures inherent stochasticity in the system. In contrast, epistemic uncertainty is uncertainty over possible <em>models</em> that could have fit the data. Multiple models can fit the data when we have a lack of data and/or a lack of mechanistic understanding of the system. We realized that fitting models using the MLE only captured aleatoric uncertainty. To additionally capture epistemic, we therefore had to rethink our modeling paradigm. Using Bayes’ rule, we were able to obtain a <em>distribution</em> over model parameters given the data, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> (the posterior). Using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, we sampled from the posterior to obtain a diversity of models that fit the data. We interpreted a greater diversity of models indicated higher epistemic ucnertainty.</p>
<p><strong>Challenge:</strong> Now that we have a posterior over model parameters, we can capture <em>epistemic</em> uncertainty. But how do we use this diverse set of models to (1) make predictions, and (2) compute the log-likelihood (for evaluation)? To do this, we will derive the <em>posterior predictive</em>, a distribution that translates a distribution over parameters to a distribution over data. This distributions can then be used to make predictions and evaluate the log-likelihood.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Provide intuition for the posterior predictive</p></li>
<li><p>Derive the posterior predictive</p></li>
<li><p>Introduce laws of conditional independence</p></li>
<li><p>Evaluate the posterior predictive</p></li>
</ul>
<section id="intuition-model-averaging">
<h2><span class="section-number">22.1. </span>Intuition: Model Averaging<a class="headerlink" href="#intuition-model-averaging" title="Link to this heading">#</a></h2>
<p><strong>Bayesian Modeling as Ensembling.</strong> Recall in the previous chapter, we initially introduced <em>ensembling</em> as a way to capture epistemic uncertainty. In ensembling, we train a collection of models independently and hope that, due to quirks in optimization, we end up with a diverse collection of models. In a sense, doesn’t our Bayesian approach provide us with an ensemble as well? After all, each set of parameters <span class="math notranslate nohighlight">\(\theta\)</span> from the posterior <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> represents a different model. Based on this analogy, we can create a “Bayesian” ensemble as follows:</p>
<ol class="arabic">
<li><p>We draw <span class="math notranslate nohighlight">\(S\)</span> samples from the posterior: <span class="math notranslate nohighlight">\(\theta_s \sim p(\theta | \mathcal{D})\)</span>.</p></li>
<li><p>Each posterior sample represents a different member of our ensemble: <span class="math notranslate nohighlight">\(p(\mathcal{D} | \theta_s)\)</span>.</p>
<blockquote>
<div><p>For regression, we have <span class="math notranslate nohighlight">\(p_{Y | X}(y | x, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Predicting.</strong> Using this ensemble, we can predict by <em>averaging</em> the predictions of the ensemble members:</p>
<ol class="arabic">
<li><p>We draw <span class="math notranslate nohighlight">\(\mathcal{D}_s \sim p(\cdot | \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>.</p>
<blockquote>
<div><p>For regression, we draw <span class="math notranslate nohighlight">\(y_s \sim p_{Y | X}(\cdot | x, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
<li><p>We average: <span class="math notranslate nohighlight">\(\frac{1}{S} \sum\limits_{s=1}^S \mathcal{D}_s\)</span>.</p>
<blockquote>
<div><p>For regression, we average <span class="math notranslate nohighlight">\(\frac{1}{S} \sum\limits_{s=1}^S y_s\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Evaluating Log-Likelihood.</strong> Given test data, <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>, we can use the ensemble to evaluate the model’s log-likelihood:</p>
<ol class="arabic">
<li><p>We evaluate <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>.</p>
<blockquote>
<div><p>For regression, we evaluate <span class="math notranslate nohighlight">\(p_{Y | X}(y^* | x^*, \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>, where <span class="math notranslate nohighlight">\(x^*, y^*\)</span> is a new data point.</p>
</div></blockquote>
</li>
<li><p>We average and take the log: <span class="math notranslate nohighlight">\(\log \frac{1}{S} \sum\limits_{s=1}^S p(\mathcal{D}^* | \theta_s)\)</span>.</p>
<blockquote>
<div><p>For regression, we average and take the log: <span class="math notranslate nohighlight">\(\log \frac{1}{S} \sum\limits_{s=1}^S p_{Y | X}(y^* | x^*, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Formalizing Intuition.</strong> As we will show next, this intuition actually holds for the Bayesian paradigm. That is, we can compute</p>
<blockquote>
<div><p>In the regression case, we have:</p>
<div class="amsmath math notranslate nohighlight" id="equation-edc3802e-294a-465f-be9a-58791b574439">
<span class="eqno">(22.1)<a class="headerlink" href="#equation-edc3802e-294a-465f-be9a-58791b574439" title="Permalink to this equation">#</a></span>\[\begin{align}
p(y^* | x^*, \mathcal{D}) &amp;= \mathbb{E}_{\theta \sim p(\theta | \mathcal{D})} \left[ p(y^* | x^*, \theta) \right] \\
&amp;\approx \frac{1}{S} \sum\limits_{s=1}^S p(y^* | x^*, \theta_s), \quad \theta_s \sim p(\theta | \mathcal{D}),
\end{align}\]</div>
<p>which is exactly the same formula we got from the “ensembling” analogy, except that the members of the ensemble are draws from the posterior.</p>
</div></blockquote>
</section>
<section id="derivation-of-the-posterior-predictive">
<h2><span class="section-number">22.2. </span>Derivation of the Posterior Predictive<a class="headerlink" href="#derivation-of-the-posterior-predictive" title="Link to this heading">#</a></h2>
<p><strong>Goal.</strong> We want to derive a formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>, which represents the distribution of new data <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span> given the observed data, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<blockquote>
<div><p>For a regression model, this distribution is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-802e78eb-68b4-429b-9894-856f4edd8557">
<span class="eqno">(22.2)<a class="headerlink" href="#equation-802e78eb-68b4-429b-9894-856f4edd8557" title="Permalink to this equation">#</a></span>\[\begin{align}
    p_{Y^* | X^*, \mathcal{D}}(y^* | x^*, \mathcal{D}) &amp;= p_{Y^* | X^*, \mathcal{D}}(y^* | x^*, x_1, \dots, x_N, y_1, \dots, y_N),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(x^*\)</span> is a <em>new</em> input for which we’d like to make a prediction, <span class="math notranslate nohighlight">\(y^*\)</span>.</p>
</div></blockquote>
<p><strong>A Graphical Model for the Training <em>and</em> Test Data.</strong> Notice that our posterior predictive includes a new random variable, <span class="math notranslate nohighlight">\(\mathcal{D}*\)</span>. Let’s incorporate it into our graphical model. This will help us reason about the conditional dependencies (below), needed in the derivation of the posterior predictive.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLeQTM6_s&#x2F;gr7qf2eGP4X0wfKa1NZFnw&#x2F;view?embed">
    </iframe>
  </div>
</div>
<p>As you can see, the original graphical model (for training data) is on the left. We then added a second component on the right for <span class="math notranslate nohighlight">\(M\)</span> test points we have not yet observed. We can similarly create a diagram for regression as follows:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLeBNwO1Q&#x2F;aZpvVjlBvUw6FQOHfzk5qA&#x2F;view?embed">
    </iframe>
  </div>
</div><p><strong>Derivation.</strong> Now we have all we need in order to derive a formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>. Our first step is to multiply and divide <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> by <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5ac2e86f-e6a5-4232-82a8-05d8f3812bdf">
<span class="eqno">(22.3)<a class="headerlink" href="#equation-5ac2e86f-e6a5-4232-82a8-05d8f3812bdf" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}^* | \mathcal{D}) &amp;= \frac{p(\mathcal{D}^* | \mathcal{D}) \cdot p(\mathcal{D})}{p(\mathcal{D})} 
\end{align}\]</div>
<p>We do this so that we can write the numerator as the <em>joint</em> distribution of <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bac1eb7b-63f9-43f9-b22b-39c0bbae8001">
<span class="eqno">(22.4)<a class="headerlink" href="#equation-bac1eb7b-63f9-43f9-b22b-39c0bbae8001" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{p(\mathcal{D}^*, \mathcal{D})}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Next, we use the law of total probability to re-write the above as a joint distribution over <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, and <span class="math notranslate nohighlight">\(\theta\)</span>. We do this to introduce <span class="math notranslate nohighlight">\(\theta\)</span> into the equation—since our model’s prior, likelihood, and posterior all depend on <span class="math notranslate nohighlight">\(\theta\)</span>, it would be weird if the formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> didn’t depend on it. This gives us:</p>
<div class="amsmath math notranslate nohighlight" id="equation-155643ff-b6ef-4ea8-b820-8e2f2e2d19ca">
<span class="eqno">(22.5)<a class="headerlink" href="#equation-155643ff-b6ef-4ea8-b820-8e2f2e2d19ca" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{\int p(\mathcal{D}^*, \mathcal{D}, \theta) \cdot d\theta}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Now, we can factorize this joint distribution to get one term that’s the posterior, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, and one term that’s the marginal, <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0cbfa793-9452-4140-bbc4-da3a554307e9">
<span class="eqno">(22.6)<a class="headerlink" href="#equation-0cbfa793-9452-4140-bbc4-da3a554307e9" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{\int p(\mathcal{D}^* | \theta, \mathcal{D}) \cdot p(\theta | \mathcal{D}) \cdot p(\mathcal{D}) \cdot d\theta}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Since <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>, we can take it out of the integral, thereby canceling it with the <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> in the denominator:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4c1fd9c2-05fa-41a7-82cc-b7ae950654d5">
<span class="eqno">(22.7)<a class="headerlink" href="#equation-4c1fd9c2-05fa-41a7-82cc-b7ae950654d5" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \int p(\mathcal{D}^* | \theta, \mathcal{D}) \cdot p(\theta | \mathcal{D}) \cdot d\theta
\end{align}\]</div>
<p>Finally, using the laws of conditional independence, we know that <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta, \mathcal{D}) = p(\mathcal{D}^* | \theta)\)</span>. This is because, by conditioning on <span class="math notranslate nohighlight">\(\theta\)</span>, we remove all paths connecting <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>. In other words, <span class="math notranslate nohighlight">\(\theta\)</span> summarizes all information from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> needed to predict <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span> (we cover the laws of conditional independence in depth below). This gives us the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1eb2427e-f85a-4fd8-b3e5-bdaf368784ac">
<span class="eqno">(22.8)<a class="headerlink" href="#equation-1eb2427e-f85a-4fd8-b3e5-bdaf368784ac" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \int \underbrace{p(\mathcal{D}^* | \theta)}_{\text{likelihood of new data}} \cdot \underbrace{p(\theta | \mathcal{D})}_{\text{posterior}} \cdot d\theta
\end{align}\]</div>
<p>As you can see, <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> is a function of the posterior and the joint data likelihood of the new data. Adding some syntactic sugar, we can write the above equation as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4257c054-e3d9-42fa-856e-3f7520358ffa">
<span class="eqno">(22.9)<a class="headerlink" href="#equation-4257c054-e3d9-42fa-856e-3f7520358ffa" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \mathbb{E}_{p(\theta | \mathcal{D})} \left[ p(\mathcal{D}^* | \theta) \right]
\end{align}\]</div>
<p>This shows that to evaluate <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>, we need to:</p>
<ol class="arabic simple">
<li><p>Draw posterior samples <span class="math notranslate nohighlight">\(\theta \sim p(\theta | \mathcal{D})\)</span>.</p></li>
<li><p>Average the likelihood <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta)\)</span> across these samples.</p></li>
</ol>
<p>As you can see this matches our intuition exactly!</p>
</section>
<section id="laws-of-conditional-independence">
<h2><span class="section-number">22.3. </span>Laws of Conditional Independence<a class="headerlink" href="#laws-of-conditional-independence" title="Link to this heading">#</a></h2>
<p><strong>Motivation.</strong> Recall in a linear regression model, we often sample the slope and intercept independently under the prior. For example, we may choose to draw each from a normal distribution (with no correlations). However, when we condition on data (i.e. under the posterior), they are no longer independent. Why does this happen? To fit the data, if the slope increases, the intercept has to decrease (and vice versa). Generalizing this insight, when conditioning on a variable, we need to rethink the statistical dependence. This becomes important when deriving distributions like the posterior predictive.</p>
<p>We will present three general cases here that can be applied to more complicated models. In all three cases, we have three random variables: <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span>. We will then condition on <span class="math notranslate nohighlight">\(B\)</span> and see what happens to the statistical dependence between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span>.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMtw_xCmk&#x2F;d3A7vCtOgxBDh4NKuXqr5w&#x2F;view?embed">
    </iframe>
  </div>
</div><p><strong>Case 1: Intuition.</strong> Under the generative process, <span class="math notranslate nohighlight">\(B\)</span> depends on <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> depends on <span class="math notranslate nohighlight">\(B\)</span>. When conditioning on <span class="math notranslate nohighlight">\(B\)</span>, however, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> become statistically <em>independent</em>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ba407846-32fd-4215-9cde-3b05960c30d5">
<span class="eqno">(22.10)<a class="headerlink" href="#equation-ba407846-32fd-4215-9cde-3b05960c30d5" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) &amp;= p_{A | B}(a | b) \cdot p_{C | B}(c | b)
\end{align}\]</div>
<blockquote>
<div><p><strong>Example:</strong> Let <span class="math notranslate nohighlight">\(A\)</span> be a latent variable, describing whether a patient has or doesn’t have COVID. <span class="math notranslate nohighlight">\(B\)</span> is the result of a COVID-test; it depends on <span class="math notranslate nohighlight">\(A\)</span>, since having the disease means a greater chance of testing positive. Of course, there’s some probability that even the test could be wrong. Finally, <span class="math notranslate nohighlight">\(C\)</span> describes whether the doctor will prescribe the patient COVID medication. <span class="math notranslate nohighlight">\(C\)</span> only depends on <span class="math notranslate nohighlight">\(B\)</span> because the doctor can only act on the results of the test—they have no other way of knowing whether the patient has or doesn’t have the disease. Again, even given a positive test, the doctor might still choose not to prescribe medicine.</p>
<p>Given <span class="math notranslate nohighlight">\(B\)</span>, the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are independent. This is because given a positive test result (<span class="math notranslate nohighlight">\(B = 1\)</span>), we can infer the chance that the patient actually has COVID (<span class="math notranslate nohighlight">\(A = 1\)</span>). But the doctor’s decision to prescribe medication (<span class="math notranslate nohighlight">\(C = 1\)</span>) is only based on the result of the test.</p>
</div></blockquote>
<p><strong>Case 1: Derivation.</strong> Before we begin the derivation, notice that we can write a conditional distribution by dividing the joint by the marginal:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5043ea03-a44f-4458-80ad-6a26d632ffae">
<span class="eqno">(22.11)<a class="headerlink" href="#equation-5043ea03-a44f-4458-80ad-6a26d632ffae" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) \cdot p_B(b) &amp;= p_{A, B, C}(a, b, c) \\
p_{A, C | B}(a, c | b) &amp;= \frac{p_{A, B, C}(a, b, c)}{p_B(b)}
\end{align}\]</div>
<p>We start using this fact:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c7ad3d82-a23c-426f-9963-f08c0e5301e7">
<span class="eqno">(22.12)<a class="headerlink" href="#equation-c7ad3d82-a23c-426f-9963-f08c0e5301e7" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) &amp;= \frac{p_{A, B, C}(a, b, c)}{p_B(b)} \\
&amp;= \frac{p_A(a) \cdot p_{B | A}(b | a) \cdot p_{C | B}(c | b)}{p_B(b)} \quad (\text{factorizing the joint using the DGM in Case 1}) \\
&amp;= \frac{p_A(a) \cdot \frac{p_{A | B}(a | b) \cdot p_B(b)}{p_A(a)} \cdot p_{C | B}(c | b)}{p_B(b)} \quad (\text{Bayes' rule}) \\
&amp;= p_{A | B}(a | b) \cdot p_{C | B}(c | b) \quad (\text{cancel out terms})
\end{align}\]</div>
<p><strong>Case 2: Intuition</strong> Under the generative process, both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> depend on <span class="math notranslate nohighlight">\(B\)</span>. When conditioning on <span class="math notranslate nohighlight">\(B\)</span>, however, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> become statistically <em>independent</em>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0fa3f081-1c84-4cf8-a42c-b8b65de86eeb">
<span class="eqno">(22.13)<a class="headerlink" href="#equation-0fa3f081-1c84-4cf8-a42c-b8b65de86eeb" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) &amp;= p_{A | B}(a | b) \cdot p_{C | B}(c | b)
\end{align}\]</div>
<blockquote>
<div><p><strong>Example:</strong> Let <span class="math notranslate nohighlight">\(B\)</span> be a latent variable, describing whether a patient has or doesn’t have COVID. Let <span class="math notranslate nohighlight">\(A\)</span> be a COVID-test; it depends on <span class="math notranslate nohighlight">\(B\)</span>, since having the disease means a greater probability of testing positive. Finally, let <span class="math notranslate nohighlight">\(C\)</span> be the probability the patient infects someone else with COVID. <span class="math notranslate nohighlight">\(C\)</span> depends on <span class="math notranslate nohighlight">\(B\)</span>, since the patient can only infect someone else if they actually have COVID. In general, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are not independent, since knowing <span class="math notranslate nohighlight">\(A\)</span> tells us something about <span class="math notranslate nohighlight">\(C\)</span> (and vice versa); if a patient tests positive, they are more likely to have COVID, and therefore also more likely to infect someone else. Here, information passes from <span class="math notranslate nohighlight">\(A\)</span> to <span class="math notranslate nohighlight">\(C\)</span> through <span class="math notranslate nohighlight">\(B\)</span>. However, conditioning on <span class="math notranslate nohighlight">\(B\)</span> (i.e. knowing whether the patient has COVID) means that <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are now statistically independent. This is because information can no longer travel from <span class="math notranslate nohighlight">\(A\)</span> to <span class="math notranslate nohighlight">\(C\)</span> through <span class="math notranslate nohighlight">\(B\)</span>. Given <span class="math notranslate nohighlight">\(B\)</span>, there’s a fixed probability of testing positive, and that probability is independent of whether the patient will infect someone else.</p>
</div></blockquote>
<p><strong>Case 2: Derivation.</strong> We start the same way as we did for Case 1:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2e263ae3-7fed-40b4-8409-2c6b254dc0d7">
<span class="eqno">(22.14)<a class="headerlink" href="#equation-2e263ae3-7fed-40b4-8409-2c6b254dc0d7" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) &amp;= \frac{p_{A, B, C}(a, b, c)}{p_B(b)} \\
&amp;= \frac{p_B(b) \cdot p_{A | B}(a | b) \cdot p_{C | B}(c | b)}{p_B(b)} \quad (\text{factorizing the joint using the DGM in Case 2}) \\
&amp;= p_{A | B}(a | b) \cdot p_{C | B}(c | b) \quad (\text{cancel out terms})
\end{align}\]</div>
<p><strong>Case 3: Intuition</strong> Under the generative process, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are independent. <span class="math notranslate nohighlight">\(B\)</span> then depends on <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span>. When conditioning on <span class="math notranslate nohighlight">\(B\)</span>, however, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are statistically <em>dependent</em>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f634c5e6-e2e4-4183-8e53-de45c2d3e8ce">
<span class="eqno">(22.15)<a class="headerlink" href="#equation-f634c5e6-e2e4-4183-8e53-de45c2d3e8ce" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, C | B}(a, c | b) &amp;\neq p_{A | B}(a | b) \cdot p_{C | B}(c | b)
\end{align}\]</div>
<blockquote>
<div><p><strong>Example:</strong> Let <span class="math notranslate nohighlight">\(B\)</span> describe whether a patient has heart disease. Let <span class="math notranslate nohighlight">\(A\)</span> be lifestyle factors (like diet) that could increase the chance of having heart disease, and let <span class="math notranslate nohighlight">\(C\)</span> be genetic factors that contribute to heart disease. <span class="math notranslate nohighlight">\(B\)</span> depends on both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span>: the probability of heart disease increases with the presence of both lifestyle and genetic factors. Moreover, here we assume that <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are independent—whether you have a certain lifestyle doesn’t tell us about your genes and vice versa. However, conditioning on <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are no longer independent. If we know an individual has heart disease, and we know they don’t have a lifestyle that contributes to the disease, then they are more likely to have genetic factors. Similarly, if we know an individual has heart disease, but they don’t have genetic factors, they are more likely to have lifestyle factors.</p>
</div></blockquote>
<p><strong>Case 3: Derivation.</strong> In applying the same tricks to factorize <span class="math notranslate nohighlight">\(p_{A, C | B}(a, c | b)\)</span> as for the previous two cases, we always end up with <span class="math notranslate nohighlight">\(p_{A, C | B}(a, c | b)\)</span>, meaning that we cannot further factorize it.</p>
<div class="admonition-exercise-laws-of-conditional-independence admonition">
<p class="admonition-title">Exercise: Laws of Conditional Independence</p>
<p><strong>Part 1:</strong> Look at the graphical model for Bayesian regression that includes both the training and test data. Having conditioned on <span class="math notranslate nohighlight">\(X_1, X_2, X_1^*, X_2^*\)</span>,</p>
<ul class="simple">
<li><p>Is <span class="math notranslate nohighlight">\(Y_1\)</span> independent of <span class="math notranslate nohighlight">\(Y_2\)</span>?</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(Y_1\)</span> independent of <span class="math notranslate nohighlight">\(Y_1^*\)</span>?</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(Y_1^*\)</span> independent of <span class="math notranslate nohighlight">\(Y_2^*\)</span>?</p></li>
</ul>
<p>Justify your reasoning.</p>
<p><strong>Part 2:</strong> Look at the graphical model for Bayesian regression that includes both the training and test data. Having conditioned on <span class="math notranslate nohighlight">\(X_1, X_2, X_1^*, X_2^*\)</span>, <em>as well as on <span class="math notranslate nohighlight">\(\theta\)</span>,</em></p>
<ul class="simple">
<li><p>Is <span class="math notranslate nohighlight">\(Y_1\)</span> independent of <span class="math notranslate nohighlight">\(Y_2\)</span>?</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(Y_1\)</span> independent of <span class="math notranslate nohighlight">\(Y_1^*\)</span>?</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(Y_1^*\)</span> independent of <span class="math notranslate nohighlight">\(Y_2^*\)</span>?</p></li>
</ul>
<p>Justify your reasoning.</p>
<p><strong>Part 3:</strong> Consider the directed graphical model below.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMuy3bhms&#x2F;3eRn0CG_F9wamDtTCsjoTQ&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<p>Using the laws of conditional probability,</p>
<ul class="simple">
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, B, D, E | C}(a, b, d, e | c)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, C, D, E | B}(a, c, d, e | b)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, B, C, D | E}(a, b, c, d | e)\)</span>.</p></li>
</ul>
<p><strong>Part 4:</strong> Consider the directed graphical model below.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMu_ovZTg&#x2F;GiWABBVFOKgajyEll2nDtQ&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<p>Using the laws of conditional probability,</p>
<ul class="simple">
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{B, C, D, E | A}(b, c, d, e | a)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, C, D, E | B}(a, c, d, e | b)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, B, D, E | C}(a, b, d, e | c)\)</span>.</p></li>
</ul>
<p><strong>Part 5:</strong> Consider the directed graphical model below.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMu-__3-E&#x2F;zF3loMkA2FjWLVAxfTd91A&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<p>Using the laws of conditional probability,</p>
<ul class="simple">
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{B, C, D, E | A}(b, c, d, e | a)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, C, D, E | B}(a, c, d, e | b)\)</span>.</p></li>
<li><p>Factorize <span class="math notranslate nohighlight">\(p_{A, B, C, E | D}(a, b, c, e | d)\)</span>.</p></li>
</ul>
</div>
</section>
<section id="posterior-predictive-of-different-models">
<h2><span class="section-number">22.4. </span>Posterior Predictive of Different Models<a class="headerlink" href="#posterior-predictive-of-different-models" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-derive-the-posterior-predictive-distribution admonition">
<p class="admonition-title">Exercise: Derive the Posterior Predictive Distribution</p>
<p>For each of the models below, derive the posterior predictive. You may find it helpful to draw the directed graphical model that captures both the train and test data.</p>
<p><strong>Part 1:</strong> Bayesian predictive model.</p>
<div class="amsmath math notranslate nohighlight" id="equation-a96166bf-e372-4895-96f3-d223bbd2265c">
<span class="eqno">(22.16)<a class="headerlink" href="#equation-a96166bf-e372-4895-96f3-d223bbd2265c" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
y_n | x_n, \theta &amp;\sim p_{Y | X}(\cdot | x_n, \theta)
\end{align}\]</div>
<p>The posterior predictive is: <span class="math notranslate nohighlight">\(p_{Y^* | X^*, X_{1:N}, Y_{1:N}}(y^* | x^*, x_{1:N}, y_{1:N})\)</span>, where <span class="math notranslate nohighlight">\(x_{1:N}\)</span> and <span class="math notranslate nohighlight">\(y_{1:N}\)</span> denote the full data.</p>
<p><strong>Part 2:</strong> Bayesian Factor Analysis.</p>
<div class="amsmath math notranslate nohighlight" id="equation-5d308c5d-1311-4471-bbb5-7144589ba7ef">
<span class="eqno">(22.17)<a class="headerlink" href="#equation-5d308c5d-1311-4471-bbb5-7144589ba7ef" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
z_n &amp;\sim p_Z(\cdot) \\
x_n | z_n, \theta &amp;\sim p_{X | Z, \theta}(\cdot | z_n, \theta) 
\end{align}\]</div>
<p>The posterior predictive is: <span class="math notranslate nohighlight">\(p_{X^* | X_{1:N}}(x^* | x_{1:N})\)</span>, where <span class="math notranslate nohighlight">\(x_{1:N}\)</span> denotes the full data.</p>
<p><strong>Part 3:</strong> Bayesian predictive model with latent variable.</p>
<div class="amsmath math notranslate nohighlight" id="equation-ea83b8f5-3ef2-4a41-bedf-e483e4bae265">
<span class="eqno">(22.18)<a class="headerlink" href="#equation-ea83b8f5-3ef2-4a41-bedf-e483e4bae265" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
z_n &amp;\sim p_Z(\cdot) \\
y_n | x_n, z_n, \theta &amp;\sim p_{Y | X, Z, \theta}(\cdot | x_n, z_n, \theta) 
\end{align}\]</div>
<p>The posterior predictive is: <span class="math notranslate nohighlight">\(p_{Y^* | X^*, X_{1:N}, Y_{1:N}}(y^* | x^*, x_{1:N}, y_{1:N})\)</span>, where <span class="math notranslate nohighlight">\(x_{1:N}\)</span> and <span class="math notranslate nohighlight">\(y_{1:N}\)</span> denote the full data.</p>
<p><strong>Part 4:</strong> Bayesian Concept-Bottlebeck model (CBM). CBMs aim to make it easier to interpret model predictions. They do this by combining two models:</p>
<ul class="simple">
<li><p>CMBs first learning to predict “concepts” <span class="math notranslate nohighlight">\(c_n\)</span> associated, associated with input <span class="math notranslate nohighlight">\(x_n\)</span>. In a CBM, a concept is just a discrete attribute associated with the input; for example, if <span class="math notranslate nohighlight">\(x_n\)</span> is an image of wildlife, a concept could be rain, grass, dog, etc. You can think of <span class="math notranslate nohighlight">\(p_{C | X}\)</span> as a classifier.</p></li>
<li><p>After having predicted the concept <span class="math notranslate nohighlight">\(c_n\)</span> from the input <span class="math notranslate nohighlight">\(x_n\)</span>, CBMs attempt to predict the final output <span class="math notranslate nohighlight">\(y_n\)</span> from the concept only. In this way, predictions of <span class="math notranslate nohighlight">\(y_n\)</span> can be analyzed in terms of the concepts, which as easier to understand, instead of with respect to the inputs, which could be high dimensional and difficult to reason about.</p></li>
</ul>
<p>A Bayesian CBM has the following generative process:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5be775ef-26db-42ca-8cc2-c9b0b28b1f06">
<span class="eqno">(22.19)<a class="headerlink" href="#equation-5be775ef-26db-42ca-8cc2-c9b0b28b1f06" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
\phi &amp;\sim p_\phi(\cdot) \\
c_n | x_n, \theta &amp;\sim p_{C | X}(\cdot | x_n, \theta) = \mathrm{Cat}(\pi(x_n; \theta)) \\
y_n | c_n, \phi &amp;\sim p_{Y | C}(\cdot | c_n, \phi),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi(\cdot; \theta)\)</span> is a function that maps <span class="math notranslate nohighlight">\(x_n\)</span> to the parameters of a categorical distribution.</p>
<p>The posterior predictive is: <span class="math notranslate nohighlight">\(p_{Y^* | X^*, X_{1:N}, C_{1:N}, Y_{1:N}}(y^* | x^*, x_{1:N}, c_{1:N}, y_{1:N})\)</span>, where <span class="math notranslate nohighlight">\(x_{1:N}\)</span>, <span class="math notranslate nohighlight">\(c_{1:N}\)</span>, and <span class="math notranslate nohighlight">\(y_{1:N}\)</span> denote the full data.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="priors-and-posteriors.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Priors and Posteriors</p>
      </div>
    </a>
    <a class="right-next"
       href="ethics-of-uncertainty-and-interpretability.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">23. </span>The Ethics of Uncertainty and Interpretability in Human-AI Systems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-model-averaging">22.1. Intuition: Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-posterior-predictive">22.2. Derivation of the Posterior Predictive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laws-of-conditional-independence">22.3. Laws of Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-of-different-models">22.4. Posterior Predictive of Different Models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>