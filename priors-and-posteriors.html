
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>21. Priors and Posteriors &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'priors-and-posteriors';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/priors-and-posteriors.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="22. Posterior Predictives" href="posterior-predictives.html" />
    <link rel="prev" title="20. The Ethics of Generative Models in Sociotechnical Systems" href="ethics-of-generative-models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/priors-and-posteriors.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Priors and Posteriors</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-uncertainty">21.1. Why We Need Uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-uncertainty">21.2. What is Uncertainty?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-paradigm">21.3. The Bayesian Modeling Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-bayes-rule">21.4. Model-Fitting via Bayes’ Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models-in-numpyro">21.5. Bayesian Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-in-numpyro">21.6. Bayesian Inference in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-bayesian-regression-models">21.7. Comparing Bayesian Regression Models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="priors-and-posteriors">
<h1><span class="section-number">21. </span>Priors and Posteriors<a class="headerlink" href="#priors-and-posteriors" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">probabilistic_foundations_of_ml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pfml</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> If there’s one thing we learned from the chapter on model selection and evaluation is that we should not blindly trust our models. Models are complicated and require a robust and diverse toolkit for responsible evaluation in their intended context. For safety-critical applications of ML, like the ones from the IHH, we must take additional precautions to ensure responsible use. We therefore adopt the following philosophy:</p>
<ol class="arabic simple">
<li><p><strong>Finite information <span class="math notranslate nohighlight">\(\rightarrow\)</span> uncertainty.</strong> We’re often asked to make decisions without all the information necessary for certainty. We ask the same of our models: given a finite data set and an incomplete understanding of the phenomenon we’re modeling, we ask models to make predictions for data they have never encountered. Therefore, for responsible use in safety-critical contexts, our models must have some way of quantifying the limits of their “knowledge.”</p></li>
<li><p><strong>Not making choices <span class="math notranslate nohighlight">\(\rightarrow\)</span> a choice will be made for you.</strong> If we avoid making explicit choices in the design of our model, a choice will still be made for us—and it might not be the choice we want. For example, without explicitly choosing what’s important to us, we might get a model with the highest accuracy for a task for which minimizing false negatives is most important. <em>It’s therefore better to make your choices explicitly.</em> Making assumptions explicit is especially important for uncertainty quantification.</p></li>
</ol>
<p><strong>Challenge:</strong> To satisfy our new modeling philosophy, we need (1) a way to quantify uncertainty, and (2) a way to understand how uncertainty depends on our modeling choices. How can we do that with the tools we have? As we show here, we can’t. We will then expand our DGM to create models that quantify uncertainty (Bayesian models) and introduce a new way of fitting ML models called Bayesian inference.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Motivate the need for uncertainty</p></li>
<li><p>Introduce a new modeling paradigm based on Bayes’ rule</p></li>
<li><p>Provide intuition for this modeling paradigm</p></li>
<li><p>Implement this modeling paradigm in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></p></li>
<li><p>Gain intuition how different models have different uncertainty</p></li>
</ul>
<p><strong>Data:</strong> To help make the concepts concrete, we’ll return to our regression data, in which we wanted to predict telekinetic ability from age. Let’s load the data in:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a bunch of libraries we&#39;ll be using below</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro.distributions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">D</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="c1"># Load the data into a pandas dataframe</span>
<span class="n">csv_fname</span> <span class="o">=</span> <span class="s1">&#39;data/IHH-CTR-CGLF-regression-augmented.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_fname</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Patient ID&#39;</span><span class="p">)</span>

<span class="c1"># Print a random sample of patients, just to see what&#39;s in the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Glow</th>
      <th>Telekinetic-Ability</th>
    </tr>
    <tr>
      <th>Patient ID</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>90</th>
      <td>30.607729</td>
      <td>0.604085</td>
      <td>-0.020933</td>
    </tr>
    <tr>
      <th>254</th>
      <td>38.531357</td>
      <td>0.613645</td>
      <td>-0.070165</td>
    </tr>
    <tr>
      <th>283</th>
      <td>21.879414</td>
      <td>0.829212</td>
      <td>0.140791</td>
    </tr>
    <tr>
      <th>445</th>
      <td>2.949004</td>
      <td>0.981120</td>
      <td>0.261027</td>
    </tr>
    <tr>
      <th>461</th>
      <td>30.237446</td>
      <td>0.688329</td>
      <td>-0.027250</td>
    </tr>
    <tr>
      <th>15</th>
      <td>29.562483</td>
      <td>0.796853</td>
      <td>-0.033701</td>
    </tr>
    <tr>
      <th>316</th>
      <td>15.283975</td>
      <td>0.839546</td>
      <td>0.344510</td>
    </tr>
    <tr>
      <th>489</th>
      <td>2.688488</td>
      <td>0.929422</td>
      <td>0.268031</td>
    </tr>
    <tr>
      <th>159</th>
      <td>4.129371</td>
      <td>0.893813</td>
      <td>0.422464</td>
    </tr>
    <tr>
      <th>153</th>
      <td>15.194182</td>
      <td>0.832483</td>
      <td>0.375658</td>
    </tr>
    <tr>
      <th>241</th>
      <td>33.391247</td>
      <td>0.676760</td>
      <td>-0.028127</td>
    </tr>
    <tr>
      <th>250</th>
      <td>32.363740</td>
      <td>0.711121</td>
      <td>-0.078376</td>
    </tr>
    <tr>
      <th>390</th>
      <td>20.699366</td>
      <td>0.683075</td>
      <td>0.176542</td>
    </tr>
    <tr>
      <th>289</th>
      <td>51.370230</td>
      <td>0.472696</td>
      <td>-0.153246</td>
    </tr>
    <tr>
      <th>171</th>
      <td>24.983784</td>
      <td>0.703657</td>
      <td>0.028212</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="why-we-need-uncertainty">
<h2><span class="section-number">21.1. </span>Why We Need Uncertainty<a class="headerlink" href="#why-we-need-uncertainty" title="Link to this heading">#</a></h2>
<p><strong>The MLE is Over-Confident.</strong> In safety-critical contexts, like those from the IHH, it’s important that our ML models don’t just fit the observed data well; they should also communicate with us the limits of their “knowledge.” Let’s illustrate what we mean. Consider the regression data below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mi">78</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;No Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;What should the model do where there</span><span class="se">\&#39;</span><span class="s1">s no data?&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/815bc88c6b50b63bd6c66036f83f11c59406280b1d1a78c47be614e9e3789cec.png" src="_images/815bc88c6b50b63bd6c66036f83f11c59406280b1d1a78c47be614e9e3789cec.png" />
</div>
</div>
<p>If we were to make a prediction for a patient of age 85, what should the model do? The model wasn’t trained on any such patients. Does the trend keep going down with age? Doing so would ignore the point at the very right of the plot, treating it as an <em>outlier</em>. Or maybe the trend should go up after age 80? It’s impossible for us to know because we haven’t observed data about such patients. In cases such as these, it’s important that our model alert us about its uncertainty.</p>
<p>Especially in recent years, there has been more and more research dedicated to developing models that can reliably quantify uncertainty. As an example, a recent paper evaluated how confident different deep learning models are on a medical imaging task. In the paper, the authors evaluated models for predicting whether patients had COVID or not from X-ray scans. Here’s what they found:</p>
<figure class="align-center" id="cats-vs-covid">
<a class="reference internal image-reference" href="_images/cats-vs-covid.png"><img alt="_images/cats-vs-covid.png" src="_images/cats-vs-covid.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21.1 </span><span class="caption-text">ML models can be over-confident about wrong predictions. Figure adapted from <a class="reference external" href="https://www.semanticscholar.org/paper/Can-Your-AI-Differentiate-Cats-from-Covid-19-Sample-Mallick-Dwivedi/c100c33afbb2efa5197f7d6042022e1227c5e298" rel="noreferrer" target="_blank">this paper</a>.</span><a class="headerlink" href="#cats-vs-covid" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As you can see from the figure, the model makes predictions that aren’t only incorrect, <em>but also overconfident</em>. And it does this for inputs (like the cat) for which it should really just communicate “I don’t know.”</p>
<p>How can a model know when it “doesn’t know”? One way to do this is to alert us when many possible models fit the data reasonably well, but behave differently away from the data (i.e. on previously unseen inputs, like the cat). Unfortunately, the learning algorithm we’ve used so far—the MLE—doesn’t provide us with a way to do this. The MLE gives a <em>single</em> model.</p>
<p><strong>Ensembling.</strong> If the MLE gives us a single model, why not use it to fit a whole <em>ensemble</em> of models? We could rely on the imperfections of the optimizer to give us a diversity of models. Remember that, especially for more expressive models, optimization tends to get stuck in local optima. What if we were to collect an ensemble of models, all fit with the MLE to data, but each optimized from a different random initialization? Because each model would get stuck in a different local optima, each <em>might</em> behave differently than the others away from the data. What’s nice about this approach is that it’s easy to implement: we already have all the tools we need! Let’s see what ensembling a neural network regression model looks like:</p>
<figure class="align-center" id="nn-ensemble-regression">
<a class="reference internal image-reference" href="_images/example_nn_ensemble_regression.png"><img alt="_images/example_nn_ensemble_regression.png" src="_images/example_nn_ensemble_regression.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21.2 </span><span class="caption-text">Ensembling 10 neural networks of different sizes.</span><a class="headerlink" href="#nn-ensemble-regression" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As you can see, with ensembling, we managed to get a more diverse set of functions (though in this case, not all that diverse—they all have roughly the same trend). Ensembling can be quite effective in practice, but it suffers from one main shortcoming when it comes to safety-critical contexts: it makes implicit assumptions that are difficult to understand. Specifically, we relied on the imperfections of our black-box optimizer to find us a diverse set of models. What kind of models will the optimizer give us, however? Do these models have an <em>inductive bias</em> that’s appropriate for our task?</p>
<p>The need for explicit assumptions motivates us to find an alternative way of fitting our models, leading us to the <em>Bayesian approach</em>.</p>
</section>
<section id="what-is-uncertainty">
<h2><span class="section-number">21.2. </span>What is Uncertainty?<a class="headerlink" href="#what-is-uncertainty" title="Link to this heading">#</a></h2>
<p>To design a system that captures uncertainty, we first need to know what it means. To do this, let’s think about the uncertainty we encounter in everyday scenarios. Maybe these will help us formalize uncertainty mathematically.</p>
<p><strong>Total Certainty.</strong> When you have lots of data, and you have a <em>mechanistic</em> understanding of the system, you have certainty. By mechanistic understand, we mean that you can characterize the system mathematically (e.g. you can predict how quickly an object will fall because you have an equation for gravity).</p>
<blockquote>
<div><p>Example: You’re asked to predict whether the sun will rise tomorrow. Of course, you know the sun will rise tomorrow, and you’re absolutely certain about it (if you have reason to believe the sun will not rise tomorrow, please do let the teaching staff know so they can head to the course bunker). So what makes you sure the sun will rise tomorrow? There are two reasons you will likely think about. (1) You have an abundance of observational data—the sun has risen every day of your life. (2) You have a model (or inductive bias)—you know that day and night are created by the earth’s rotation around its axis. Because you understand the mechanical properties of the system, you know that certain predictions just don’t make sense—like the sun can’t rise twice in the span of 24 hours.</p>
</div></blockquote>
<p><strong>Aleatoric Uncertainty.</strong> When you have lots of data from a <em>noisy</em> system, you can be certain about the probability of an outcome, but not about the actual outcome. As an example, having observed 100 flips of a fair coin, you can say with fair certainty that the coin will land heads 50% of the time. But you will never be able to predict whether the next flip will be heads with any greater accuracy. We call this type of uncertainty, <em>aleatoric uncertainty</em>. This is uncertainty that’s due to the inherent stochasticity in the system.</p>
<blockquote>
<div><p>You’re asked to predict whether it will rain at Wellesley next week. Having lived in New England for a little while, you scoff at the possibility of getting this prediction correct. New England weather is notoriously unpredictable. So what makes you uncertain about our prediction? You have an abundance of experiential data suggesting that weather is difficult to predict (how many times have you stood outside in the rain, while your phone’s weather app says it’s sunny?). As a result, you’re certain we can’t make a good prediction.</p>
</div></blockquote>
<p>All models we’ve worked with so far quantify aleatoric uncertainty. For example, in our regression and classification models, our observation error captures aleatoric uncertainty.</p>
<p><strong>Epistemic Uncertainty.</strong> When you don’t have enough data, and you also don’t have a mechanistic understanding of the system, you have epistemic uncertainty. In this case, we’d ideally like to have a diversity of possible models that fit the data.</p>
<blockquote>
<div><p>Your colleague for the IHH is getting married on Venus next week. Your friend, who is also attending the wedding, asked you to predict the weather on Venus next week (so you can choose your outfit). Having forgotten all of your astrophysics knowledge (or having never learned it), you actually don’t know what the weather on Venus is like in general. As a result, what makes you uncertain is: (1) a lack of observations or experiential knowledge, and (2) a model (or domain knowledge) about Venus’s climate.</p>
</div></blockquote>
<p>None of our models so far have been able to capture epistemic uncertainty. In the regression case, epistemic uncertainty would be uncertainty over the <em>parameters</em> of the model. Epistemic uncertainty indicates that many potential models could explain the observed data, but we don’t know which one is the “right” one. We can reduce our uncertainty by observing more data.</p>
<p><strong>Visualizing Regions of Uncertainty.</strong> Returning to our original data of Telekinetic Ability vs. Age, we see that:</p>
<ul class="simple">
<li><p>Where we’ve observed data, there’s <em>aleatoric uncertainty</em>: there’s “noise” around the trend. No matter how good your model is, it will only be able to predict the trend, not the noise around it.</p></li>
<li><p>Where we haven’t observed data, there’s <em>epistemic uncertainty</em>: we don’t know what’s the appropriate model behavior. In addition to epistemic uncertainty, there’s still aleatoric uncertainty.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mi">78</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Epistemic Uncertainty&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Aleatoric Uncertainty&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epistemic vs. Aleatoric Uncertainty&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/81e63c9b908bc4ed0d50801edc465a09dcf946e6dcc8a633fe5ed2e97b77e0d9.png" src="_images/81e63c9b908bc4ed0d50801edc465a09dcf946e6dcc8a633fe5ed2e97b77e0d9.png" />
</div>
</div>
<p><strong>Conclusion.</strong> We need some way of capturing epistemic uncertainty. To do this, we’ll next extend our DGM toolkit, as well as introduce a different way of fitting models, called <em>Bayesian inference</em>.</p>
<div class="admonition-exercise-identify-type-of-uncertainty admonition">
<p class="admonition-title">Exercise: Identify Type of Uncertainty</p>
<p>In each of the scenarios below, identify the sources of uncertainty (i.e. why you can’t be certain about the outcome—be creative!), and categorize them into aleatoric vs. epistemic. Explain your reasoning.</p>
<ol class="arabic simple">
<li><p>A new tree was planted outside your window. You want to predict how tall it will be in one year.</p></li>
<li><p>You want to predict your grade on your next CS exam.</p></li>
<li><p>You just met someone at a party. You want to predict their mood tomorrow.</p></li>
</ol>
</div>
</section>
<section id="the-bayesian-modeling-paradigm">
<h2><span class="section-number">21.3. </span>The Bayesian Modeling Paradigm<a class="headerlink" href="#the-bayesian-modeling-paradigm" title="Link to this heading">#</a></h2>
<p><strong>Capturing Epistemic Uncertainty.</strong> So let’s go back to the drawing board and rethink how we’ve been fitting models this whole time. So far, our approach has been finding the <em>single</em> model that maximizes the probability of our observed data: <span class="math notranslate nohighlight">\(\theta^\text{MLE} = \mathrm{argmax}_\theta \log p(\mathcal{D}; \theta)\)</span>. But isn’t what we’re actually interested is the <em>distribution</em> of models given the data, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>? In other words, conditioned on the data we’ve observed so far, we want to know which models (represented by their parameters, <span class="math notranslate nohighlight">\(\theta\)</span>) are likely to fit the data well. In this new paradigm, we hope that:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> will capture a diversity of models with different inductive biases.</p></li>
<li><p>We can make our assumptions clear, and we can specify what type of inductive biases are appropriate for our task.</p></li>
</ol>
<p>And assuming we could compute this distribution, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, how would we actually use it for fitting a model? The process goes something like this:</p>
<ol class="arabic simple">
<li><p><strong>Prior:</strong> Prior to having observed data, we express our beliefs about possible sets of parameters, <span class="math notranslate nohighlight">\(\theta\)</span>. Our beliefs don’t have to be correct, just reasonable. We then encode our beliefs into a distribution, <span class="math notranslate nohighlight">\(p_\theta(\cdot)\)</span>, called the “prior.”  For example, we scientifically believe that as age increases, glow decreases. As such, we force the slope to be negative by setting <span class="math notranslate nohighlight">\(p_\theta(\cdot)\)</span> to be a Normal distribution centered at some negative number.</p></li>
<li><p><strong>Likelihood:</strong> Having observed data, we can score how well any set of parameters, <span class="math notranslate nohighlight">\(\theta\)</span>, from the prior fits the data by evaluating <span class="math notranslate nohighlight">\(p(\mathcal{D} | \theta)\)</span>, the joint data likelihood. This distribution is the <em>very same</em> distribution we’ve worked with in all previous chapters.</p></li>
<li><p><strong>Posterior Update:</strong> Post observing data, we <em>update</em> our beliefs about <span class="math notranslate nohighlight">\(\theta\)</span>. We do this by computing <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>. Observing data will help us reduce the initial uncertainty from the prior, honing in on a set of parameters that could explain the data well. As we show in a bit, this posterior update will depend on both the prior and the likelihood distributions.</p></li>
</ol>
<p><strong>Bayesian Models.</strong> By having a prior distribution <span class="math notranslate nohighlight">\(\theta\)</span> to encode our beliefs, we now treat <span class="math notranslate nohighlight">\(\theta\)</span> as a <em>random variable</em>. This means that our generative process will now include an additional line, in which we sample <span class="math notranslate nohighlight">\(\theta\)</span> from the prior. For example, for Bayesian regression, our generative process is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-926777fa-c831-4b00-b46a-68a63c3b581e">
<span class="eqno">(21.1)<a class="headerlink" href="#equation-926777fa-c831-4b00-b46a-68a63c3b581e" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \quad (\text{prior}) \\
y_n | x_n, \theta &amp;\sim p_{Y | X}(\cdot | x_n, \theta) = \mathcal{N}(\mu(x_n; \theta), \sigma^2) \quad (\text{likelihood}) \\
\end{align}\]</div>
<p>We can similarly depict our Bayesian model using a directed graphical model as follows:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe" src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLCabxKck&#x2F;QEOGc0tkynDEzLYyuE3z9w&#x2F;view?embed">
    </iframe>
  </div>
</div>
<p>As you can see, the difference between the depiction of the non-Bayesian and the Bayesian regression is that <span class="math notranslate nohighlight">\(\theta\)</span> is in a circle, indicating its a random variable. Next, the circle is <em>white</em> (not filled in), indicating that <span class="math notranslate nohighlight">\(\theta\)</span> is not observed. Our goal will be to infer it given the data.</p>
<p>Before defining <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, let’s walk through an example to show you what this process looks like.</p>
<p><strong>Illustration: Bayesian Regression.</strong> Let’s see what the Bayesian modeling paradigm looks like for regression, visually. We’ll use the generative process from above, setting <span class="math notranslate nohighlight">\(\sigma\)</span> as a constant (so we can ignore it). We’ve picked an expressive function, <span class="math notranslate nohighlight">\(\mu(x_n; \theta)\)</span>, that will be fun to visualize—its details aren’t important.</p>
<p>Given our generative process, our goal is to sample the posterior,</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e56c285-8b60-437f-a976-4388de4db12a">
<span class="eqno">(21.2)<a class="headerlink" href="#equation-6e56c285-8b60-437f-a976-4388de4db12a" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\theta | \mathcal{D}) &amp;= p(\theta | x_1, \dots, x_N, y_1, \dots, y_n).
\end{align}\]</div>
<p>For intuition, we can visualize posterior samples <span class="math notranslate nohighlight">\(\theta \sim p(\theta | \mathcal{D})\)</span> by plotting the <em>functions</em> they represent, <span class="math notranslate nohighlight">\(\mu(x_n; \theta)\)</span>. The plot below shows samples from the posterior as the number of points, <span class="math notranslate nohighlight">\(N\)</span>, increases.</p>
<figure class="align-center" id="bayesian-update-example">
<a class="reference internal image-reference" href="_images/example_online_bayesian_regression.png"><img alt="_images/example_online_bayesian_regression.png" src="_images/example_online_bayesian_regression.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21.3 </span><span class="caption-text">Samples from the posterior of a Bayesian regression model, capturing epistemic uncertainty.</span><a class="headerlink" href="#bayesian-update-example" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In the above plot, <span class="math notranslate nohighlight">\(N = 0\)</span> represents our <em>prior</em>. The functions drawn from our prior illustrate our beliefs about which functions are appropriate for the data. In this specific case, our prior functions don’t exhibit any strong trends; overall, the functions don’t increase/decrease as age increases—they just wiggle about. However, the functions are incredibly smooth—another prior may have drawn more jagged functions. Whether this prior is appropriate for our task is up to you to decide.</p>
<p>Next, we see what happens as we start observing data. As <span class="math notranslate nohighlight">\(N\)</span> increases, you can see our prior distribution getting “filtered out” by the likelihood. By this, we mean that our posterior will sample functions that are both likely under the prior <em>and</em> likelihood. It therefore keeps samples from the prior that also go <em>through the data</em> to ensure the likelihood is high. As you can see, in regions of the input space near our observed data, the posterior is quite certain about the trend; it knows the function must pass close to the observed data. But as we move away from the observed data, the posterior maintains a diversity of possible functions.</p>
</section>
<section id="model-fitting-via-bayes-rule">
<h2><span class="section-number">21.4. </span>Model-Fitting via Bayes’ Rule<a class="headerlink" href="#model-fitting-via-bayes-rule" title="Link to this heading">#</a></h2>
<p><strong>Bayes’ Rule.</strong> But what is <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, exactly? How can we possibly write down a distribution of models that fit the data well by hand? To avoid specifying this distribution by hand, we will use <em>Baye’s rule</em> to write down <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> in terms of what we already know how to specify: the joint data likelihood, <span class="math notranslate nohighlight">\(p(\mathcal{D} | \theta)\)</span>, and the prior.</p>
<p>Let’s derive Bayes’ rule in general before applying it to our problem. Recall from the chapter on joint probability that a joint distribution over two random variables, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, can be factorized as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-92380bf1-4ebb-45af-a91f-03a1f05572e9">
<span class="eqno">(21.3)<a class="headerlink" href="#equation-92380bf1-4ebb-45af-a91f-03a1f05572e9" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B}(a, b) &amp;= p_{B | A}(b | a) \cdot p_A(a) \quad (\text{Option 1}) \\
&amp;= p_{A | B}(a | b) \cdot p_B(b) \quad (\text{Option 2})
\end{align}\]</div>
<p>This means we can also equate the two factorizations:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5e613a90-6300-4328-9e12-ee28a7de0e37">
<span class="eqno">(21.4)<a class="headerlink" href="#equation-5e613a90-6300-4328-9e12-ee28a7de0e37" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{B | A}(b | a) \cdot p_A(a) &amp;= p_{A | B}(a | b) \cdot p_B(b)
\end{align}\]</div>
<p>Diving both sides by <span class="math notranslate nohighlight">\(p_A(a)\)</span>, we get:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f0c894dd-56cf-4bdb-939a-51647b1e2fe4">
<span class="eqno">(21.5)<a class="headerlink" href="#equation-f0c894dd-56cf-4bdb-939a-51647b1e2fe4" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{B | A}(b | a) &amp;= \frac{p_{A | B}(a | b) \cdot p_B(b)}{p_A(a)} \quad \text{(Bayes' Rule)}
\end{align}\]</div>
<p>This is Bayes’ rule. What’s cool about it is that relates <span class="math notranslate nohighlight">\(p_{B | A}(b | a)\)</span> to <span class="math notranslate nohighlight">\(p_{A | B}(a | b)\)</span>.</p>
<p><strong>Bayesian Inference.</strong> Using Bayes’ rule in the context of our problem, let’s treat <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> <em>and</em> <span class="math notranslate nohighlight">\(\theta\)</span> as random variables. We can now relate <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, which we don’t know how to specify, to <span class="math notranslate nohighlight">\(p(\mathcal{D} | \theta)\)</span>, which we do know how to specify:</p>
<div class="amsmath math notranslate nohighlight" id="equation-13e572f3-d072-4ef2-9b4d-5f659a319582">
<span class="eqno">(21.6)<a class="headerlink" href="#equation-13e572f3-d072-4ef2-9b4d-5f659a319582" title="Permalink to this equation">#</a></span>\[\begin{align}
\underbrace{p(\theta | \mathcal{D})}_{\text{posterior}} &amp;= \frac{\overbrace{p(\mathcal{D} | \theta)}^{\text{likelihood}} \cdot \overbrace{p(\theta)}^{\text{prior}}}{\underbrace{p(\mathcal{D})}_{\text{normalizing const.}}}
\end{align}\]</div>
<p>When used as a model-fitting paradigm, each term in Bayes’ rule has a special name. We’ll now define each:</p>
<ul>
<li><p><strong>Likelihood:</strong> This is the data joint likelihood, which we’ve previously maximized as part of the MLE.</p>
<blockquote>
<div><p>For example, suppose we’re fitting a linear regression model to predict an intergalactic being’s glow given age. Our model is then:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b4f4555b-f547-4590-9da9-1a2a44871d52">
<span class="eqno">(21.7)<a class="headerlink" href="#equation-b4f4555b-f547-4590-9da9-1a2a44871d52" title="Permalink to this equation">#</a></span>\[\begin{align}
    p(\mathcal{D} | \theta) &amp;= \prod\limits_{n=1}^N p(\mathcal{D}_n | \theta) \\
    &amp;= \prod\limits_{n=1}^N p_{Y | X}(y_n | x_n, \theta) \\
    &amp;= \prod\limits_{n=1}^N \mathcal{N}(y_n | \underbrace{\theta_0 + \theta_1 \cdot x_n}_{\mu(x_n; \theta)}, \sigma^2)
    \end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = \{ \theta_0, \theta_1 \}\)</span> is the slope and intercept, and <span class="math notranslate nohighlight">\(\sigma\)</span> is observation noise variance (which we fix as a constant for now).</p>
</div></blockquote>
</li>
<li><p><strong>Prior:</strong> This is the distribution of models we’re willing to consider <em>before having observed any data</em>. The prior allows us to specify our model’s <em>inductive bias</em>.</p>
<blockquote>
<div><p>Continuing with the above example, we know that in general, glow decreases with age. We can encode this belief into the inductive bias of the model by selecting an appropriate prior distribution—one for which the slope, <span class="math notranslate nohighlight">\(\theta_1\)</span>, is likely negative. As an example, we could select, <span class="math notranslate nohighlight">\(\mathcal{N}(-1, 0.1)\)</span>. In this way, <span class="math notranslate nohighlight">\(\theta_1\)</span> is most likely to be near <span class="math notranslate nohighlight">\(-1\)</span>. We can similarly encode our belief into the intercept, <span class="math notranslate nohighlight">\(\theta_0\)</span>, saying we believe it should be positive: <span class="math notranslate nohighlight">\(\mathcal{N}(1, 0.1)\)</span>.
Putting these together, we get the following prior distribution over our model parameters:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d2afb5a0-7048-4b67-a54c-96c45120e781">
<span class="eqno">(21.8)<a class="headerlink" href="#equation-d2afb5a0-7048-4b67-a54c-96c45120e781" title="Permalink to this equation">#</a></span>\[\begin{align}
    p_\theta(\cdot) = p_{\theta_1}(\cdot) \cdot p_{\theta_0}(\cdot) = \mathcal{N}(-1, 0.1) \cdot \mathcal{N}(1, 0.1)
    \end{align}\]</div>
</div></blockquote>
<p>In contrast to the ensembling approach, prior specification makes our assumptions about uncertainty explicit and easier to interrogate.</p>
</li>
<li><p><strong>Posterior:</strong> This is the distribution of interest. It’s called a posterior because it determines the distribution of likely models, <span class="math notranslate nohighlight">\(\theta\)</span>, <em>after having observed data</em>. The posterior balances information from both the prior and the likelihood.</p></li>
<li><p><strong>Normalizing Constant:</strong> This is a constant that turns the whole fraction into a valid probability density function (i.e. a function that integrates to 1). To compute <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>, we use the law of total probability:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dc3daa27-c062-4985-9e4f-e1f3803568a5">
<span class="eqno">(21.9)<a class="headerlink" href="#equation-dc3daa27-c062-4985-9e4f-e1f3803568a5" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}) &amp;= \int\limits \underbrace{p(\mathcal{D} | \theta) \cdot p(\theta)}_{\text{numerator of Bayes' rule}} d\theta
\end{align}\]</div>
<p>Notice that the law of total probability tells us to integrate the numerator of Bayes’ rule over the support of <span class="math notranslate nohighlight">\(\theta\)</span>. In doing so, when we divide by it, the whole fraction integrates to <span class="math notranslate nohighlight">\(1\)</span>. In general, marginalizing out a variable via the law of total probability is intractable—there’s no analytic solution to it, and approximating it is computationally too expensive. For now, we won’t worry about how to compute this integral (or how to <em>avoid</em> computing it).</p>
</li>
</ul>
<p><strong>Computational Efficiency.</strong> Unfortunately for us, for most models, Bayesian inference is <em>intractable</em>, meaning there exists no efficient algorithm for posterior sampling. As a result, we will have to resort to approximations. This is the main drawback of Bayesian inference. Approximate Bayesian inference is fascinating, but unfortunately, we will not get to study it here. We will, however, learn how to use some approximate inference algorithms.</p>
</section>
<section id="bayesian-models-in-numpyro">
<h2><span class="section-number">21.5. </span>Bayesian Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code><a class="headerlink" href="#bayesian-models-in-numpyro" title="Link to this heading">#</a></h2>
<p><strong>The Model.</strong> As you may have expected, writing out explicitly all parts of the model in math will now allow us to translate into <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>. And using some wizardry, <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> will do the heavy lifting for us, sampling from the model’s posterior.</p>
<p>The process of describing a Bayesian in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> will actually not differ much from the process of writing its coding its non-Bayesian counterpart. We’ll therefore start with the non-Bayesian version using the syntax you’re already familiar with, and then we’ll show you how to make it Bayesian.</p>
<p><strong>Review: Non-Bayesian Linear Regression.</strong> For the specific model we’ll implement, we’ll use a univariate linear regression model. Here’s the non-Bayesian version.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">univariate_linear_regression</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;slope&#39;</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">]),</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">intercept</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;intercept&#39;</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
        <span class="s1">&#39;std_dev&#39;</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">deterministic</span><span class="p">(</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">/</span> <span class="mf">100.0</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span>
        <span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">std_dev</span><span class="p">)</span>
        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">obs</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In the above model, we use <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code>. Notice how <code class="docutils literal notranslate"><span class="pre">pfml.sample_generative_process</span></code> returns variables created with <code class="docutils literal notranslate"><span class="pre">numpyro.param</span></code> and <code class="docutils literal notranslate"><span class="pre">numpyro.sample</span></code>? Recall that <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code>, allows you to save all other variables. In this case, since we’re interested in visualizing <em>epistemic uncertainty</em>, we want to visualize <span class="math notranslate nohighlight">\(\mu(\cdot; \theta)\)</span>. This new primitive allows us to save it. When calling <code class="docutils literal notranslate"><span class="pre">pfml.sample_generative_process</span></code>, we’ll now be able to see a new variable called <code class="docutils literal notranslate"><span class="pre">mu</span></code>.</p>
<p>For completeness, let’s also fit the non-Bayesian model using the MLE to our IHH data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_ITERATIONS</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># Define an optimizer; here we chose the &quot;Adam&quot; algorithm</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Pick a random generator seed for the optimizer</span>
<span class="n">key_optimizer</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fit the model via the MLE</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pfml</span><span class="o">.</span><span class="n">mle</span><span class="p">(</span>
    <span class="n">univariate_linear_regression</span><span class="p">,</span> 
    <span class="n">optimizer</span><span class="p">,</span> 
    <span class="n">key_optimizer</span><span class="p">,</span> 
    <span class="n">NUM_ITERATIONS</span><span class="p">,</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> 
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]),</span> 
    <span class="n">y</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">]),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/10000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 1/10000 [00:00&lt;33:47,  4.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  6%|▌         | 578/10000 [00:00&lt;00:03, 2387.10it/s, init loss: 566.6539, avg. loss [1-500]: -229.7082]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 11%|█         | 1051/10000 [00:00&lt;00:02, 3255.42it/s, init loss: 566.6539, avg. loss [501-1000]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 15%|█▌        | 1509/10000 [00:00&lt;00:02, 3713.89it/s, init loss: 566.6539, avg. loss [1001-1500]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 20%|█▉        | 1963/10000 [00:00&lt;00:02, 3985.39it/s, init loss: 566.6539, avg. loss [1001-1500]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 24%|██▍       | 2416/10000 [00:00&lt;00:01, 4158.61it/s, init loss: 566.6539, avg. loss [1501-2000]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 29%|██▉       | 2898/10000 [00:00&lt;00:01, 4366.39it/s, init loss: 566.6539, avg. loss [2001-2500]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 34%|███▍      | 3447/10000 [00:00&lt;00:01, 4714.33it/s, init loss: 566.6539, avg. loss [2501-3000]: -382.8964]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 39%|███▉      | 3932/10000 [00:01&lt;00:01, 4639.13it/s, init loss: 566.6539, avg. loss [3001-3500]: -382.8932]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 44%|████▍     | 4405/10000 [00:01&lt;00:01, 4643.30it/s, init loss: 566.6539, avg. loss [3501-4000]: -382.8935]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 49%|████▉     | 4876/10000 [00:01&lt;00:01, 4654.33it/s, init loss: 566.6539, avg. loss [4001-4500]: -382.8937]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 54%|█████▍    | 5402/10000 [00:01&lt;00:00, 4832.93it/s, init loss: 566.6539, avg. loss [4501-5000]: -382.8937]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 59%|█████▉    | 5903/10000 [00:01&lt;00:00, 4883.72it/s, init loss: 566.6539, avg. loss [5001-5500]: -382.8937]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 64%|██████▍   | 6394/10000 [00:01&lt;00:00, 4809.01it/s, init loss: 566.6539, avg. loss [5501-6000]: -382.8936]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 69%|██████▉   | 6877/10000 [00:01&lt;00:00, 4753.73it/s, init loss: 566.6539, avg. loss [6001-6500]: -382.8941]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 74%|███████▎  | 7354/10000 [00:01&lt;00:00, 4666.11it/s, init loss: 566.6539, avg. loss [6501-7000]: -382.8961]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 79%|███████▊  | 7874/10000 [00:01&lt;00:00, 4820.44it/s, init loss: 566.6539, avg. loss [7001-7500]: -382.8939]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 84%|████████▎ | 8358/10000 [00:01&lt;00:00, 4747.52it/s, init loss: 566.6539, avg. loss [7501-8000]: -382.8923]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 88%|████████▊ | 8834/10000 [00:02&lt;00:00, 4743.26it/s, init loss: 566.6539, avg. loss [8001-8500]: -382.8934]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 93%|█████████▎| 9311/10000 [00:02&lt;00:00, 4747.96it/s, init loss: 566.6539, avg. loss [8501-9000]: -382.8937]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 99%|█████████▉| 9877/10000 [00:02&lt;00:00, 5015.47it/s, init loss: 566.6539, avg. loss [9001-9500]: -382.8939]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 10000/10000 [00:02&lt;00:00, 4415.08it/s, init loss: 566.6539, avg. loss [9501-10000]: -382.8940]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done.
</pre></div>
</div>
</div>
</div>
<p>Using the fitted model, we can make predictions as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions for a set of test inputs</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">pfml</span><span class="o">.</span><span class="n">sample_generative_process</span><span class="p">(</span>
    <span class="n">result</span><span class="o">.</span><span class="n">model_mle</span><span class="p">,</span> 
    <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> 
    <span class="n">x_test</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Notice how <code class="docutils literal notranslate"><span class="pre">samples</span></code> now contains <code class="docutils literal notranslate"><span class="pre">mu</span></code>, saved from <code class="docutils literal notranslate"><span class="pre">numpyro.deterministic</span></code>.</p>
<p>Finally, let’s plot its loss and trend against the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot the loss</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">NUM_ITERATIONS</span><span class="p">),</span> <span class="n">result</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Optimization Step&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Convergence of MLE&#39;</span><span class="p">)</span>

<span class="c1"># Plot the trend of the regression</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x_test</span><span class="p">,</span> 
    <span class="n">y_pred</span><span class="p">,</span> 
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> 
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mu(\cdot; \theta)$&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plot the data</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">],</span> 
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Non-Bayesian Linear Regression&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15f7b0eba09fa356e73fd6b4f74251d3f190ec3881adda4504b4cf6f1ccf02a8.png" src="_images/15f7b0eba09fa356e73fd6b4f74251d3f190ec3881adda4504b4cf6f1ccf02a8.png" />
</div>
</div>
<p>As you can see, our gradient optimizer converged, and our model fits the data as well as any linear model can fit a non-linear trend.</p>
<p><strong>Bayesian Linear Regression.</strong> The main difference between the non-Bayesian and the Bayesian versions of the model is that we no longer have fixed parameters. Our parameters are now random variables with distributions (or priors). As such, instead of using the primitive <code class="docutils literal notranslate"><span class="pre">numpyro.param</span></code>, we will use <code class="docutils literal notranslate"><span class="pre">numpyro.sample</span></code>. For example, instead of describing <code class="docutils literal notranslate"><span class="pre">slope</span></code> as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">slope</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
    <span class="s1">&#39;slope&#39;</span><span class="p">,</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">]),</span>
    <span class="n">constraint</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>we instead write:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p_slope</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;slope&#39;</span><span class="p">,</span> <span class="n">p_slope</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, we specified that the prior distribution for <code class="docutils literal notranslate"><span class="pre">slope</span></code> is a Gaussian, centered at <span class="math notranslate nohighlight">\(-1.0\)</span>. Lastly, notice that, unlike in <code class="docutils literal notranslate"><span class="pre">numpyro.sample('y',</span> <span class="pre">p_y_given_x,</span> <span class="pre">obs=y)</span></code>,  when sampling the slope we <em>do not</em> pass in <code class="docutils literal notranslate"><span class="pre">obs=</span></code>. This is because we have not <em>observed</em> slope in the data—this tells <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> we’d like to <em>infer</em> it.</p>
<p>Replacing all fixed parameters with random variables yields the following model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">univariate_bayesian_linear_regression</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Prior for the slope</span>
    <span class="n">p_slope</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;slope&#39;</span><span class="p">,</span> <span class="n">p_slope</span><span class="p">)</span>

    <span class="c1"># Prior for the intercept</span>
    <span class="n">p_intercept</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;intercept&#39;</span><span class="p">,</span> <span class="n">p_intercept</span><span class="p">)</span>

    <span class="c1"># Inverse gamma is a standard prior for the observation noise variance</span>
    <span class="c1"># To get the standard deviation, we just take the square-root</span>
    <span class="n">p_var</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">InverseGamma</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">p_var</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

    <span class="c1"># Same as in the non-Bayesian version</span>
    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">deterministic</span><span class="p">(</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">/</span> <span class="mf">100.0</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span>
        <span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">std_dev</span><span class="p">)</span>
        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">obs</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Note:</strong> In these examples, we divide <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(100.0\)</span> when computing <code class="docutils literal notranslate"><span class="pre">mu</span></code> since Age is on a range from 0 to 100, which is quite large. This will numerically help our inference later.</p>
</section>
<section id="bayesian-inference-in-numpyro">
<h2><span class="section-number">21.6. </span>Bayesian Inference in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code><a class="headerlink" href="#bayesian-inference-in-numpyro" title="Link to this heading">#</a></h2>
<p><strong>Posterior Sampling.</strong> Now that we have our Bayesian model, let’s fit it by sampling from the posterior. We will do this using a helper function we’ve created, <code class="docutils literal notranslate"><span class="pre">pfml.bayesian_inference</span></code>. This function uses a class of algorithm called <em>Markov Chain Monte Carlo</em> (MCMC). We will not get into how such algorithms work, but there are a few things you should know about them:</p>
<ol class="arabic simple">
<li><p>They are iterative.</p></li>
<li><p>Given an infinite number of iterations, they will eventually draw samples from the model’s posterior.</p></li>
<li><p>Since we cannot run them for an infinite number of iterations, we have to take a few precautions. First, it takes MCMC a while to start drawing samples from the posterior. As a result, we toss out the first batch of samples drawn in what’s called a “warmup” phase. Second, there are some diagnostics you can run to assess the quality of the samples.</p></li>
</ol>
<p>Our helper function takes in the following arguments:</p>
<ul class="simple">
<li><p>A Bayesian <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> model.</p></li>
<li><p>A random generator key, used by the inference algorithm.</p></li>
<li><p>The number of warmup steps we’d like the algorithm to run for. To ensure your model converges, you should make this number as large as you can tolerate!</p></li>
<li><p>The number of samples to draw (after the warmup phase).</p></li>
<li><p>Finally, we pass in the arguments of the model.</p></li>
</ul>
<p>Putting all of this together, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">pfml</span><span class="o">.</span><span class="n">bayesian_inference</span><span class="p">(</span>
    <span class="n">univariate_bayesian_linear_regression</span><span class="p">,</span> 
    <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> 
    <span class="mi">20000</span><span class="p">,</span> 
    <span class="mi">5000</span><span class="p">,</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> 
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]),</span> 
    <span class="n">y</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">]),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our helper function returns a dictionary, <code class="docutils literal notranslate"><span class="pre">posterior_samples</span></code>, the posterior samples. Have a look:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">posterior_samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot; has shape = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;intercept&quot; has shape = (5000,)
&quot;mu&quot; has shape = (5000, 500)
&quot;slope&quot; has shape = (5000,)
&quot;var&quot; has shape = (5000,)
</pre></div>
</div>
</div>
</div>
<p>As you can see, the <span class="math notranslate nohighlight">\(0\)</span>th dimension of all of the shapes is the number of samples drawn.</p>
<p><strong>Visualizing the Posterior.</strong> Since we have such a small number of parameters—the slope, intercept, and observation noise variance—let’s visualize their posterior distributions. Specifically, let’s visualize the joint distribution of the slope and intercept (darker means higher probability):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist2d</span><span class="p">(</span>
    <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;slope&#39;</span><span class="p">],</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">],</span> 
    <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;BuPu&#39;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Slope&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Intercept&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior Distribution of the Slope and Intercept&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8543dc67b34786d48466359020ba41828a32dbac333b2e496739083e41c76ff8.png" src="_images/8543dc67b34786d48466359020ba41828a32dbac333b2e496739083e41c76ff8.png" />
</div>
</div>
<p>Looking at the above distribution, you can see that the slope and intercept are <em>negatively correlated</em>; as slope increases, the intercept decreases. But in the prior, we sampled the slope and intercept from two <em>independent</em> Gaussians—how come they are correlated in the posterior? This is because once we observe data, we the posterior hones in on models that fit the data well. As a result, if the slope increases, the intercept has to decrease to maintain good fit (and vice versa).</p>
<p><strong>Predicting from Posterior Samples.</strong> Looking at <code class="docutils literal notranslate"><span class="pre">posterior_samples</span></code>, notice that the shape of <code class="docutils literal notranslate"><span class="pre">mu</span></code> is the number of samples by number of training points. However, we would like to make predictions for non-training points. How can we do that? Using another helper function we’ve created: <code class="docutils literal notranslate"><span class="pre">pfml.sample_predictive</span></code>. This function takes in:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> model.</p></li>
<li><p>A random generator key.</p></li>
<li><p>The posterior samples, returned from <code class="docutils literal notranslate"><span class="pre">pfml.bayesian_inference</span></code>.</p></li>
<li><p>Arguments needed for the model. In this case, the arguments are the test points.</p></li>
</ul>
<p>The functions will then apply posterior samples to the new test inputs, giving us a collection of <code class="docutils literal notranslate"><span class="pre">mu</span></code>s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictive_samples</span> <span class="o">=</span> <span class="n">pfml</span><span class="o">.</span><span class="n">sample_predictive</span><span class="p">(</span>
    <span class="n">univariate_bayesian_linear_regression</span><span class="p">,</span> 
    <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">posterior_samples</span><span class="p">,</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span>
    <span class="n">x_test</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">predictive_samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot; has shape = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;mu&quot; has shape = (5000, 100)
&quot;y&quot; has shape = (5000, 100)
</pre></div>
</div>
</div>
</div>
<p>Now, we can go ahead and visualize our samples of <code class="docutils literal notranslate"><span class="pre">mu</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Iterate over posterior samples and plot them</span>
<span class="c1"># Plot every 5th sample so as not to overwhelm </span>
<span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">predictive_samples</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">][::</span><span class="mi">5</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">x_test</span><span class="p">,</span> 
        <span class="n">mu</span><span class="p">,</span> 
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> 
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Plot the training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">],</span> 
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bayesian Linear Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a1167166142cffe92b4cfd2fca43e1c7874d39e5169132e104ac679b2c99ed33.png" src="_images/a1167166142cffe92b4cfd2fca43e1c7874d39e5169132e104ac679b2c99ed33.png" />
</div>
</div>
<p>As you can see from the figure, we have a distribution over different linear functions that fit our data best. Of course, none of them are particularly good, since the model is non-linear. But in the region where we have less data (age <span class="math notranslate nohighlight">\(&gt; 80\)</span>), the model is more uncertain.</p>
</section>
<section id="comparing-bayesian-regression-models">
<h2><span class="section-number">21.7. </span>Comparing Bayesian Regression Models<a class="headerlink" href="#comparing-bayesian-regression-models" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-compare-posteriors-of-different-bayesian-regression-models admonition">
<p class="admonition-title">Exercise: Compare Posteriors of Different Bayesian Regression Models</p>
<p>Your goal is to develop a Bayesian predictive model for Telekinetic Ability given Age. We’ll work with a smaller version of the data for this problem: <code class="docutils literal notranslate"><span class="pre">data/IHH-CTR-CGLF-regression-subsampled.csv</span></code>.</p>
<p><strong>Part 1:</strong> Implement a Bayesian polynomial regression model. Please use your previous implementations of the non-Bayesian versions of these models as your starting point. Use a standard Gaussian, <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>, for the prior over all parameters.</p>
<p><strong>Part 2:</strong> Implement a 1-layer Bayesian neural network. Please use your previous implementations of the non-Bayesian versions of these models as your starting point. Use a standard Gaussian, <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>, for the prior over all parameters (except the observation noise variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>). (For this exercise, please do not use <code class="docutils literal notranslate"><span class="pre">neural_network_fn</span></code>, which provided only for the chapter on Factor Analysis).</p>
<p><strong>Part 3:</strong> Given the IHH data above, perform inference on the following models:</p>
<ul class="simple">
<li><p>Bayesian linear regression (polynomial regression of degree 1)</p></li>
<li><p>Bayesian polynomial regression of degree 5</p></li>
<li><p>Bayesian polynomial regression of degree 6</p></li>
<li><p>Bayesian neural network regression with 1 layer, 20 hidden nodes, and sigmoid activation</p></li>
<li><p>Bayesian neural network regression with 1 layer, 20 hidden nodes, and ReLU activation</p></li>
</ul>
<p><strong>Part 4:</strong> Plot their epistemic uncertainty (i.e. the distribution of <code class="docutils literal notranslate"><span class="pre">mu</span></code>).</p>
<p><strong>Part 5:</strong> How does each of their epistemic uncertainty differ?</p>
<p><strong>Part 6:</strong> For every model, change the standard deviation of the Normal priors. What happens to the epistemic uncertainty as you increase/decrease the standard deviation of the prior?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ethics-of-generative-models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">20. </span>The Ethics of Generative Models in Sociotechnical Systems</p>
      </div>
    </a>
    <a class="right-next"
       href="posterior-predictives.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Posterior Predictives</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-uncertainty">21.1. Why We Need Uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-uncertainty">21.2. What is Uncertainty?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-paradigm">21.3. The Bayesian Modeling Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-bayes-rule">21.4. Model-Fitting via Bayes’ Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models-in-numpyro">21.5. Bayesian Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-in-numpyro">21.6. Bayesian Inference in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-bayesian-regression-models">21.7. Comparing Bayesian Regression Models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>