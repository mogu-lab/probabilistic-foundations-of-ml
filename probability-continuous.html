
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>11. Probability (Continuous) &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'probability-continuous';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/probability-continuous.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="12. The Ethics of Learning from Data" href="ethics-of-learning-from-data.html" />
    <link rel="prev" title="10. Optimization" href="optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/probability-continuous.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability (Continuous)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-continuous-and-discrete-probability">11.1. Differences Between Continuous and Discrete Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations">11.2. Expectations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-discrete-continuous-models">11.3. MLE for Discrete-Continuous Models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability-continuous">
<h1><span class="section-number">11. </span>Probability (Continuous)<a class="headerlink" href="#probability-continuous" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> Many real-world data sets include non-discrete values (e.g. a patient’s body-mass index (BMI), the dosage of medicine, and more). Here, we will introduce what you need to know in order to model continuous-valued data.</p>
<p><strong>Challenge:</strong> In many ways, continuous probability is similar to discrete probability. However, there are a few “gotchas” that are important to highlight.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce and practice the concepts, terminology, and notation behind continuous probability distributions.</p></li>
<li><p>Gain familiarity with several common continuous distributions.</p></li>
</ul>
<section id="differences-between-continuous-and-discrete-probability">
<h2><span class="section-number">11.1. </span>Differences Between Continuous and Discrete Probability<a class="headerlink" href="#differences-between-continuous-and-discrete-probability" title="Link to this heading">#</a></h2>
<p>Continuous probability distributions function the same way as discrete probability, except for a few key differences.</p>
<p><strong>Sample Space of Support:</strong> The support of continuous probability distributions is over “uncountably infinite sets.” If you’re not familiar with this term, that’s ok! In this course, we’ll think about it more as a distribution supported over the real numbers, <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (or some subset thereof).</p>
<blockquote>
<div><p>Example: Let <span class="math notranslate nohighlight">\(H\)</span> be a continuous RV, describing the distribution of heights of patients entering the IHH ER. The support of <span class="math notranslate nohighlight">\(H\)</span> is the interval <span class="math notranslate nohighlight">\((0, \infty)\)</span> on the real line.</p>
</div></blockquote>
<p><strong>No Probability Mass Function (PMF):</strong> Continuous probability distributions <em>DO NOT have PMFs</em>; this is because, unlike discrete distributions, we cannot think of continuous distributions in terms of frequency. Let’s illustrate this with an example.</p>
<blockquote>
<div><p>Example: Suppose we are modeling the probability of intoxication as a Bernoulli RV, <span class="math notranslate nohighlight">\(I\)</span>. If we say that the probability of intoxication (or <span class="math notranslate nohighlight">\(I=1\)</span>) is <span class="math notranslate nohighlight">\(0.5\)</span> (meaning <span class="math notranslate nohighlight">\(p_I(1) = 0.5\)</span>), meaning half of our patients will have intoxication. Such a statement about the PMF of a discrete distribution can be immediately translated into intuition about frequency. On the other hand, suppose we have a continuous RV, <span class="math notranslate nohighlight">\(H\)</span>, modeling the height of patients. How can we describe the probability that a patient is 50 inches tall (i.e. what is <span class="math notranslate nohighlight">\(p_H(50)\)</span>?). Let’s try to get some intuition. Of the patients in the data, maybe we have one that’s <span class="math notranslate nohighlight">\(50.1\)</span> inches tall, another that is <span class="math notranslate nohighlight">\(49.9\)</span>, or maybe one that’s <span class="math notranslate nohighlight">\(49.999991\)</span> inches tall—but what are the chances we will observe a patient that is <em>exactly</em> (not approximately) <span class="math notranslate nohighlight">\(50\)</span> inches tall? The answer is: zero. This is because of the arbitrary precision we have on continuous values. So if we can’t describe continuous distributions using PMFs, how else can we describe them? As you will see next, we will have to use a more circuitous route.</p>
</div></blockquote>
<div class="admonition-exercise-why-continuous-distributions-do-not-have-pmfs admonition">
<p class="admonition-title">Exercise: Why continuous distributions do not have PMFs</p>
<p>Demonstrate empirically that continuous distributions do not have valid PMFs via the following procedure:</p>
<ol class="arabic simple">
<li><p>Sample 10,000 values from a Uniform distribution on the range <span class="math notranslate nohighlight">\([0, 1]\)</span> (using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>). This will draw samples on the range <span class="math notranslate nohighlight">\([0, 1]\)</span> with equal probability.</p></li>
<li><p>Count the number of samples that exactly equal <span class="math notranslate nohighlight">\(0.5\)</span>. How many samples did you count?</p></li>
</ol>
</div>
<p><strong>Cumulative Density Function (CDF):</strong> As we showed, we can’t use the frequency of a continuous RV as its probability. But we <em>can</em> ask a different question: what’s the probability that a continuous RV is smaller than or equal to some value. This is called the CDF, which we will denote using <span class="math notranslate nohighlight">\(F\)</span>. What’s interesting about the CDF is that it abides by our intuition of probability as frequencies; we can empirically estimate the CDF by counting all samples for which the variable is <span class="math notranslate nohighlight">\(\leq\)</span> our value of interest. Next, we will use the CDF to define the “continuous RV version” of the PMF. Before moving on, however, let’s illustrate this by continuing with our example of IHH ER patient height.</p>
<blockquote>
<div><p>Example: Let <span class="math notranslate nohighlight">\(F_H(\cdot)\)</span> define the CDF of a continuous RV, <span class="math notranslate nohighlight">\(H\)</span>, describing the height of IHH ER patients. We use <span class="math notranslate nohighlight">\(F_H(h)\)</span> to denote the probability that the height <span class="math notranslate nohighlight">\(H\)</span> is smaller than some specific value, <span class="math notranslate nohighlight">\(h\)</span> (e.g. <span class="math notranslate nohighlight">\(F_H(50)\)</span> is the probability that the patient’s height is <span class="math notranslate nohighlight">\(\leq 50\)</span>). To evaluate <span class="math notranslate nohighlight">\(F_H(50)\)</span> from data, we can simply look at the frequency of patients for which <span class="math notranslate nohighlight">\(H \leq 50\)</span>, just as we did with discrete probability:</p>
<div class="amsmath math notranslate nohighlight" id="equation-584aef66-f65e-4d09-9336-579065e9be73">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-584aef66-f65e-4d09-9336-579065e9be73" title="Permalink to this equation">#</a></span>\[\begin{align}
F_H(50) &amp;= \frac{\text{num patients shorter than 50}}{\text{total num patients}}
\end{align}\]</div>
</div></blockquote>
<p><strong>Properties of CDFs:</strong> Let <span class="math notranslate nohighlight">\(R\)</span> be an RV with CDF <span class="math notranslate nohighlight">\(F_R(\cdot)\)</span>.</p>
<ol class="arabic">
<li><p><em>The CDF is non-decreasing.</em> When <span class="math notranslate nohighlight">\(r_1 &lt; r_2\)</span>, we also have that <span class="math notranslate nohighlight">\(F_R(r_1) \leq F_R(r_2)\)</span>.</p>
<blockquote>
<div><p>Example: Continuing with our running example, the probability that height is smaller than 50 inches is smaller or equal to the probability that height is smaller than 60 inches: <span class="math notranslate nohighlight">\(F_H(50) \leq F_H(60)\)</span>. This is because the number of patients with <span class="math notranslate nohighlight">\(H \leq 60\)</span> should be <span class="math notranslate nohighlight">\(\geq\)</span> the number of patients with <span class="math notranslate nohighlight">\(H \leq 50\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-fbf834f0-825d-4db9-a39f-42085ca9ce17">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-fbf834f0-825d-4db9-a39f-42085ca9ce17" title="Permalink to this equation">#</a></span>\[\begin{align}
    F_H(50) &amp;= \frac{\text{num patients shorter than 50}}{\text{total num patients}} \\
    &amp;\leq \frac{(\text{num patients shorter than 50}) + (\text{num patients with height between 50 and 60})}{\text{total num patients}} \\
    &amp;= \frac{\text{num patients shorter than 60}}{\text{total num patients}} \\
    &amp;= F_H(60)
    \end{align}\]</div>
</div></blockquote>
</li>
<li><p><em>The CDF is bounded by <span class="math notranslate nohighlight">\(0\)</span> below and by <span class="math notranslate nohighlight">\(1\)</span> above.</em> Let <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(U\)</span> be the smallest and largest values in the sample space, respectively. Then, <span class="math notranslate nohighlight">\(\lim\limits_{r \rightarrow L} F_R(r) = 0\)</span>, and <span class="math notranslate nohighlight">\(\lim\limits_{r \rightarrow U} F_R(r) = 1\)</span>.</p>
<blockquote>
<div><p>Example: In our running example, <span class="math notranslate nohighlight">\(L = 0\)</span> (people must have a positive height) and <span class="math notranslate nohighlight">\(U = \infty\)</span> (they can be arbitrarily tall). We denote <span class="math notranslate nohighlight">\(\lim\limits_{h \rightarrow 0} F_H(h)\)</span> as the probability that a patient’s height is <span class="math notranslate nohighlight">\(\leq 0\)</span>. Since a patient can’t be 0 inches or less, this limit evaluates to <span class="math notranslate nohighlight">\(0\)</span>. Similarly, <span class="math notranslate nohighlight">\(\lim\limits_{h \rightarrow \infty} F_H(h)\)</span> denotes the probability that a patient’s height is smaller than infinity. Since this is always true, this limit evaluates to <span class="math notranslate nohighlight">\(1\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p>We will now plot the CDF of some different continuous RVs (a uniform distribution and a normal distribution, denoted by <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{N}\)</span>, respectively). Can you check that both properties are satisfied?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpyro</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro.distributions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">D</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">Uniform</span><span class="p">()</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;CDF of $\mathcal</span><span class="si">{U}</span><span class="s1">[0, 1]$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">()</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;CDF of $\mathcal</span><span class="si">{N}</span><span class="s1">(0, 1)$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$r$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$F_R(r)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;The CDF of Different Continuous RVs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/61e89f49df9daccab79deb337a56b32de47be9dc0b01a336bb0ae74858190017.png" src="_images/61e89f49df9daccab79deb337a56b32de47be9dc0b01a336bb0ae74858190017.png" />
</div>
</div>
<p><strong>Probability Density Function (PDF):</strong> We will now introduce the PDF—the “continuous RV version of the PMF”—using the CDF. To do this, we will introduce just one more fact about the CDF. This fact will allow us to compute the probability of an RV lying <em>between</em> two values: suppose we have a continuous RV, <span class="math notranslate nohighlight">\(R\)</span> with CDF <span class="math notranslate nohighlight">\(F_R(\cdot)\)</span>, then the probability that <span class="math notranslate nohighlight">\(l &lt; R \leq u\)</span> is given by <span class="math notranslate nohighlight">\(F_R(u) - F_R(l)\)</span>.</p>
<blockquote>
<div><p>Example: Suppose we want to know the probability that a patient’s height is between <span class="math notranslate nohighlight">\(40\)</span> and <span class="math notranslate nohighlight">\(60\)</span> inches. Let’s start by computing the probability that the patient’s height is less than <span class="math notranslate nohighlight">\(60\)</span> inches—but this also includes patients whose height is also less than <span class="math notranslate nohighlight">\(40\)</span> inches (which we want to exclude). To do this, we can subtract the probability that their height is less than <span class="math notranslate nohighlight">\(40\)</span> inches. This gives us <span class="math notranslate nohighlight">\(F_H(60) - F_H(40)\)</span>.</p>
</div></blockquote>
<p>Using our new fact, let’s set up the problem differently. Instead of trying to directly compute the probability of <span class="math notranslate nohighlight">\(R = r\)</span> (which is always 0), let’s compute the probability that <span class="math notranslate nohighlight">\(R\)</span> is <em>super close to</em> <span class="math notranslate nohighlight">\(r\)</span>. We can do this by making up a variable, <span class="math notranslate nohighlight">\(\epsilon\)</span>, that is very small, and computing how the CDF changes from <span class="math notranslate nohighlight">\(F_R(r - \epsilon)\)</span> to <span class="math notranslate nohighlight">\(F_R(r)\)</span>. In other words, we want the instantaneous rate-of-change from <span class="math notranslate nohighlight">\(F_R(r - \epsilon)\)</span> to <span class="math notranslate nohighlight">\(F_R(r)\)</span>, since this rate-of-chance can be thought of as “the inclusion of <span class="math notranslate nohighlight">\(r\)</span>”.</p>
<p>Now, the instantaneous rate-of-change is also known as the derivative. Thus, we define the PDF as the derivative of the CDF:</p>
<div class="amsmath math notranslate nohighlight" id="equation-36057e7e-650a-4dd8-b033-0915c74fb247">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-36057e7e-650a-4dd8-b033-0915c74fb247" title="Permalink to this equation">#</a></span>\[\begin{align}
p_R(r) &amp;= \frac{d F_R(r)}{d r}.
\end{align}\]</div>
<p><strong>Properties of PDFs:</strong> Let <span class="math notranslate nohighlight">\(R\)</span> be a continuous RV with PDF <span class="math notranslate nohighlight">\(p_R(\cdot)\)</span>, defined on the sample-space <span class="math notranslate nohighlight">\(S\)</span>.</p>
<ol class="arabic">
<li><p><em>The PDF must integrate to 1.</em> That is, <span class="math notranslate nohighlight">\(\int\limits_{r \in S} p_R(r) dr = 1\)</span>. This is analogous to the fact that discrete probabilities must sum to 1.</p>
<blockquote>
<div><p>Example: If <span class="math notranslate nohighlight">\(R\)</span> is drawn from a Normal distribution, then our sample space is <span class="math notranslate nohighlight">\(S = (-\infty, \infty)\)</span>. We must therefore have that <span class="math notranslate nohighlight">\(\int\limits_{-\infty}^\infty p_R(r) dr = 1\)</span>.</p>
</div></blockquote>
</li>
<li><p><em>The PDF can be greater than 1.</em> This is in contrast to discrete probability, in which the PMF can never be greater than 1.</p>
<blockquote>
<div><p>Example: Examine the plot of the PDF of the normal distribution (with parameters of <span class="math notranslate nohighlight">\(\mu = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma = 0.1\)</span>) below. Notice that it rises above 1.</p>
</div></blockquote>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">r</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$r$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p_R(r)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;The PDF of a continuous RV can be $\geq 1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/29900425b2bc3451c6d97c2aa299b003f369bc87a5ac276802f6e0755862a0ba.png" src="_images/29900425b2bc3451c6d97c2aa299b003f369bc87a5ac276802f6e0755862a0ba.png" />
</div>
</div>
<div class="admonition-exercise-gaining-comfort-with-commonly-used-continuous-distributions admonition">
<p class="admonition-title">Exercise: Gaining comfort with commonly-used continuous distributions</p>
<p>Browse the Wikipedia pages for the following distributions:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" rel="noreferrer" target="_blank">Uniform</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noreferrer" target="_blank">Beta</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noreferrer" target="_blank">Normal (or Gaussian)</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Truncated_normal_distribution" rel="noreferrer" target="_blank">Truncated Normal</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Laplace_distribution" rel="noreferrer" target="_blank">Laplace</a></p></li>
</ul>
<p><strong>Part 1:</strong> For each of the distributions above, plot its PDF (using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>). Play with the parameters of each distribution—how does each affect the shape of the distribution? Summarize what you learn.</p>
<p><strong>Part 2:</strong> Just as before, you can match a distribution to its application by looking at two properties—it’s support, and the shape of the PDF. Using this, answer the following questions:</p>
<ol class="arabic simple">
<li><p>You’re modeling the distribution of heights in the US. Which of the above distributions would you choose and why?</p></li>
<li><p>You have a large collection of antique coins. Unlike modern-day coins, your coins don’t have a 50% probability of landing heads. You’re interested in modeling the distribution of the probability of them landing heads. That is, each coin has a different probability of landing heads—you want to model the distribution of these probabilities. Which of the above distributions would you choose and why?</p></li>
<li><p>You’ve been given a prototype of a new sensor that determines the location of the nearest intergalactic being. The sensor is, on average, correct, but is typically a little off (sometimes it overshoots and sometimes it undershoots the location). Which of the above distributions would you use to describe the error and why?</p></li>
</ol>
<p><em>Hint: On each Wikipedia page, there’s a panel on the right side that summarizes the properties of the distribution (e.g. its support, PDF, example plots, etc.)—all of the information you need is there.</em></p>
</div>
</section>
<section id="expectations">
<h2><span class="section-number">11.2. </span>Expectations<a class="headerlink" href="#expectations" title="Link to this heading">#</a></h2>
<p>An expectation, or expected value, is a fancy name for an <em>average</em> (or <em>mean</em>). As you may have noticed, the parameters for several of the distributions above control their mean (e.g. for a Gaussian and Laplace). This is useful for modeling since it allows us to specify what the model should do “on average.” Expectations will also keep popping up in later derivations—so what are they exactly?</p>
<p><strong>Approximating Expectations.</strong> Given samples from an RV, you likely know how to compute the average: you add up all of the samples and divide by the total number of samples. For example, let <span class="math notranslate nohighlight">\(N\)</span> be an RV representing the number of presents a person gets for their birthday. You can approximate the expectation/average, by:</p>
<ol class="arabic simple">
<li><p>Randomly selecting <span class="math notranslate nohighlight">\(S\)</span> individuals from the population</p></li>
<li><p>Asking each for the number of presents they got, <span class="math notranslate nohighlight">\(n_s\)</span>,</p></li>
<li><p>Computing the average via the formula below. You can expect that for a large <span class="math notranslate nohighlight">\(S\)</span>, this approximation will be close to the true average.</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-82f74eca-7871-4014-b975-39e94f7c90b6">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-82f74eca-7871-4014-b975-39e94f7c90b6" title="Permalink to this equation">#</a></span>\[\begin{align}
\underbrace{\mathbb{E}_{n \sim p_N(\cdot)} [n]}_{\text{fancy notation}} &amp;\approx \underbrace{\frac{1}{S} \sum_{s=1}^S n_s}_{\text{average}}, \text{ where, } n_s \sim p_N(\cdot).
\end{align}\]</div>
<p>In the fancy notation above,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\)</span> denotes “expectation.”</p></li>
<li><p>The subscript, <span class="math notranslate nohighlight">\(n \sim p_N(\cdot)\)</span>, denotes the random variable we’re drawing. In this case, it’s the number of birthday presents.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span>, surrounded by the brackets, is the thing we’re averaging. If we knew each present cost 10 USD, and if we wanted to approximate the average cost, we can replace <span class="math notranslate nohighlight">\(n\)</span> with <span class="math notranslate nohighlight">\(10.0 \cdot n\)</span>.</p></li>
</ul>
<p><strong>Defining Expectations.</strong> Suppose we have a <em>discrete</em> RV, <span class="math notranslate nohighlight">\(A\)</span>, with PMF <span class="math notranslate nohighlight">\(p_A(\cdot)\)</span>, and that we want to compute the expected value of <span class="math notranslate nohighlight">\(f(a)\)</span>, where <span class="math notranslate nohighlight">\(a \sim p_A(\cdot)\)</span>. Then:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cf01deea-4461-4fb5-b41a-0314180672f0">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-cf01deea-4461-4fb5-b41a-0314180672f0" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathbb{E}_{a \sim p_A(\cdot)} [f(a)] &amp;= \sum_{a \in S} p_A(a) \cdot f(a).
\end{align}\]</div>
<p>If <span class="math notranslate nohighlight">\(A\)</span> were a <em>continuous</em> RV, we replace the sum in the above definition with an integral:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c2aeffa5-c5dd-4904-b356-6bbce324e712">
<span class="eqno">(11.6)<a class="headerlink" href="#equation-c2aeffa5-c5dd-4904-b356-6bbce324e712" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathbb{E}_{a \sim p_A(\cdot)} [f(a)] &amp;= \int_{a \in S} p_A(a) \cdot f(a) \cdot da.
\end{align}\]</div>
<p>So where did these formulas come from? And how do they relate to our approximation? Let’s return to our birthday present example. We re-order our samples, <span class="math notranslate nohighlight">\(n_s\)</span>, in ascending order, from least to most number of birthday presents, and see the following pattern:</p>
<ul class="simple">
<li><p>2 people received 0 presents</p></li>
<li><p>3 people received 1 present</p></li>
<li><p>4 people received 2 presents</p></li>
<li><p>1 person received 3 presents</p></li>
<li><p>0 people received 4 or more presents</p></li>
</ul>
<p>Plugging this into our approximation formula above, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}_{n \sim p_N(\cdot)} [n] &amp;= \sum_{n=1}^\infty p_N(n) \cdot n \\
&amp;\approx \frac{1}{S} \sum_{s=1}^S n_s \\
&amp;= \frac{1}{S} ( 0 + 0 + 1 + 1 + 1 + 2 + 2 + 2 + 2 + 3 + 0 \cdot 4 ) 
\end{align*}\]</div>
<p>We can then aggregate the numbers as follows, and distribute the division by <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \frac{1}{S} ( 2 \cdot 0 + 3 \cdot 1 + 4 \cdot 2 + 1 \cdot 3 + 0 \cdot 4 ) \\
&amp;= \frac{2}{S} \cdot 0 + \frac{3}{S} \cdot 1 + \frac{4}{S} \cdot 2 + \frac{1}{S} \cdot 3 + \frac{0}{S} \cdot 4 \\
&amp;= \sum_{n=1}^\infty \underbrace{\frac{\text{number of people who got $n$ presents}}{\text{number of people sampled}}}_{\text{Approaches $p_N(n)$ as $S \rightarrow \infty$}} \cdot n,
\end{align*}\]</div>
<p>which is our definition of expectation.</p>
</section>
<section id="mle-for-discrete-continuous-models">
<h2><span class="section-number">11.3. </span>MLE for Discrete-Continuous Models<a class="headerlink" href="#mle-for-discrete-continuous-models" title="Link to this heading">#</a></h2>
<p>You’ve recently been contacted by researchers at the IHH’s Center for Telekinesis Research (CTR). The researchers at the IHH’s CTR study the propensity of intergalactic beings for telekinesis—the ability of moving physical objects with their mind (how cool!). At the moment, they are interested in understanding how different physiological conditions affect a being’s telekinetic abilities (measured as a real number, with larger numbers indicating heightened telekinetic abilities). They have collected the following data sets and would like your help in analyzing it. Let’s load it and have a look!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a bunch of libraries we&#39;ll be using below</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Load the data into a pandas dataframe</span>
<span class="n">csv_fname</span> <span class="o">=</span> <span class="s1">&#39;data/IHH-CTR.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_fname</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Patient ID&#39;</span><span class="p">)</span>

<span class="c1"># Print a random sample of patients, just to see what&#39;s in the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Condition</th>
      <th>Telekinetic-Ability</th>
    </tr>
    <tr>
      <th>Patient ID</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>398</th>
      <td>Allergic Reaction</td>
      <td>0.510423</td>
    </tr>
    <tr>
      <th>3833</th>
      <td>Allergic Reaction</td>
      <td>0.479960</td>
    </tr>
    <tr>
      <th>4836</th>
      <td>Intoxication</td>
      <td>2.043218</td>
    </tr>
    <tr>
      <th>4572</th>
      <td>Allergic Reaction</td>
      <td>-0.443333</td>
    </tr>
    <tr>
      <th>636</th>
      <td>Intoxication</td>
      <td>1.423190</td>
    </tr>
    <tr>
      <th>2545</th>
      <td>Intoxication</td>
      <td>1.392568</td>
    </tr>
    <tr>
      <th>1161</th>
      <td>Intoxication</td>
      <td>2.110151</td>
    </tr>
    <tr>
      <th>2230</th>
      <td>Intoxication</td>
      <td>2.102866</td>
    </tr>
    <tr>
      <th>148</th>
      <td>Intoxication</td>
      <td>1.865081</td>
    </tr>
    <tr>
      <th>2530</th>
      <td>Allergic Reaction</td>
      <td>0.401414</td>
    </tr>
    <tr>
      <th>4070</th>
      <td>Intoxication</td>
      <td>2.271342</td>
    </tr>
    <tr>
      <th>1261</th>
      <td>Allergic Reaction</td>
      <td>-0.455159</td>
    </tr>
    <tr>
      <th>4682</th>
      <td>Entangled Antennas</td>
      <td>-1.713834</td>
    </tr>
    <tr>
      <th>333</th>
      <td>Intoxication</td>
      <td>2.000120</td>
    </tr>
    <tr>
      <th>906</th>
      <td>Intoxication</td>
      <td>1.693633</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="admonition-exercise-modeling-data-from-the-ihh-s-center-for-telekinesis-research admonition">
<p class="admonition-title">Exercise: Modeling Data from the IHH’s Center for Telekinesis Research</p>
<p>Let <span class="math notranslate nohighlight">\(C\)</span> and <span class="math notranslate nohighlight">\(A\)</span> denote the RVs representing “Condition” and “Telekinetic-Ability,” respectively. Your goal is to learn a generative model (i.e. a joint distribution) of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p><strong>Part 1:</strong> Let’s first perform an exploratory data analysis.</p>
<ul class="simple">
<li><p>Visualize the marginals, <span class="math notranslate nohighlight">\(p_C(\cdot)\)</span> and <span class="math notranslate nohighlight">\(p_A(\cdot)\)</span>.</p></li>
<li><p>Visualize the conditional <span class="math notranslate nohighlight">\(p_{A | C}(\cdot | c)\)</span> for every value of <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
<li><p>Can you visualize <span class="math notranslate nohighlight">\(p_{C | A}(c | a)\)</span>? Explain what makes this challenging.</p></li>
</ul>
<p><strong>Part 2:</strong> Based on the visualizations, which of the following factorizations of the joint would you prefer to use? And which distributions would you use for each component in the factorization? Explain.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{A, C}(a, c) &amp;= p_{A | C}(a | c) \cdot p_C(c) \quad \text{(Option 1)} \\
p_{A, C}(a, c) &amp;= p_{C | A}(c | a) \cdot p_A(a) \quad \text{(Option 2)}
\end{align*}\]</div>
<p><em>Hint: Think of which of the distributions already covered in the course you would use for each component of the joint distribution.</em></p>
<p><strong>Part 3:</strong> Draw a directed graphical model for the joint data likelihood. Write down the log of the joint data likelihood (the joint probability of all <span class="math notranslate nohighlight">\(N\)</span> observations). Both should explicitly depict the parameters of the model that you’d like to learn.</p>
<p><strong>Part 4:</strong> Implement your model in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>. Perform the MLE on your model. Don’t forget to visualize your convergence plot to make sure your training converged!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">IHH_CER_generative_model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">pass</span> <span class="c1"># TODO implement</span>
</pre></div>
</div>
<p><strong>Part 5:</strong> Verify your model learned the data distribution well by visualizing each component of your joint distribution against the empirical distribution of the data, as well as the marginals (you can do this by histogramming data generated from your model).</p>
<p><strong>Part 6:</strong> Help the researchers interpret your fitted model. Which conditions hinder telekinetic ability, and which enhance it? How can you tell?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="ethics-of-learning-from-data.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>The Ethics of Learning from Data</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-continuous-and-discrete-probability">11.1. Differences Between Continuous and Discrete Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations">11.2. Expectations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-discrete-continuous-models">11.3. MLE for Discrete-Continuous Models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>