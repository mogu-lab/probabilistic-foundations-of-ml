
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Joint Probability (Discrete) &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'probability-joint';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/probability-joint.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. The Ethics of Data" href="ethics-of-data.html" />
    <link rel="prev" title="5. Conditional Probability (Discrete)" href="probability-conditional.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/probability-joint.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Joint Probability (Discrete)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation">6.1. Terminology and Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#directed-graphical-models-dgms">6.2. Directed Graphical Models (DGMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#translating-dgms-into-code-with-numpyro">6.3. Translating DGMs into Code with <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="joint-probability-discrete">
<h1><span class="section-number">6. </span>Joint Probability (Discrete)<a class="headerlink" href="#joint-probability-discrete" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> So far, you’ve spent some time conducting a preliminary exploratory data analysis (EDA) of IHH’s ER data. You noticed that considering variables separately can result in misleading information. As a result, you decided to use <em>conditional distributions</em> to model the <em>relationship between variables</em>. Using these conditional distributions, you were able to develop <em>predictive models</em> (e.g. predicting the probability of intoxication given the day of the week). These predictive models are useful for the IHH administration to make decisions.</p>
<p>However, you’ve noticed that your modeling toolkit is still limited. The conditional distributions we introduced can model how the probability of one variable changes given a <em>set</em> of variables. What if we wanted to describe how the probability of a <em>set</em> of variables (i.e. more than one) changes given a <em>set</em> of variables? For example, we may want to answer questions like: “how does the probability that a patient is hospitalized for an allergic reaction change given the day of the week?” In this question, we’re inquiring about two variables—that the condition is an allergic reaction, <em>and</em> that the patient was hospitalized—given the day of the week.</p>
<p><strong>Challenge:</strong> We need to expand our modeling toolkit to include yet another tool—joint probabilities.</p>
<p><strong>Outline:</strong></p>
<ol class="arabic simple">
<li><p>Introduce and practice the concepts, terminology, and notation behind discrete joint probability distributions (leaving continuous distributions to a later time).</p></li>
<li><p>Introduce a graphical representation to describe joint distributions.</p></li>
<li><p>Translate this graphical representation directly into code in a probabilistic programming language (using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>) that we can then use to fit the data.</p></li>
</ol>
<section id="terminology-and-notation">
<h2><span class="section-number">6.1. </span>Terminology and Notation<a class="headerlink" href="#terminology-and-notation" title="Link to this heading">#</a></h2>
<p>We, again introduce the statistical language—terminology and notation—to precisely specify to a computer how to model our data. We will then translate statements in this language directly into code in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> that a computer can run.</p>
<p><strong>Concept.</strong> The concept behind a joint probability is elegant; it allows us to build complicated distributions over many variables using simple conditional and non-conditional distributions (that we already covered).</p>
<p>We can illustrate this using an example with just two variables. Suppose you have two RVs, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. The probability that <span class="math notranslate nohighlight">\(A = a\)</span> and <span class="math notranslate nohighlight">\(B = b\)</span> are <em>both</em> satisfied is called their <em>joint probability</em>. It is denoted by <span class="math notranslate nohighlight">\(p_{A,B}(a, b)\)</span>. This joint distribution can be <em>factorized</em> to a product of conditional and non-conditional (or “marginal”) distributions as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1b0f7a86-d15d-4f58-80ae-e7dc02b1e796">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-1b0f7a86-d15d-4f58-80ae-e7dc02b1e796" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B}(a, b) &amp;= p_{A | B}(a | b) \cdot p_B(b) \quad \text{(Option 1)} \\
\underbrace{\phantom{p_{A, B}(a, b)}}_{\text{joint}} &amp;= \underbrace{p_{B | A}(b | a)}_{\text{conditional}} \cdot \underbrace{p_A(a)}_{\text{marginal}} \quad \text{(Option 2)}
\end{align}\]</div>
<p>Notice that the joint is now described in terms of conditional and marginal distributions, which we already know how to work with!</p>
<p><strong>Intuition.</strong> So what’s the intuition behind this formula? Let’s depict events <span class="math notranslate nohighlight">\(A = a\)</span> and <span class="math notranslate nohighlight">\(B = b\)</span> as follows:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsWuYeac&#x2F;wTE4hVV6OUAOvuXdh4rGzw&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>In this diagram, each shaded area represents the probability of an event—i.e. area is proportional to probability. We use it to pictorially represent the marginal, conditional, and joint distributions. The marginal probability of <span class="math notranslate nohighlight">\(B = b\)</span>, for example, is the ratio of the blue square relative to the whole space (the gray square):</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsSFgi48&#x2F;j-Si33Y0Ju0lc3Uv3DnZjA&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>Similarly, the marginal probability of <span class="math notranslate nohighlight">\(A = a\)</span> is the ratio of the red square relative to the gray square.</p>
<p>Next, the conditional <span class="math notranslate nohighlight">\(p_{A | B}(a | b)\)</span> is the ratio of the purple intersection relative to the blue square. This is because the blue square represents our conditioning on <span class="math notranslate nohighlight">\(B = b\)</span>, and the purple intersection represents the probability that we <em>also</em> have <span class="math notranslate nohighlight">\(A = a\)</span>.</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsXEFi-k&#x2F;tEkK_m_gt12xny6m7tPSGQ&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>Finally, the joint <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span> is the ratio between the purple intersection and the whole space (the gray square). This is because the intersection is where both <span class="math notranslate nohighlight">\(A = a\)</span> and <span class="math notranslate nohighlight">\(B = b\)</span>.</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsomXvPo&#x2F;_75OUnHI7cHExKhOetoyOg&#x2F;view?embed">
  </iframe>
</div>
</div><p>Now we can see that the joint is the product of the conditional and the marginal because the blue squares “cancel out”:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIslUBjEg&#x2F;q32JNdMs0rXmTPGyETngOA&#x2F;view?embed">
  </iframe>
</div>
</div><p><strong>Choice of Factorization.</strong> Lastly, notice that we have a <em>choice</em> to factorize the distribution in two ways. How do you know which one to use? Typically, we choose a factorization that is <em>intuitive to us</em> and what we can compute.</p>
<blockquote>
<div><p><strong>Example:</strong> Suppose you want to model the joint distribution of the day of the week, <span class="math notranslate nohighlight">\(D\)</span> and whether a patient arrive with intoxication, <span class="math notranslate nohighlight">\(I\)</span>. The joint distribution can be factorized in two ways:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6d444ceb-9b73-4663-a42b-958ff69957c6">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-6d444ceb-9b73-4663-a42b-958ff69957c6" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{D, I}(d, i) &amp;= p_{I | D}(i | d) \cdot p_D(d) \quad \text{(Option 1)} \\
&amp;= p_{D | I}(d | i) \cdot p_I(i) \quad \text{(Option 2)} \\
\end{align}\]</div>
<p>Which one makes more intuitive sense? Well, it’s a little weird to try to predict the day of the week given whether a patient arrives with intoxication; we typically know what the day of the week is and we don’t need to predict it. In contrast, given the day of the week, it makes a lot of sense to wonder about the probability of a patient arriving with intoxication. As such, Option 1 makes more sense here.</p>
</div></blockquote>
<p><strong>Generalizing to More than Two RVs.</strong> So now we have the tools to work with joint distributions with two RVs. What do we do if we have three or more? The same ideas apply. The joint distribution for random variables <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> can be factorized in a number of ways. For example, we can condition on two variables at a time:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b048918c-f1df-4f9c-b6e3-72af9f4f44c3">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-b048918c-f1df-4f9c-b6e3-72af9f4f44c3" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B, C}(a, b, c) &amp;= p_{A | B, C}(a | b, c) \cdot p_{B, C}(b, c) \quad \text{(Option 1)} \\
&amp;= p_{B | A, C}(b | a, c) \cdot p_{A, C}(a, c) \quad \text{(Option 2)} \\
&amp;= p_{C | A, B}(c | a, b) \cdot p_{A, B}(a, b) \quad \text{(Option 3)}
\end{align}\]</div>
<p>wherein the above, we already know how to factorize <span class="math notranslate nohighlight">\(p_{B, C}(b, c)\)</span>, <span class="math notranslate nohighlight">\(p_{A, C}(a, c)\)</span>, and <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span> (since they are joint distributions with two variables).</p>
<p>We can also condition on one variable at a time:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a8711b1b-252b-426d-8c6c-3239649673c4">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-a8711b1b-252b-426d-8c6c-3239649673c4" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B, C}(a, b, c) &amp;= p_{A, B | C}(a, b | c) \cdot p_C(c) \quad \text{(Option 1)} \\
&amp;= p_{A, C | B}(a, c | b) \cdot p_B(b) \quad \text{(Option 2)} \\
&amp;= p_{B, C | A}(b, c | a) \cdot p_A(a) \quad \text{(Option 3)}
\end{align}\]</div>
<p>And how do we further factorize distributions of the form <span class="math notranslate nohighlight">\(p_{A, B | C}(a, b | c)\)</span>? We apply the same factorization for a joint distribution with two variables, and simply add a “conditioned on <span class="math notranslate nohighlight">\(C\)</span>” to each one:</p>
<div class="amsmath math notranslate nohighlight" id="equation-080160ce-5dcc-469d-821b-c2411b95583b">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-080160ce-5dcc-469d-821b-c2411b95583b" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B | C}(a, b | c) &amp;= p_{A | B, C}(a | b, c) \cdot p_{B | C}(b | c) \quad \text{(Option 1)} \\
&amp;= p_{B | A, C}(b | a, c) \cdot p_{A | C}(a | c) \quad \text{(Option 2)} \\
\end{align}\]</div>
</section>
<section id="directed-graphical-models-dgms">
<h2><span class="section-number">6.2. </span>Directed Graphical Models (DGMs)<a class="headerlink" href="#directed-graphical-models-dgms" title="Link to this heading">#</a></h2>
<p>As you may have already noticed, the number of possible ways to factorize a joint distribution increases <em>very quickly</em> with the number of RVs. In fact, the more RVs we have, the more unwieldy it becomes for us as data analysts to specify each component in the factorization. What can we do to simplify our model? Often, we can use our <em>domain knowledge</em> (knowledge of the specifics of the problem) to simplify the joint distribution. Specifically, we’ll use our knowledge of “conditional independence” to do this. Let’s get started by first introducing the idea of <em>statistical independence</em>.</p>
<blockquote>
<div><p><strong>Example:</strong> Suppose you want to model the joint distribution of: a patient’s true temperature, <span class="math notranslate nohighlight">\(T\)</span>, a thermometer’s measurement of this temprature, <span class="math notranslate nohighlight">\(M\)</span>, and the doctor’s decision whether or not the patient has fever, <span class="math notranslate nohighlight">\(F\)</span>. As we saw above, this joint distribution, <span class="math notranslate nohighlight">\(p_{F, M, T}(f, m, t)\)</span>, which has three RVs, has <em>many</em> factorizations. Most of these, however, don’t make any sense to a domain expert. That is, the doctor’s decision, <span class="math notranslate nohighlight">\(F\)</span>, is <em>only</em> based on the thermometer’s measurement, <span class="math notranslate nohighlight">\(M\)</span>; the doctor has <em>no way</em> of knowing what the patient’s actual temperature is—only its noisy observation through <span class="math notranslate nohighlight">\(M\)</span>. Thus, <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(T\)</span> are <em>independent</em>. Next, let’s see what this means formally.</p>
</div></blockquote>
<p><strong>Statistical Independence.</strong> We say two variables <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are statistically independent if their joint can be factorized to the product of their marginals:</p>
<div class="amsmath math notranslate nohighlight" id="equation-30c00653-64a4-41e9-8248-42ce344fd175">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-30c00653-64a4-41e9-8248-42ce344fd175" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B}(a, b) &amp;= p_B(b) \cdot p_A(a)
\end{align}\]</div>
<p>This equation implies that to sample <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> jointly, we don’t have to consider their relationship (since the conditional isn’t used)—they are entirely independent.</p>
<p>Another way to understand this equation is by thinking its implications on the conditionals. We do this by factorizing <span class="math notranslate nohighlight">\(p_{A, B}(a, b)\)</span> into the product of the conditional and marginal:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e0808ea1-da14-43f2-a3e1-c13f0ace7dea">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-e0808ea1-da14-43f2-a3e1-c13f0ace7dea" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B}(a, b) &amp;= \underbrace{p_{B | A}(b | a)}_{\text{must equal } p_B(b)} \cdot p_A(a)
\end{align}\]</div>
<p>We then observe that <span class="math notranslate nohighlight">\(p_{B | A}(b | a)\)</span> must equal <span class="math notranslate nohighlight">\(p_B(b)\)</span> to satisfy our definition of statistical independence. And <span class="math notranslate nohighlight">\(p_{B | A}(b | a) = p_B(b)\)</span> implies that having observed <span class="math notranslate nohighlight">\(A = a\)</span> does not affect the probability of <span class="math notranslate nohighlight">\(B = b\)</span>.</p>
<blockquote>
<div><p><strong>Example:</strong> Returning to the above example, we know that <span class="math notranslate nohighlight">\(F\)</span> should only depend on <span class="math notranslate nohighlight">\(M\)</span>, and that <span class="math notranslate nohighlight">\(M\)</span> should only depend on <span class="math notranslate nohighlight">\(T\)</span>. That is, the doctor’s decision only depends on the thermometer’s measurement, and the thermometer only depends on the true temperature. Formally, we can say that <span class="math notranslate nohighlight">\(p_{F | M, T}(f | m, t) = p_{F | M}(f | m)\)</span>, since <span class="math notranslate nohighlight">\(F\)</span> only depends on the thermometer reading, <span class="math notranslate nohighlight">\(M = m\)</span>. Similarly, <span class="math notranslate nohighlight">\(p_{M | T, F}(m | t, f) = p_{M | T}(m | t)\)</span> since the thermometer reading only depends on the patient’s temperature, not on what the doctor might say. Using this, we can factorize the joint as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-376190bd-d27c-4784-b9d3-a4dffe018a59">
<span class="eqno">(6.8)<a class="headerlink" href="#equation-376190bd-d27c-4784-b9d3-a4dffe018a59" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{F, M, T}(f, m, t) &amp;= p_{F | M, T}(f | m, t) \cdot p_{M, T}(m, t) \quad (\text{factorization of the joint}) \\
&amp;= p_{F | M}(f | m) \cdot p_{M, T}(m, t) \quad (\text{since $F$ and $T$ are independent}) \\
&amp;= p_{F | M}(f | m) \cdot p_{M | T}(m | t) \cdot p_T(t) \quad (\text{factorizing the joint of $M$ and $T$})
\end{align}\]</div>
<p>Thus, using our <em>domain expertise</em>, we’ve selected a factorization of the model that makes the most sense.</p>
</div></blockquote>
<p><strong>Graphical Representation of Statistical Dependencies.</strong> Since reasoning about many variables jointly is difficult, we introduce a graphical representation to aid with it. This representation is called a <em>directed graphical model</em> (DGM), and it will help us convey which variables depend on one another in what way.</p>
<p>A DGM is represented using a <em>graph</em> (or network) in which nodes represent RVs and arrows represent conditional dependencies. For example, consider the following DGM for some hypothetical joint distribution, <span class="math notranslate nohighlight">\(p_{A, B, C}(\cdot)\)</span>:</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsdHASpY&#x2F;Se3n0vLWVj3cvkNIiwN9RA&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>In this DGM, there are three nodes, corresponding to our three RVs. Our factorization then consists of one factor for each node:</p>
<ul class="simple">
<li><p>The factor corresponding to <span class="math notranslate nohighlight">\(B\)</span> is <span class="math notranslate nohighlight">\(p_{B | A}(\cdot)\)</span>, since there’s an arrow from <span class="math notranslate nohighlight">\(A\)</span> to <span class="math notranslate nohighlight">\(B\)</span>, indicating a conditional dependence.</p></li>
<li><p>The factor corresponding to <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(p_{A}(\cdot)\)</span>, since there aren’t any arrows pointing into <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>The factor corresponding to <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(p_{C}(\cdot)\)</span>, since there aren’t any arrows pointing into <span class="math notranslate nohighlight">\(C\)</span>.</p></li>
</ul>
<p>In total, the DGM represents the following factorization:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1a88d586-a8ce-4438-9934-d7353a9d9b93">
<span class="eqno">(6.9)<a class="headerlink" href="#equation-1a88d586-a8ce-4438-9934-d7353a9d9b93" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B, C}(a, b, c) &amp;= p_{B | A}(b | a) \cdot p_A(a) \cdot p_C(c)
\end{align}\]</div>
<blockquote>
<div><p><strong>Example:</strong> Continuing with our fever example, we can represent our factorization, <span class="math notranslate nohighlight">\(p_{F, M, T}(f, m, t) = p_{F | M}(f | m) \cdot p_{M | T}(m | t) \cdot p_T(t)\)</span>, with the following DGM: <div class="canva-centered-embedding"><div class="canva-iframe-container"><iframe loading="lazy" class="canva-iframe" src="https://www.canva.com/design/DAGwjS7H9nM/4S-hhl-L-2NlUNP-hLZsIg/view?embed"></iframe></div></div>
As you can see, since <span class="math notranslate nohighlight">\(F\)</span> depends on <span class="math notranslate nohighlight">\(M\)</span>, there’s an arrow leading from <span class="math notranslate nohighlight">\(M\)</span> to <span class="math notranslate nohighlight">\(F\)</span>. And since <span class="math notranslate nohighlight">\(F\)</span> does not depend on <span class="math notranslate nohighlight">\(T\)</span>, there is <em>no</em> arrow leading from <span class="math notranslate nohighlight">\(T\)</span> to <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div></blockquote>
<p>As you can see, out of the numerous ways we can factorize <span class="math notranslate nohighlight">\(p_{A, B, C}(a, b, c)\)</span> or <span class="math notranslate nohighlight">\(p_{F, M, T}(f, m, t)\)</span>, the factorizations given to us just <em>one</em> to focus on—one that, hopefully, makes sense for our application. But what does this simplicity cost us? Expressivity. By assuming certain variables do not have conditional dependencies, we are limiting the possible joint distributions we represent. Is this a problem? Not if our dependence assumptions are reasonable!</p>
<div class="admonition-exercise-practice-with-dgms admonition">
<p class="admonition-title">Exercise: Practice with DGMs</p>
<p>Let’s practice converting between distribution notation and DGMs.</p>
<p><strong>Part 1:</strong> Write down the factorizations implied by the DGMs below.</p>
<ol class="arabic simple">
<li></li>
</ol>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsWvxpuY&#x2F;PHRUe4rJt1T7mcRyUQrAKg&#x2F;view?embed">
  </iframe>
</div>
</div>
<ol class="arabic simple" start="2">
<li></li>
</ol>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMuy3bhms&#x2F;3eRn0CG_F9wamDtTCsjoTQ&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<ol class="arabic simple" start="3">
<li></li>
</ol>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMu_ovZTg&#x2F;GiWABBVFOKgajyEll2nDtQ&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<ol class="arabic simple" start="4">
<li></li>
</ol>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMu-__3-E&#x2F;zF3loMkA2FjWLVAxfTd91A&#x2F;view?embed" allowfullscreen="allowfullscreen">
    </iframe>
  </div>
</div>
<p><strong>Part 2:</strong> Draw DGMs representing each joint distribution below.</p>
<ol class="arabic simple">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{A, B, C, D, E}(a, b, c, d, e) = p_{C | A, B, D}(c | a, b, d) \cdot p_{A | B}(a | b) \cdot p_{D | B, E}(d | b, e) \cdot p_{B}(b) \cdot p_{E}(e)
\end{align*}\]</div>
<ol class="arabic simple" start="2">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{A, B, C, D, E}(a, b, c, d, e) = p_{E | D}(e | d) \cdot p_{D | C}(d | c) \cdot p_{C | A, B}(c | a, b) \cdot p_{A | B}(a | b) \cdot p_B(b)
\end{align*}\]</div>
<ol class="arabic simple" start="3">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{A, B, C, D, E}(a, b, c, d, e) = p_{E | A, B, C, D}(e | a, b, c, d) \cdot p_{D | A, B, C}(d | a, b, c) \cdot p_{C | A, B}(c | a, b) \cdot p_{B | A}(b | a) \cdot p_{A}(a)
\end{align*}\]</div>
</div>
<p><strong>Sampling from Joint Distributions:</strong> Sampling from a joint distribution can be done by sampling from each component in their factorization. When doing this, we must ensure our sampling order is <em>valid</em>. That is, if we have a distribution <span class="math notranslate nohighlight">\(p_{A | B}(a | b) \cdot p_B(b)\)</span>, we cannot sample from <span class="math notranslate nohighlight">\(p_{A | B}(\cdot | b)\)</span> first because we don’t know what <span class="math notranslate nohighlight">\(b\)</span> is yet. We first have to sample <span class="math notranslate nohighlight">\(p_B(\cdot)\)</span> to obtain a specific value <span class="math notranslate nohighlight">\(b\)</span>, and then use that value of <span class="math notranslate nohighlight">\(b\)</span> when sampling from the conditional.</p>
<p>Let’s illustrate this with some examples.</p>
<blockquote>
<div><p><strong>Example 1:</strong> Consider the distribution of temperature, thermometer measurement, and doctor’s decision from the above examples:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c5d0aba6-e680-42ad-aaa9-b30181ae7488">
<span class="eqno">(6.10)<a class="headerlink" href="#equation-c5d0aba6-e680-42ad-aaa9-b30181ae7488" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{F, M, T}(f, m, t) &amp;= p_{F | M}(f | m) \cdot p_{M | T}(m | t) \cdot p_T(t)
\end{align}\]</div>
<p>Suppose we want to draw <span class="math notranslate nohighlight">\(S\)</span> samples from this joint distribution. For every <span class="math notranslate nohighlight">\(s \in \{1, \dots, S \}\)</span>, we would follow the steps below, corresponding to the <em>logic</em> behind the model. That is,</p>
<ol class="arabic simple">
<li><p>First, we sample the patient’s temperature: <span class="math notranslate nohighlight">\(t_s \sim p_T(\cdot)\)</span>.</p></li>
<li><p>Now that the patient’s temperature has been determined (<span class="math notranslate nohighlight">\(T = t_s\)</span>), we can try to measure it: <span class="math notranslate nohighlight">\(m_s | t_s \sim p_{M | T}(\cdot | t_s)\)</span>. This is a draw from a distribution because our thermometer has some random error in its measurement.</p></li>
<li><p>Finally, given the reading of the thermometer (<span class="math notranslate nohighlight">\(M = m_s\)</span>), the doctor can decide if the patient has fever: <span class="math notranslate nohighlight">\(f_s | m_s \sim p_{F | M}(\cdot | m_s)\)</span>.</p></li>
</ol>
<p>Each time we carry out these steps, we get one sample from the joint distribution: <span class="math notranslate nohighlight">\(f_s, m_s, t_s \sim p_{F, M, T}(\cdot, \cdot, \cdot)\)</span>.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Example 2:</strong> Consider the distribution,</p>
<div class="amsmath math notranslate nohighlight" id="equation-804ea987-c537-4a1f-90ed-79798f786e68">
<span class="eqno">(6.11)<a class="headerlink" href="#equation-804ea987-c537-4a1f-90ed-79798f786e68" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B, C}(a, b, c) &amp;= p_{B | A}(b | a) \cdot p_A(a) \cdot p_C(c)
\end{align}\]</div>
<p>Suppose we wanted to draw <span class="math notranslate nohighlight">\(S\)</span> samples from this joint distribution. For every <span class="math notranslate nohighlight">\(s \in \{1, \dots, S \}\)</span>, we would have to:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(c_s \sim p_C(\cdot)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_s \sim p_A(\cdot)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_s | a_s \sim p_{B | A}(\cdot | a_s)\)</span> (wherein <span class="math notranslate nohighlight">\(a\)</span> was sampled in the previous step)</p></li>
</ol>
<p>In carrying out these sampling steps, we would obtain <span class="math notranslate nohighlight">\(S\)</span> samples from the joint distribution: <span class="math notranslate nohighlight">\(a_s, b_s, c_s \sim p_{A, B, C}(\cdot, \cdot, \cdot)\)</span>.</p>
<p>Notice that here, steps (1) and (2) are interchangeable, since they don’t depend on one another; i.e. it doesn’t matter if we sample <span class="math notranslate nohighlight">\(A\)</span> first or <span class="math notranslate nohighlight">\(C\)</span> first. In contrast, step (3) <em>had</em> to happen after step (2), because depends on the value of <span class="math notranslate nohighlight">\(A\)</span> sampled.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Example 3:</strong> Consider the distribution,</p>
<div class="amsmath math notranslate nohighlight" id="equation-1ee89b84-df6a-4a1c-bfd9-24f45a7f31b9">
<span class="eqno">(6.12)<a class="headerlink" href="#equation-1ee89b84-df6a-4a1c-bfd9-24f45a7f31b9" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{A, B, C}(a, b, c) &amp;= p_{B | A, C}(b | a, c) \cdot p_{A | C}(a | c) \cdot p_C(c)
\end{align}\]</div>
<p>Suppose we wanted to draw <span class="math notranslate nohighlight">\(S\)</span> samples from this joint distribution. For every <span class="math notranslate nohighlight">\(s \in \{1, \dots, S \}\)</span>, we would have to:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(c_s \sim p_C(\cdot)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_s | c_s \sim p_{A | C}(\cdot | c_s)\)</span> (wherein <span class="math notranslate nohighlight">\(c_s\)</span> was sampled in the previous step)</p></li>
<li><p><span class="math notranslate nohighlight">\(b_s | a_s, c_s \sim p_{B | A, C}(\cdot | a_s, c_s)\)</span> (wherein <span class="math notranslate nohighlight">\(a_s, c_s\)</span> were sampled in the previous steps)</p></li>
</ol>
<p>In carrying out these sampling steps, we would obtain <span class="math notranslate nohighlight">\(S\)</span> samples from the joint distribution: <span class="math notranslate nohighlight">\(a_s, b_s, c_s \sim p_{A, B, C}(\cdot, \cdot, \cdot)\)</span>.</p>
<p>Notice again that step (2) depends on <span class="math notranslate nohighlight">\(c_s\)</span>, which was sampled in step (1), and that step (3) depends on both <span class="math notranslate nohighlight">\(c_s\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_s\)</span>, sampled in steps (1) and (2). Thus, the order of the sampling does matter!</p>
</div></blockquote>
<div class="admonition-exercise-practice-the-sampling-order admonition">
<p class="admonition-title">Exercise: Practice the sampling order</p>
<p>For each of the 4 DGMs from the previous exercise, write down <em>two</em> different valid sampling orderings. Please use the same notation used here.</p>
</div>
</section>
<section id="translating-dgms-into-code-with-numpyro">
<h2><span class="section-number">6.3. </span>Translating DGMs into Code with <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code><a class="headerlink" href="#translating-dgms-into-code-with-numpyro" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-joint-distributions-are-generative-models admonition">
<p class="admonition-title">Exercise: Joint distributions are generative models</p>
<p><strong>Context:</strong> Your friend is an ML researcher at a nearby university. She heard all about the interesting data you have from the IHH ER and wants to help with the analysis. However, because this is sensitive medical data, she needs to obtain the right credentials, undergo a lengthy training on secure data management, and more, before obtaining access to the data. Realistically, this means she’ll only be able to gain access to the IHH ER data in several months.</p>
<p><strong>Idea:</strong> To help her out, you have an idea—instead of sending her the data directly, you will develop a <em>generative model</em> of the IHH ER data and send that to her instead. This generative model will allow your friend to generate (or sample) realistic data with the same characteristics as the real data without violating any privacy constraints. But what is exactly a generative model? A generative model is a joint probability distribution over all variables in the data: <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(H\)</span>, and <span class="math notranslate nohighlight">\(A\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D\)</span>: Day-of-Week</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span>: Condition</p></li>
<li><p><span class="math notranslate nohighlight">\(H\)</span>: Hospitalized</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span>: Antibiotics</p></li>
</ul>
<p><strong>Limitations:</strong> Note that, in general, releasing a generative model instead of a data set to ensure its anonymity requires serious care. There’s a whole field called “Differential Privacy” devoted to doing this responsibly. We will not cover these techniques in this class.</p>
<p><strong>Domain Expertise:</strong> You consulted with clinical collaborators at the IHH ER and together came up with the following DGM.</p>
<div class="canva-centered-embedding">
<div class="canva-iframe-container">
  <iframe loading="lazy" class="canva-iframe"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIsbt2Mvc&#x2F;_p7bxsayZ9Qc6k9X8G25bw&#x2F;view?embed">
  </iframe>
</div>
</div>
<p>Notice the conditional distributions in this DGM are ones you’ve previously learned (by hand)!</p>
<p><strong>Part 1:</strong> Use what you already know about the marginal and conditional distributions of the IHH ER data, in tandem with what you learned here about joint distributions to implement this generative model. Implement your model as a function that takes in a random generator <code class="docutils literal notranslate"><span class="pre">key</span></code> and outputs a Python dictionary with a single sample from the joint distribution <span class="math notranslate nohighlight">\(p_{D, C, H, A}(\cdot)\)</span>. That is, the dictionary has keys <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(H\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, and <span class="math notranslate nohighlight">\(A\)</span>, with values corresponding to the sample. Your function should rely on your previous implementations of the conditional distributions.</p>
<p><strong>Part 2:</strong> Implement a second function that, given <span class="math notranslate nohighlight">\(D = d\)</span>, <span class="math notranslate nohighlight">\(C = c\)</span>, <span class="math notranslate nohighlight">\(H = h\)</span>, and <span class="math notranslate nohighlight">\(A = a\)</span>, computes the log probability of the joint, <span class="math notranslate nohighlight">\(\log p_{D, C, H, A}(d, c, h, a)\)</span>. Your function should rely on your previous implementations of the conditional distributions.</p>
<p><em>Hint: <span class="math notranslate nohighlight">\(\log (X \cdot Y) = \log X + \log Y\)</span></em>.</p>
<p><strong>Part 3:</strong> Draw 10,000 samples from the generative model, and evaluate the model’s log probability on each. Of all the samples you drew, print out the sample with the <em>highest</em> and <em>lowest</em> log probability. What does each tell you?</p>
<p>Note: You’re welcome to use a loop for this problem (we’ll learn how to vectorize sampling in the next chapter).</p>
<p><strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> discrete distributions only work with integers, not strings. For example, instead of using <span class="math notranslate nohighlight">\(d = \text{Monday}\)</span>, you should convert the days of the week into integers from 0 to 6 (Monday to Sunday), and instead use <span class="math notranslate nohighlight">\(d = 0\)</span> for “Monday”. We’ve created two helper functions to help you with this conversion: <code class="docutils literal notranslate"><span class="pre">convert_day_of_week_to_int</span></code> and <code class="docutils literal notranslate"><span class="pre">convert_condition_to_int</span></code>. You can use them as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">convert_day_of_week_to_int(data['Day-of-Week'])</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_condition_to_int(data['Condition'])</span></code></p></li>
</ul>
<p>Please use the function signatures below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.random</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jrandom</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro.distributions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">IHH_ER_generative_model_sample</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
    <span class="k">pass</span> <span class="c1"># TODO implement</span>

<span class="k">def</span><span class="w"> </span><span class="nf">IHH_ER_generative_model_log_prob</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="k">pass</span> <span class="c1"># TODO implement</span>
</pre></div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="probability-conditional.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Conditional Probability (Discrete)</p>
      </div>
    </a>
    <a class="right-next"
       href="ethics-of-data.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>The Ethics of Data</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation">6.1. Terminology and Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#directed-graphical-models-dgms">6.2. Directed Graphical Models (DGMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#translating-dgms-into-code-with-numpyro">6.3. Translating DGMs into Code with <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>