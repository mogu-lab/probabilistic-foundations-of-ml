
<!DOCTYPE html>


<html lang="en" data-content_root="./" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>13. Regression &#8212; Probabilistic Foundations of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css?v=244d4a68" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css?v=19873a65" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'regression';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/probabilistic-foundations-of-ml/regression.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="14. Classification" href="classification.html" />
    <link rel="prev" title="12. The Ethics of Learning from Data" href="ethics-of-learning-from-data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Probabilistic Foundations of Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Probabilistic Foundations of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Probabilistic ML: What is it? Why use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Introduction to Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-data.html">7. The Ethics of Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">8. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">9. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">11. Probability (Continuous)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-learning-from-data.html">12. The Ethics of Learning from Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">14. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">15. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">16. Model Selection &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-predictive-models.html">17. The Ethics of Predictive Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">18. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">19. Factor Analysis (Dimensionality Reduction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-generative-models.html">20. The Ethics of Generative Models in Sociotechnical Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="priors-and-posteriors.html">21. Priors and Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="posterior-predictives.html">22. Posterior Predictives</a></li>
<li class="toctree-l1"><a class="reference internal" href="ethics-of-uncertainty-and-interpretability.html">23. The Ethics of Uncertainty and Interpretability in Human-AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Synthesis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ethics-of-ml.html">24. The Ethics of Machine Learning: A View from History</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-models">13.1. Predictive Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-regression-model">13.2. The Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-regression-models-in-numpyro">13.3. Implementing Regression Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-vs-causation">13.4. Correlation vs. Causation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-non-probabilistic-regression">13.5. Comparison with Non-Probabilistic Regression</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression">
<h1><span class="section-number">13. </span>Regression<a class="headerlink" href="#regression" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> At this point, you have the building blocks for developing many commonly-used ML models. Using these building blocks, we will develop a commonly used ML model—regression. A regression model is a <em>predictive</em> model (i.e. a conditional distribution), in which the variable we’re interested in predicting is a <em>real number</em>.</p>
<p><strong>Challenge:</strong> Recall that in a conditional distribution, the parameters of the distribution depend on what we’re conditioning. For example, when predicting the probability of intoxication, <span class="math notranslate nohighlight">\(I\)</span>, given the day of the week, <span class="math notranslate nohighlight">\(D\)</span>, we can use a model like,</p>
<div class="amsmath math notranslate nohighlight" id="equation-88547c41-a6f3-4613-ab22-533ef3ea8225">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-88547c41-a6f3-4613-ab22-533ef3ea8225" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{I | D}(i | d) = \mathrm{Ber}(\rho(d)) = \underbrace{\rho(d)^{i} \cdot \left(1 - \rho(d) \right)^{1 - i}}_{\text{Bernoulli PMF (see Wikipedia)}},
\end{align}\]</div>
<p>where the parameter, <span class="math notranslate nohighlight">\(\rho(d)\)</span>, is a function of the condition (i.e. the day <span class="math notranslate nohighlight">\(d\)</span>):</p>
<div class="amsmath math notranslate nohighlight" id="equation-9033f387-b454-4604-8ada-2a3e996aab53">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-9033f387-b454-4604-8ada-2a3e996aab53" title="Permalink to this equation">#</a></span>\[\begin{align} \rho(d) &amp;= \begin{cases}
0.1 &amp; \text{if $d$ is weekday} \\
0.4 &amp; \text{if $d$ is weekend}
\end{cases} 
\end{align}\]</div>
<p>As in this model, in all models we’ve considered so far, the parameters depend on the condition using if-else expressions (e.g. if <span class="math notranslate nohighlight">\(d\)</span> is a weekday, then <span class="math notranslate nohighlight">\(\rho = 0.1\)</span>, else <span class="math notranslate nohighlight">\(\rho = 0.4\)</span>). Unfortunately, as models become more complex, this approach for specifying a model becomes unwieldy. For example, if instead of conditioning on something simple, like the day, we instead conditioned on something complicated, like medical image—<span class="math notranslate nohighlight">\(1000 \times 1000\)</span> pixels in size—how would we write the appropriate if-else expression? Out of a total of 1-million pixels in the image, which ones are important to even include in the if-else expression? The classes of predictive models we will introduce now—regression and (later) classification—precisely allow us to circumvent this issue. How? By making the parameter a more interesting function of the condition. Continuing with the above example, we can define, <span class="math notranslate nohighlight">\(\rho(d)\)</span> as a linear function of <span class="math notranslate nohighlight">\(d\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9fd735f2-40e3-447c-9e30-af971e031fb3">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-9fd735f2-40e3-447c-9e30-af971e031fb3" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho(d) &amp;= \mathrm{intercept} + \mathrm{slope} \cdot d.
\end{align}\]</div>
<p>We can similarly make <span class="math notranslate nohighlight">\(\rho(d)\)</span> a quadratic, or sinusoidal, or polynomial function of <span class="math notranslate nohighlight">\(d\)</span>—there are endless possibilities. Of course, in this example, <span class="math notranslate nohighlight">\(d\)</span> can be one of two values, so making <span class="math notranslate nohighlight">\(\rho(d)\)</span> some function of <span class="math notranslate nohighlight">\(d\)</span> is not helpful—but when <span class="math notranslate nohighlight">\(d\)</span> is more complicated, like an image, this trick will be incredibly helpful. We will start with simple, linear functions, and build up to more complex functions (using neural networks).</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce predictive models generally</p></li>
<li><p>Instantiate a regression model as a specific type of predictive model</p></li>
<li><p>Implement the regression in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></p></li>
<li><p>Understand correlation vs causation</p></li>
<li><p>Relate to non-probabilistic regression</p></li>
</ul>
<p><strong>Data:</strong> Before diving in, let’s familiarize ourself with the data set we’ll be working with. This data set comes from a special, interdisciplinary effort at the IHH, spanning two major centers. The first is one you’re already familiar with: the Center for Telekinesis Research (CTR), dedicated to the study of telekinesis (the ability of moving physical objects with your mind). The second is the Center for Glow and Positive Life Flow (CGLF). As you know, one of the great mysteries of the galaxy is why intergalactic beings glow, and what it can tell us about other aspects of their life. As part of this effort, IHH researchers want to understand how glow affects telekinetic ability. They have recruited you to help with the data analysis.</p>
<p>They sent us a data set consisting of two variables:</p>
<ul class="simple">
<li><p>A measure of glow (real number), in which higher values indicate higher brightness</p></li>
<li><p>A measure of telekinetic ability (also real number), in which higher values indicate higher ability (able to move heavier objects to greater heights, etc.)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a bunch of libraries we&#39;ll be using below</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro.distributions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">D</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="c1"># Load the data into a pandas dataframe</span>
<span class="n">csv_fname</span> <span class="o">=</span> <span class="s1">&#39;data/IHH-CTR-CGLF-regression.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_fname</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Patient ID&#39;</span><span class="p">)</span>

<span class="c1"># Print a random sample of patients, just to see what&#39;s in the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Glow</th>
      <th>Telekinetic-Ability</th>
    </tr>
    <tr>
      <th>Patient ID</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>90</th>
      <td>0.604085</td>
      <td>-0.020933</td>
    </tr>
    <tr>
      <th>254</th>
      <td>0.613645</td>
      <td>-0.070165</td>
    </tr>
    <tr>
      <th>283</th>
      <td>0.829212</td>
      <td>0.140791</td>
    </tr>
    <tr>
      <th>445</th>
      <td>0.981120</td>
      <td>0.261027</td>
    </tr>
    <tr>
      <th>461</th>
      <td>0.688329</td>
      <td>-0.027250</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.796853</td>
      <td>-0.033701</td>
    </tr>
    <tr>
      <th>316</th>
      <td>0.839546</td>
      <td>0.344510</td>
    </tr>
    <tr>
      <th>489</th>
      <td>0.929422</td>
      <td>0.268031</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.893813</td>
      <td>0.422464</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.832483</td>
      <td>0.375658</td>
    </tr>
    <tr>
      <th>241</th>
      <td>0.676760</td>
      <td>-0.028127</td>
    </tr>
    <tr>
      <th>250</th>
      <td>0.711121</td>
      <td>-0.078376</td>
    </tr>
    <tr>
      <th>390</th>
      <td>0.683075</td>
      <td>0.176542</td>
    </tr>
    <tr>
      <th>289</th>
      <td>0.472696</td>
      <td>-0.153246</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.703657</td>
      <td>0.028212</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Glow&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Telekinetic-Ability&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Glow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Telekinetic Ability vs. Glow&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d6e87edfe8439de28d7ae1e72cd133facc13fa23c5c346be544ebcfaee0b4e0a.png" src="_images/d6e87edfe8439de28d7ae1e72cd133facc13fa23c5c346be544ebcfaee0b4e0a.png" />
</div>
</div>
<section id="predictive-models">
<h2><span class="section-number">13.1. </span>Predictive Models<a class="headerlink" href="#predictive-models" title="Link to this heading">#</a></h2>
<p>To understand the effects of glow on telekinetic ability, we will try to see if there is a relationship between them—if glow changes, does telekinetic ability change as well? To do this, we will fit a predictive model (and more specifically, a regression model) to the data. We will then interpret it and see what we learn. Of course, we have to be cautious about our interpretation of the results: just because telekinetic ability may change as a function of glow <em>does not</em> mean that it <em>causes</em> the change—more on that later.</p>
<p><strong>Approach.</strong> Since regression models are a specific type of predictive model, let’s first describe what a predictive model is using the toolkit we’ve developed so far. Specifically, we’ll describe a predictive model using the directed graphical model and its corresponding generative process. Then, we will write down its joint data distribution and MLE objective.</p>
<p><strong>Directed Graphical Model.</strong> We have data set of observations of two RVs: “inputs” <span class="math notranslate nohighlight">\(X\)</span>, representing glow, and “outputs” <span class="math notranslate nohighlight">\(Y\)</span>, representing telekinetic ability. Our goal is to learn to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span> (i.e. learn a conditional distribution <span class="math notranslate nohighlight">\(p_{Y | X}\)</span>). Our graphical model should therefore include the following:</p>
<ul class="simple">
<li><p>It should have a node for <span class="math notranslate nohighlight">\(X\)</span> and a node for <span class="math notranslate nohighlight">\(Y\)</span>, representing our two random variables.</p></li>
<li><p>It should have an arrow pointing from <span class="math notranslate nohighlight">\(X\)</span> into <span class="math notranslate nohighlight">\(Y\)</span> to represent <span class="math notranslate nohighlight">\(Y\)</span>’s dependence on <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Since we have many i.i.d observations of <span class="math notranslate nohighlight">\((X, Y)\)</span>-pairs, we’ll need <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> in a plate.</p></li>
<li><p>Finally, the we’ll probably want the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> to be controlled by some parameter, <span class="math notranslate nohighlight">\(\theta\)</span>, that can be learned from the data.</p></li>
</ul>
<p>A predictive model has the following graphical representation:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGJv58NH1k&#x2F;-MhOv0aEL8dP0kRgjrIN6Q&#x2F;view?embed">
    </iframe>
  </div>
</div><p><strong>Generative Process.</strong> Our graphical model can be directly translated into the following generative process: for <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span>,</p>
<div class="amsmath math notranslate nohighlight" id="equation-72a7ab93-1097-46ac-b884-afe6dc3d6fe1">
<span class="eqno">(13.4)<a class="headerlink" href="#equation-72a7ab93-1097-46ac-b884-afe6dc3d6fe1" title="Permalink to this equation">#</a></span>\[\begin{align}
x_n &amp;\sim p_X(x_n) \\
y_n | x_n &amp;\sim p_{Y | X}(\cdot | x_n; \theta) 
\end{align}\]</div>
<p>Notice that we’ve chosen our marginal over the inputs, <span class="math notranslate nohighlight">\(p_X(x_n)\)</span>, to <em>not depend</em> on any parameters. This is because we <em>just</em> care about learning how to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span>. That is, we assume our use-case will be that we are <em>given</em> specific <span class="math notranslate nohighlight">\(x\)</span>’s for which we want to make predictions—we therefore don’t need to sample them from <span class="math notranslate nohighlight">\(p_X(\cdot)\)</span> (or learn <span class="math notranslate nohighlight">\(p_X(\cdot)\)</span>). As you will see in the derivation of our objective function, <span class="math notranslate nohighlight">\(p_X(x_n)\)</span> will not impact our MLE in any way.</p>
<p><strong>Joint Data Log-Likelihood.</strong> For the above model, we have the following joint data log-likelihood:</p>
<div class="amsmath math notranslate nohighlight" id="equation-008d64fd-d29e-4bc2-a3b4-1cadde49cd5e">
<span class="eqno">(13.5)<a class="headerlink" href="#equation-008d64fd-d29e-4bc2-a3b4-1cadde49cd5e" title="Permalink to this equation">#</a></span>\[\begin{align}
\log p(\mathcal{D}; \theta) &amp;= \log \prod\limits_{n=1}^N p(\mathcal{D}_n; \theta) \quad (\text{i.i.d observations}) \\
&amp;= \sum\limits_{n=1}^N \log p(\mathcal{D}_n; \theta) \quad (\text{since } \log(a \cdot b) = \log a + \log b) \\
&amp;= \sum\limits_{n=1}^N \log p_{X, Y}(x_n, y_n; \theta) \quad (\text{since } \mathcal{D}_n = (x_n, y_n)) \\
&amp;= \sum\limits_{n=1}^N \log \left( p_{Y | X}(y_n | x_n; \theta) \cdot p_X(x_n) \right) \quad (\text{using our factorization of the joint}) \\
&amp;= \sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta) + \log p_X(x_n) \quad (\text{since } \log(a \cdot b) = \log a + \log b) 
\end{align}\]</div>
<p><strong>MLE Objective.</strong> Our goal is to maximize the probability of the joint data log-likelihood with respect to parameters <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8297e944-0b6c-4f73-99a5-c55d9aa27c4b">
<span class="eqno">(13.6)<a class="headerlink" href="#equation-8297e944-0b6c-4f73-99a5-c55d9aa27c4b" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathrm{argmax}_\theta \log p(\mathcal{D}; \theta) &amp;= \mathrm{argmax}_\theta \sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta) + \log p_X(x_n) \quad (\text{substituting in our joint data log-likelihood}) \\
&amp;= \mathrm{argmin}_\theta -\sum\limits_{n=1}^N \left( \log p_{Y | X}(y_n | x_n; \theta) + \log p_X(x_n) \right) \quad (\text{taking an argmax is like taking an argmin of the negative}) \\
&amp;= \mathrm{argmin}_\theta -\sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta) - \underbrace{\sum\limits_{n=1}^N \log p_X(x_n)}_{\text{does not depend on } \theta} \quad (\text{we split out the sum into two parts}) \\
&amp;= \mathrm{argmin}_\theta -\sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta) \quad (\text{term that doesn't depend on $\theta$ doesn't affect argmin}) 
\end{align}\]</div>
<p>This gives us the following objective (or loss function):</p>
<div class="amsmath math notranslate nohighlight" id="equation-48f83f00-5aeb-46ff-809a-cec0a922d80e">
<span class="eqno">(13.7)<a class="headerlink" href="#equation-48f83f00-5aeb-46ff-809a-cec0a922d80e" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathcal{L}(\theta) = -\sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta)
\end{align}\]</div>
<p>Now that we have our objective, the only thing missing now is a specific choice of <span class="math notranslate nohighlight">\(p_{Y | X}(y_n | x_n; \theta)\)</span>; given a specific value of glow, <span class="math notranslate nohighlight">\(x_n\)</span>, what’s the distribution of telekinetic ability, <span class="math notranslate nohighlight">\(y_n\)</span>? We will next select <span class="math notranslate nohighlight">\(p_{Y | X}(y_n | x_n; \theta)\)</span> to give us a regression model. Later, we will do the same for other models (e.g. classification).</p>
</section>
<section id="the-regression-model">
<h2><span class="section-number">13.2. </span>The Regression Model<a class="headerlink" href="#the-regression-model" title="Link to this heading">#</a></h2>
<p><strong>The Model.</strong> Looking at our objective function, <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, the only thing we need to define is <span class="math notranslate nohighlight">\(p_{Y | X}(y_n | x_n; \theta)\)</span>. Let’s introduce a picture to help our intuition for what this distribution should be. Looking at the plot above of telekinetic ability vs. glow, we see that <span class="math notranslate nohighlight">\(p_{Y | X}(y_n | x_n; \theta)\)</span>, should have two parts:</p>
<ol class="arabic simple">
<li><p>It needs some way of accounting for the general “trend”. As glow increases, telekinetic ability also increases until the very end, when it drops off.</p></li>
<li><p>Around this trend, there is some “noise”—no point exactly fits the trend.</p></li>
</ol>
<p>To capture both components, we define our model as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b2ee1b41-9ff2-4dca-93dc-cca05380f878">
<span class="eqno">(13.8)<a class="headerlink" href="#equation-b2ee1b41-9ff2-4dca-93dc-cca05380f878" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{Y | X}(\cdot | x_n; \underbrace{W, \sigma}_{\theta}) = \mathcal{N}( \underbrace{\mu(x_n; W)}_{\text{trend}}, \underbrace{\sigma^2}_{\text{noise}} ),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = \{ W, \sigma \}\)</span>, and where <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> is a function parameterized by <span class="math notranslate nohighlight">\(W\)</span> (which controls its shape). Putting this all together, our generative process says:</p>
<ol class="arabic simple">
<li><p>We’re given an input, <span class="math notranslate nohighlight">\(x_n\)</span>, from our data.</p></li>
<li><p>We feed it through our function <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> to predict what <span class="math notranslate nohighlight">\(y_n\)</span> will be, on average. This captures the “trend.”</p></li>
<li><p>Finally, we add some Gaussian noise to the result. This Gaussian noise represents the error in our measurement. For example, our devices that measure height, blood pressure, etc. at the IHH are correct on average, but are always off by some small amount of random noise. How much noise is determining by the variance of the Gaussian, <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
</ol>
<p><strong>Expressivity.</strong> Let’s visualize what this looks like:</p>
<figure class="align-center" id="fig-regression-example">
<img alt="_images/example_regression.png" src="_images/example_regression.png" />
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">Examples of regression with linear <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> (left) and polynomial <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> (right).</span><a class="headerlink" href="#fig-regression-example" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Here, the blue line, representing <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> captures the “trend” of the data. The data (in red) surrounds the trend; it lies around it due to the observation error (or noise). In the “linear” regression plot, <span class="math notranslate nohighlight">\(\mu(x_n; W)\)</span> is a line, where <span class="math notranslate nohighlight">\(W = \{ a, b \}\)</span> is the slope and intercept of a line:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7a73da0a-6668-458d-9d44-a2612df8042c">
<span class="eqno">(13.9)<a class="headerlink" href="#equation-7a73da0a-6668-458d-9d44-a2612df8042c" title="Permalink to this equation">#</a></span>\[\begin{align}
a \cdot x_n + b.
\end{align}\]</div>
<p>In the “polynomial” regression plot, <span class="math notranslate nohighlight">\(\mu(x_n; W)\)</span> is a polynomial function of <span class="math notranslate nohighlight">\(x_n\)</span>, where <span class="math notranslate nohighlight">\(W = \{ a, b, c, d, e \}\)</span> captures the coefficients of the polynomial:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ef1df888-183f-4475-8bd8-f6eada1623c7">
<span class="eqno">(13.10)<a class="headerlink" href="#equation-ef1df888-183f-4475-8bd8-f6eada1623c7" title="Permalink to this equation">#</a></span>\[\begin{align}
a \cdot x_n^4 + b \cdot x_n^3 + c \cdot x_n^2 + d \cdot x_n + e.
\end{align}\]</div>
</section>
<section id="implementing-regression-models-in-numpyro">
<h2><span class="section-number">13.3. </span>Implementing Regression Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code><a class="headerlink" href="#implementing-regression-models-in-numpyro" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-implement-linear-regression admonition">
<p class="admonition-title">Exercise: Implement Linear Regression</p>
<p><strong>Part 1:</strong> Using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, implement a linear regression model. Your model should only include the conditional distribution of telekinetic ability given glow; it should <em>not</em> include the marginal distribution of glow. Your model should therefore have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_linear_regressor</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">pass</span> <span class="c1"># TODO implement</span>
</pre></div>
</div>
<p>Notice that here, we expect <span class="math notranslate nohighlight">\(x\)</span> / <code class="docutils literal notranslate"><span class="pre">glow</span></code> to <em>always</em> be passed in. This is because we’re not looking to learn the distribution of <code class="docutils literal notranslate"><span class="pre">glow</span></code>—we’re always going to be given it.</p>
<p><strong>Part 2:</strong> Fit the model to the data using the MLE. Plot the loss and verify that your optimization converged. In your plot you may want to zoom in on the last iterations of optimizations to verify it indeed converged.</p>
<p><strong>Part 3:</strong> Visualize your model fit by plotting samples from your model against the real data.</p>
<ul class="simple">
<li><p>How well does your model fit the data?</p></li>
<li><p>For what patients will it perform well, and for which patients will it perform badly?</p></li>
</ul>
</div>
<div class="admonition-exercise-implement-polynomial-regression admonition">
<p class="admonition-title">Exercise: Implement Polynomial Regression</p>
<p><strong>Part 1:</strong> Using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, implement polynomial regression. Your model will now take one additional argument—the degree of the polynomial (a polynomial in which the highest term is <span class="math notranslate nohighlight">\(x^4\)</span> is of degree <span class="math notranslate nohighlight">\(4\)</span>). Please use the function signature below. You can use <code class="docutils literal notranslate"><span class="pre">jnp.polyval</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_polynomial_regressor</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">pass</span> <span class="c1"># TODO implement</span>
</pre></div>
</div>
<p><strong>Part 2:</strong> Fit the model to the data using the MLE with <code class="docutils literal notranslate"><span class="pre">degree=2</span></code>, <code class="docutils literal notranslate"><span class="pre">5</span></code> and <code class="docutils literal notranslate"><span class="pre">8</span></code>. For each, plot the loss and verify that your optimization converged.</p>
<p><strong>Part 3:</strong> Answer the following questions.</p>
<ul class="simple">
<li><p>How well does each model fit the data?</p></li>
<li><p>Which model would you ultimately recommend to the researchers at the IHH? Why? Your answer should look at how the model <em>interpolates</em>, meaning the trend it learns for the points in your data, and also how it <em>extrapolates</em>, meaning the trend it learns away from your data (e.g. in the extreme values of the inputs).</p></li>
</ul>
</div>
</section>
<section id="correlation-vs-causation">
<h2><span class="section-number">13.4. </span>Correlation vs. Causation<a class="headerlink" href="#correlation-vs-causation" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-correlation-vs-causation admonition">
<p class="admonition-title">Exercise: Correlation vs. Causation</p>
<p>Now that you have several models that reasonably fit the data, you have a meeting scheduled with the IHH researchers from the Center for Telekinesis Research and the Center for Glow and Positive Life Flow. In the meeting, you will discuss the results of the modeling and try to determine what scientific insights you can gather.</p>
<p><strong>Part 1:</strong> Just based on the analysis you’ve conducted so far, do you think a brighter glow causes heightened telekinetic ability? Do you think a heightened telekinetic ability causes a brighter glow? Or neither? Why?</p>
<p><strong>Part 2:</strong> After much deliberation, the other IHH researchers mention that since they’ve given you the data, they’ve collected some additional variables. They send you their new, augmented data set, which can be found at <code class="docutils literal notranslate"><span class="pre">data/IHH-CTR-CGLF-regression-augmented.csv</span></code>. Load the data in and create scatter plots of the relationship between every pair of variables. Does this data set offer another potential explanation for the relationships between glow and telekinetic ability?</p>
<p><strong>Part 3:</strong> As the augmented data shows, instead of a brighter glow <em>causing</em> heightened telekinetic ability, there is a third variable that actually explains this relationship. This variable is called a “confounding variable”—a variable that explains what we perceive to be the cause <em>and</em> its effect. In the following examples, identify the confounder and use it to explain why the initial conclusions are wrong.</p>
<ul class="simple">
<li><p>When ice cream sales increase, so do wildfires. Therefore, we should ban ice cream to prevent wildfires.</p></li>
<li><p>As the number of CS and DS faculty Wellesley hires increases, so does the level of the ocean. Therefore, by decreasing the size of Wellesley’s CS and DS departments, we can lower rising ocean levels.</p></li>
<li><p>As the number of pirates decreases, the average temperature of the earth increases. Therefore, we need to bring back pirates to reduce global warming.</p></li>
<li><p>Browse <a class="reference external" href="https://www.tylervigen.com/spurious-correlations" rel="noreferrer" target="_blank">this website</a>, which presents a variety of spurious correlations. Pick one and explain it.</p></li>
</ul>
<p><strong>Part 4:</strong> In performing data analysis like the types of analyses covered here, we are always at risk of drawing wrong conclusions.</p>
<ul class="simple">
<li><p>What are the pitfalls we might fall into?</p></li>
<li><p>In this specific instance, what are the implications of reaching the wrong conclusion?</p></li>
<li><p>How can you avoid these pitfalls?</p></li>
</ul>
</div>
</section>
<section id="comparison-with-non-probabilistic-regression">
<h2><span class="section-number">13.5. </span>Comparison with Non-Probabilistic Regression<a class="headerlink" href="#comparison-with-non-probabilistic-regression" title="Link to this heading">#</a></h2>
<p><strong>Non-Probabilistic Regression.</strong> In non-probabilistic approaches to ML, one typically writes down a loss function directly (without necessarily deriving it from the MLE of the model). Specifically, one typically choses some notion of prediction error they want to minimize, and write down an objective to minimize it directly. A common choice of loss function for regression is minimizing the mean squared error (MSE).</p>
<div class="amsmath math notranslate nohighlight" id="equation-900805a3-58ca-4462-8f46-a143b4e67661">
<span class="eqno">(13.11)<a class="headerlink" href="#equation-900805a3-58ca-4462-8f46-a143b4e67661" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathrm{MSE}(W) &amp;= \frac{1}{N} \sum\limits_{n=1}^N \underbrace{( y_n - \overbrace{\mu(x_n; W)}^{\text{predictor}} )^2}_{\text{squared error}}.
\end{align}\]</div>
<p>As you can see in the loss function above, we have some function, <span class="math notranslate nohighlight">\(\mu(x_n; W)\)</span>, that aims to predict <span class="math notranslate nohighlight">\(y_n\)</span>. Our goal is to minimize the square error between the predictor and <span class="math notranslate nohighlight">\(y_n\)</span>, averaged across our whole data set. Of course, we can pick any notion of error we want, but what’s interesting is that different choices of error implicitly define a probabilistic model.</p>
<p><strong>Relating MSE-Loss to Gaussian Observation Error.</strong> We will now show how finding a <span class="math notranslate nohighlight">\(W\)</span> that minimizes the MSE-loss is equivalent to finding a <span class="math notranslate nohighlight">\(W\)</span> via the MLE. Here, we will only minimize our objective relative to <span class="math notranslate nohighlight">\(W\)</span> (ignoring <span class="math notranslate nohighlight">\(\sigma\)</span>). We start with the MLE objective for our regression model from above:</p>
<div class="amsmath math notranslate nohighlight" id="equation-372b742d-262e-4e21-8040-e70f056aa2b8">
<span class="eqno">(13.12)<a class="headerlink" href="#equation-372b742d-262e-4e21-8040-e70f056aa2b8" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathrm{argmin}_W \mathcal{L}(W) &amp;= \mathrm{argmin}_W -\sum\limits_{n=1}^N \log p_{Y | X}(y_n | x_n; \theta) \\
&amp;= \mathrm{argmin}_W -\sum\limits_{n=1}^N \log \mathcal{N}( \mu(x_n; W), \sigma^2 ) \quad (\text{substitute in our choice of conditional}) \\
&amp;= \mathrm{argmin}_W -\sum\limits_{n=1}^N \log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{\left(y - \mu(x_n; W)\right)^2}{2 \sigma^2} \right) \right) \quad (\text{plug in the definition for Gaussian PDF from Wikipedia}) \\
&amp;= \mathrm{argmin}_W -\sum\limits_{n=1}^N \log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) - \frac{\left(y - \mu(x_n; W)\right)^2}{2 \sigma^2} \quad (\log (a + b) = \log a + \log b) \\
&amp;= \mathrm{argmin}_W \sum\limits_{n=1}^N -\log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) + \sum\limits_{n=1}^N \frac{\left(y - \mu(x_n; W)\right)^2}{2 \sigma^2} \quad (\text{split sum into two parts}) \\
&amp;= \mathrm{argmin}_W \sum\limits_{n=1}^N \frac{\left(y - \mu(x_n; W)\right)^2}{2 \sigma^2} \quad (\text{additive terms that don't depend on $W$ don't affect argmin}) \\
&amp;= \mathrm{argmin}_W \sum\limits_{n=1}^N \left(y - \mu(x_n; W)\right)^2 \quad (\text{scaling terms that don't depend on $W$ don't affect argmin}) \\
&amp;= \mathrm{argmin}_W \frac{1}{N} \sum\limits_{n=1}^N \left(y - \mu(x_n; W)\right)^2 \quad (\text{scaling terms that don't depend on $W$ don't affect argmin}) \\
&amp;= \mathrm{argmin}_W \text{ } \mathrm{MSE}(W)
\end{align}\]</div>
<p>As you can see, both the probabilistic and non-probabilistic formulations yield the same <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p><strong>Why use the probabilistic approach?</strong> The probabilistic approach forces us to spell out our assumptions in the form of a model. It forces to spell out exactly which variables depend on which other variables (via a directed graphical model), and what is the nature of the dependence (when selecting specific distributions and parameterizations for functions). As such, when our model fits the data poorly, the probabilistic approach allows us to investigate why. For example, you may have noticed that none of the models you developed to predict telekinetic ability from glow fit the data perfectly. Why is that? Let’s investigate.</p>
<div class="admonition-exercise-diagnosing-model-mismatch admonition">
<p class="admonition-title">Exercise: Diagnosing Model Mismatch</p>
<p>Pick one of the models you’ve fit to the data above. For the model you chose, plot the distribution of observation errors using a histogram. That is, plot the distribution of <span class="math notranslate nohighlight">\(y_n - \mu(x_n; W^\text{MLE})\)</span>—these are called “residuals.”</p>
<ul class="simple">
<li><p>Given our modeling assumptions, what <em>should</em> this distribution be? How do you know?</p></li>
<li><p>Does the distribution we observe empirically match the distribution from our model? Why?</p></li>
</ul>
<p><em>Hint: You can estimate <span class="math notranslate nohighlight">\(\mu(x_n; W^\text{MLE})\)</span> using the mean of <span class="math notranslate nohighlight">\(y \sim p_{Y|X}\)</span>.</em></p>
</div>
<p><strong>Conclusion.</strong> There are two important bits we want you to take away from this last part:</p>
<ol class="arabic simple">
<li><p>The derivation above that equates Gaussian observation noise with MSE-loss points to the importance of our modeling assumptions. For example, each choice of observation noise distribution implies a different notion of “error” between our trend <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. It’s therefore important to pick distributions that are both informed by domain knowledge <em>and</em> by the data.</p></li>
<li><p>While for the types of predictive models we introduce here, both the probabilistic and the non-probabilistic approaches may yield the same predictor, the probabilistic approach offers an important advantage. It allows us to interrogate <em>why</em> a model fits poorly. This is, of course, not the only advantage of the probabilistic paradigm! We will soon get to more things you can do with it that you cannot do with the non-probabilistic paradigm.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ethics-of-learning-from-data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>The Ethics of Learning from Data</p>
      </div>
    </a>
    <a class="right-next"
       href="classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-models">13.1. Predictive Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-regression-model">13.2. The Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-regression-models-in-numpyro">13.3. Implementing Regression Models in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-vs-causation">13.4. Correlation vs. Causation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-non-probabilistic-regression">13.5. Comparison with Non-Probabilistic Regression</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-8">

        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Probabilistic Foundations of Machine Learning</span> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://yanivyacoby.github.io/" target="_blank">Yaniv Yacoby</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

      </div>      
      <div class="col-4">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>